note	bugId	status	times	testName	failure	failureMessage	stackTrace	reproStatus	replayedFailure	replayedErrorMessage	replayedStackTrace	replayedFile	minConfig	debugFiles											
	Bug-87	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate#testQuotaIssuesWhileCommitting	java.lang.IllegalArgumentException		java.base/java.util.concurrent.LinkedBlockingQueue.<init>(LinkedBlockingQueue.java:254), org.apache.hadoop.ipc.Server$Listener$Reader.<init>(Server.java:1267), org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1242), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371), org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:853), org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:466), org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:865), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:771), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate.setUp(TestDiskspaceQuotaUpdate.java:76), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate/testQuotaIssuesWhileCommitting/campaign/failures/debug_000006	{"ipc.server.read.connection-queue.size": "0"}	["debug_000006"]											
	Bug-188	BUG	1	org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testMoveBlockFailure	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.testMoveBlockFailure(TestFsDatasetImpl.java:1005), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.testMoveBlockFailure$$CONFUZZ(TestFsDatasetImpl.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl/testMoveBlockFailure/campaign/failures/debug_000000	{"dfs.namenode.resource.checked.volumes.minimum": "2139095042"}	["debug_000000"]											
	Bug-189	BUG	1	org.apache.hadoop.hdfs.TestPread#testPreadLocalFS	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.read(ChecksumFileSystem.java:210), org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:124), org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:117), org.apache.hadoop.hdfs.TestPread.writeFile(TestPread.java:96), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS(TestPread.java:503), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS$$CONFUZZ(TestPread.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestPread/testPreadLocalFS/campaign/failures/debug_000000	{"io.file.buffer.size": "2081664379", "file.stream-buffer-size": "1398914017"}	["debug_000000"]											
the stale interval is too long	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock#testAviodStaleDatanodes	java.lang.AssertionError		org.junit.Assert.fail(Assert.java:87), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertTrue(Assert.java:53), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleDatanodes(TestSortLocatedBlock.java:197), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleDatanodes$$CONFUZZ(TestSortLocatedBlock.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock/testAviodStaleDatanodes/campaign/failures/debug_000000	{"dfs.namenode.stale.datanode.minimum.interval": "757603536"}	["debug_000000"]											
consider load affects the sort	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageType	org.junit.ComparisonFailure	expected:<IP-[2-4]> but was:<IP-[0-1]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageType(TestDatanodeManager.java:733), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageType$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderStorageType/campaign/failures/debug_000000	{"dfs.namenode.read.considerLoad": "true"}	["debug_000000", "debug_000004", "debug_000002", "debug_000003"]											
	Bug-190	BUG	1	org.apache.hadoop.hdfs.web.TestWebHDFSXAttr#testRawXAttrs	java.lang.IllegalArgumentException		java.base/java.util.concurrent.ScheduledThreadPoolExecutor.scheduleWithFixedDelay(ScheduledThreadPoolExecutor.java:671), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminManager.activate(DatanodeAdminManager.java:151), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.activate(DatanodeManager.java:451), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.activate(BlockManager.java:740), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startCommonServices(FSNamesystem.java:1278), org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:870), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:791), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.FSXAttrBaseTest.initCluster(FSXAttrBaseTest.java:1377), org.apache.hadoop.hdfs.server.namenode.FSXAttrBaseTest.init(FSXAttrBaseTest.java:109), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.web.TestWebHDFSXAttr/testRawXAttrs/campaign/failures/debug_000001	{"dfs.namenode.decommission.interval.testing": "0"}	["debug_000001"]											
unfortunate.. The connection is scanned frequently such that the connection has been closed before the read	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.TestMalformedURLs#testTryStartingCluster	java.io.EOFException		java.base/java.io.DataInputStream.readInt(DataInputStream.java:397), org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922), org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238), org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestMalformedURLs/testTryStartingCluster/campaign/failures/debug_000009	{"ipc.client.idlethreshold": "0", "ipc.client.connection.idle-scan-interval.ms": "263", "ipc.client.connection.maxidletime": "2130640638"}	["debug_000009"]											
the option does not inherit permission from parent directory	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl#testDefaultAclNewFileIntermediate	java.lang.AssertionError	expected:<432> but was:<416>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.hdfs.server.namenode.AclTestHelpers.assertPermission(AclTestHelpers.java:163), org.apache.hadoop.hdfs.server.namenode.AclTestHelpers.assertPermission(AclTestHelpers.java:147), org.apache.hadoop.hdfs.server.namenode.FSAclBaseTest.assertPermission(FSAclBaseTest.java:1823), org.apache.hadoop.hdfs.server.namenode.FSAclBaseTest.testDefaultAclNewFileIntermediate(FSAclBaseTest.java:1130), org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl.testDefaultAclNewFileIntermediate$$CONFUZZ(TestFileContextAcl.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl/testDefaultAclNewFileIntermediate/campaign/failures/debug_000004	{"dfs.namenode.posix.acl.inheritance.enabled": "false"}	["debug_000004"]											
Failed to set storage policy since dfs.storage.policy.enabled is set to false.	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.web.TestWebHDFS#testStoragePolicy	org.apache.hadoop.ipc.RemoteException	Failed to set storage policy since dfs.storage.policy.enabled is set to false.	org.apache.hadoop.hdfs.web.JsonUtilClient.toRemoteException(JsonUtilClient.java:89), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:522), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$300(WebHdfsFileSystem.java:145), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:739), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:814), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:637), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:675), java.base/java.security.AccessController.doPrivileged(Native Method), java.base/javax.security.auth.Subject.doAs(Subject.java:423), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:671), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.setStoragePolicy(WebHdfsFileSystem.java:2006), org.apache.hadoop.hdfs.web.TestWebHDFS.testStoragePolicy(TestWebHDFS.java:1640), org.apache.hadoop.hdfs.web.TestWebHDFS.testStoragePolicy$$CONFUZZ(TestWebHDFS.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.web.TestWebHDFS/testStoragePolicy/campaign/failures/debug_000004	{"dfs.storage.policy.enabled": "false"}	["debug_000004"]											
RPC response exceeds maximum data length	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.web.TestWebHDFS#testWebHdfsAllowandDisallowSnapshots	org.apache.hadoop.ipc.RpcException	RPC response exceeds maximum data length	org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936), org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238), org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.web.TestWebHDFS/testWebHdfsAllowandDisallowSnapshots/campaign/failures/debug_000004	{"dfs.namenode.safemode.min.datanodes": "7044", "ipc.maximum.response.length": "767"}	["debug_000004"]											
the connection is closed	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot#testDatanodeRestarts	java.io.EOFException		java.base/java.io.DataInputStream.readInt(DataInputStream.java:397), org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1922), org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238), org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot/testDatanodeRestarts/campaign/failures/debug_000007	{"ipc.server.max.connections": "1"}	["debug_000007"]											
	Bug-107	BUG	1	org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithReconstructFail	java.lang.IllegalArgumentException		java.base/java.util.concurrent.LinkedBlockingQueue.<init>(LinkedBlockingQueue.java:254), java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490), org.apache.hadoop.ipc.CallQueueManager.createCallQueueInstance(CallQueueManager.java:168), org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:81), org.apache.hadoop.ipc.Server.<init>(Server.java:3115), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371), org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:853), org.apache.hadoop.hdfs.server.datanode.DataNode.initIpcServer(DataNode.java:1027), org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424), org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507), org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2828), org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2734), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1755), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestFileChecksum.setup(TestFileChecksum.java:94), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestFileChecksum/testStripedFileChecksumWithReconstructFail/campaign/failures/debug_000001	{"ipc.server.handler.queue.size": "67141635", "dfs.datanode.handler.count": "1075364191"}	["debug_000001"]											
Shuai: I got a different error: java.io.IOException: Too many open files		Non-Reproducible	1	org.apache.hadoop.hdfs.server.namenode.ha.TestUpdateBlockTailing#testStandbyAppendBlock	org.apache.hadoop.hdfs.qjournal.client.QuorumException	Unable to check if JNs are ready for formatting. 1 successful responses:127.0.0.1:46595: false1 exceptions thrown:127.0.0.1:42335: Call From b3866d044c96/172.17.0.2 to localhost:42335 failed on socket timeout exception: java.net.SocketTimeoutException: 67 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:56114 remote=/127.0.0.1:42335]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout	org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81), org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:305), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.hasSomeData(QuorumJournalManager.java:282), org.apache.hadoop.hdfs.server.common.Storage.confirmFormat(Storage.java:1185), org.apache.hadoop.hdfs.server.namenode.FSImage.confirmFormat(FSImage.java:212), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1274), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:129), org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:38), org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:68), org.apache.hadoop.hdfs.server.namenode.ha.TestUpdateBlockTailing.startUpCluster(TestUpdateBlockTailing.java:77), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestUpdateBlockTailing/testStandbyAppendBlock/campaign/failures/debug_000000	{"ipc.server.read.threadpool.size": "1792", "ipc.client.rpc-timeout.ms": "67"}	["debug_000000"]											
	Bug-194	BUG	1	org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testWhenStoragePolicySetToONESSD	java.lang.NullPointerException		org.apache.hadoop.hdfs.MiniDFSCluster.setDataNodeStorageCapacities(MiniDFSCluster.java:1838), org.apache.hadoop.hdfs.MiniDFSCluster.setDataNodeStorageCapacities(MiniDFSCluster.java:1817), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1797), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.startCluster(TestExternalStoragePolicySatisfier.java:211), org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.createCluster(TestExternalStoragePolicySatisfier.java:194), org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.testWhenStoragePolicySetToONESSD(TestExternalStoragePolicySatisfier.java:480), org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.testWhenStoragePolicySetToONESSD$$CONFUZZ(TestExternalStoragePolicySatisfier.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier/testWhenStoragePolicySetToONESSD/campaign/failures/debug_000003	{"dfs.datanode.cache.revocation.polling.ms": "12623751"}	["debug_000003"]											
the value caused a fallback at line 1253 of DatanodeManager	Hardcode Assertion	FP	2	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoad	org.junit.ComparisonFailure	expected:<IP-[1-2]> but was:<IP-[0-1]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad(TestDatanodeManager.java:581), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderLoad/campaign/failures/debug_000001	{"net.topology.script.number.args": "0"}	["debug_000001"]	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageTypeAndLoad	org.junit.ComparisonFailure	expected:<IP-[1-2]> but was:<IP-[0-1]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad(TestDatanodeManager.java:822), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderStorageTypeAndLoad/campaign/failures/debug_000001	{"net.topology.script.number.args": "0"}	["debug_000001"]
	Bug-124	BUG	1	org.apache.hadoop.hdfs.TestDecommission#testNodeUsageAfterDecommissioned	java.lang.NullPointerException		org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned(TestDecommission.java:1498), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned$$CONFUZZ(TestDecommission.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommission/testNodeUsageAfterDecommissioned/campaign/failures/debug_000000	{"dfs.namenode.top.windows.minutes": "4182576,457,884"}	["debug_000000"]											
does not support KERBEROS	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDFSClientRetries#testClientDNProtocolTimeout	org.apache.hadoop.security.AccessControlException	Client cannot authenticate via:[KERBEROS]	org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:179), org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:392), org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:623), org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:414), org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:843), org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:839), java.base/java.security.AccessController.doPrivileged(Native Method), java.base/javax.security.auth.Subject.doAs(Subject.java:423), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878), org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:839), org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:414), org.apache.hadoop.ipc.Client.getConnection(Client.java:1677), org.apache.hadoop.ipc.Client.call(Client.java:1502), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy22.getReplicaVisibleLength(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolTranslatorPB.java:201), org.apache.hadoop.hdfs.TestDFSClientRetries.testClientDNProtocolTimeout(TestDFSClientRetries.java:894), org.apache.hadoop.hdfs.TestDFSClientRetries.testClientDNProtocolTimeout$$CONFUZZ(TestDFSClientRetries.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDFSClientRetries/testClientDNProtocolTimeout/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]											
the size is simply too big for the timeout	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestEpochsAreUnique#testSingleThreaded	java.lang.AssertionError	Time out while waiting for journal node 0 to start.	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.waitActive(MiniJournalCluster.java:262), org.apache.hadoop.hdfs.qjournal.client.TestEpochsAreUnique.testSingleThreaded(TestEpochsAreUnique.java:55), org.apache.hadoop.hdfs.qjournal.client.TestEpochsAreUnique.testSingleThreaded$$CONFUZZ(TestEpochsAreUnique.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestEpochsAreUnique/testSingleThreaded/campaign/failures/debug_000002	{"ipc.server.handler.queue.size": "268470015"}	["debug_000002"]											
	Bug-124	BUG	1	org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testNodeUsageWhileDecommissioining	java.lang.NullPointerException		org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageWhileDecommissioining(TestDecommission.java:1510), org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor.testNodeUsageWhileDecommissioining$$CONFUZZ(TestDecommissionWithBackoffMonitor.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor/testNodeUsageWhileDecommissioining/campaign/failures/debug_000000	{"dfs.datanode.ec.reconstruction.xmits.weight": "-0.17852377891540527"}	["debug_000000"]											
	Bug-167	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs#testNameEditsConfigsFailure	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs.testNameEditsConfigsFailure(TestNameEditsConfigs.java:450), org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs.testNameEditsConfigsFailure$$CONFUZZ(TestNameEditsConfigs.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs/testNameEditsConfigsFailure/campaign/failures/debug_000000	{"dfs.namenode.top.windows.minutes": "1546,39198698,26"}	["debug_000000"]											
Shuai: I can't reproduce this		Non-Reproducible	1	org.apache.hadoop.hdfs.server.datanode.TestBatchIbr#testIbr	org.apache.hadoop.ipc.RemoteException	File /dir/6753034785164449211_5 could only be written to 0 of the 1 minReplication nodes. There are 4 datanode(s) running and 4 node(s) are excluded in this operation.        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2315)        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2960)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:904)        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:593)        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)        at java.base/java.security.AccessController.doPrivileged(Native Method)        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy29.addBlock(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy32.addBlock(Unknown Source), org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088), org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915), org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717), org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.TestBatchIbr/testIbr/campaign/failures/debug_000001	{"dfs.encrypt.data.overwrite.downstream.derived.qop": "true"}	["debug_000001"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.server.datanode.TestDataXceiverBackwardsCompat#testBackwardsCompat	java.lang.IllegalArgumentException	dfs.datanode.max.transfer.threads should not be less than 1.	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.<init>(DataXceiverServer.java:191), org.apache.hadoop.hdfs.server.datanode.TestDataXceiverBackwardsCompat.testBackwardsCompat(TestDataXceiverBackwardsCompat.java:149), org.apache.hadoop.hdfs.server.datanode.TestDataXceiverBackwardsCompat.testBackwardsCompat$$CONFUZZ(TestDataXceiverBackwardsCompat.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.TestDataXceiverBackwardsCompat/testBackwardsCompat/campaign/failures/debug_000003	{"dfs.datanode.max.transfer.threads": "0"}	["debug_000003"]											
	Bug-102	BUG	1	org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testEncodedPathUrl	java.lang.IllegalArgumentException	timeouts can't be negative	java.base/sun.net.www.protocol.http.HttpURLConnection.setReadTimeout(HttpURLConnection.java:3349), org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.configure(SSLConnectionConfigurator.java:65), org.apache.hadoop.hdfs.web.URLConnectionFactory.openConnection(URLConnectionFactory.java:191), org.apache.hadoop.hdfs.web.URLConnectionFactory.openConnection(URLConnectionFactory.java:160), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:758), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:736), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:814), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:637), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:675), java.base/java.security.AccessController.doPrivileged(Native Method), java.base/javax.security.auth.Subject.doAs(Subject.java:423), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:671), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:1767), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getDelegationToken(WebHdfsFileSystem.java:377), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.getAuthParameters(WebHdfsFileSystem.java:598), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.toUrl(WebHdfsFileSystem.java:626), org.apache.hadoop.hdfs.web.TestWebHdfsUrl.testEncodedPathUrl(TestWebHdfsUrl.java:83), org.apache.hadoop.hdfs.web.TestWebHdfsUrl.testEncodedPathUrl$$CONFUZZ(TestWebHdfsUrl.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.web.TestWebHdfsUrl/testEncodedPathUrl/campaign/failures/debug_000004	{"dfs.webhdfs.socket.read-timeout": "6529429s", "hadoop.security.authentication": "kerberos"}	["debug_000004"]											
		Non-Reproducible	1	org.apache.hadoop.hdfs.TestFileStatus#testListStatusOnFile	java.io.IOException	Running in secure mode, but config doesn't have a keytab	org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:308), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1232), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestFileStatus.testSetUp(TestFileStatus.java:70), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestFileStatus/testListStatusOnFile/campaign/failures/debug_000002	{}	["debug_000002"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock#testAviodStaleAndSlowDatanodes	org.junit.ComparisonFailure	expected:<[4.4.4.4]> but was:<[3.3.3.3]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleAndSlowDatanodes(TestSortLocatedBlock.java:155), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleAndSlowDatanodes$$CONFUZZ(TestSortLocatedBlock.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock/testAviodStaleAndSlowDatanodes/campaign/failures/debug_000000	{"dfs.heartbeat.interval": "24111"}	["debug_000000", "debug_000001", "debug_000003"]											
timeout and too small bandwidth	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestParallelShortCircuitLegacyRead#testParallelReadByteBuffer	java.io.InterruptedIOException	No ack received after 6s and a timeout of 5s	org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:931), org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:778), org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:888), org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:847), org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77), org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106), org.apache.hadoop.hdfs.client.impl.BlockReaderTestUtil.writeFile(BlockReaderTestUtil.java:131), org.apache.hadoop.hdfs.TestParallelReadUtil.runParallelRead(TestParallelReadUtil.java:326), org.apache.hadoop.hdfs.TestParallelReadUtil.runTestWorkload(TestParallelReadUtil.java:382), org.apache.hadoop.hdfs.TestParallelReadUtil.testParallelReadByteBuffer(TestParallelReadUtil.java:411), org.apache.hadoop.hdfs.TestParallelShortCircuitLegacyRead.testParallelReadByteBuffer$$CONFUZZ(TestParallelShortCircuitLegacyRead.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestParallelShortCircuitLegacyRead/testParallelReadByteBuffer/campaign/failures/debug_000000	{"dfs.datanode.socket.write.timeout": "767", "dfs.datanode.data.write.bandwidthPerSec": "4538"}	["debug_000000"]											
permission turned off	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotRootAfterRemoval	java.lang.AssertionError	expected AccessControlException for user diana (auth:SIMPLE), path = /p5	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot.assertDirPermissionDenied(TestAclWithSnapshot.java:857), org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot.testOriginalAclEnforcedForSnapshotRootAfterRemoval(TestAclWithSnapshot.java:298), org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot.testOriginalAclEnforcedForSnapshotRootAfterRemoval$$CONFUZZ(TestAclWithSnapshot.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot/testOriginalAclEnforcedForSnapshotRootAfterRemoval/campaign/failures/debug_000004	{"dfs.permissions.enabled": "false"}	["debug_000004"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDFSShell#testCount	org.apache.hadoop.ipc.RemoteException	File has reached the limit on maximum number of blocks (dfs.namenode.fs-limits.max-blocks-per-file): 0 >= 0        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:186)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2948)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:904)        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:593)        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)        at java.base/java.security.AccessController.doPrivileged(Native Method)        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy30.addBlock(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy31.addBlock(Unknown Source), org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088), org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915), org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717), org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDFSShell/testCount/campaign/failures/debug_000003	{"dfs.namenode.fs-limits.max-blocks-per-file": "0"}	["debug_000003"]											
	invalid value	FP	2	org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl#testRemoveDefaultAclStickyBit	org.apache.hadoop.HadoopIllegalArgumentException	Invalid value configured for dfs.datanode.disk.check.timeout - 0 (should be > 0)	org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.<init>(StorageLocationChecker.java:98), org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2818), org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2734), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1755), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.FSAclBaseTest.startCluster(FSAclBaseTest.java:91), org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl.init(TestFileContextAcl.java:41), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl/testRemoveDefaultAclStickyBit/campaign/failures/debug_000006	{"dfs.datanode.disk.check.timeout": "0h"}	["debug_000006"]	org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks#testWithReplicationFactorAsOne	org.apache.hadoop.HadoopIllegalArgumentException	Invalid value configured for dfs.datanode.disk.check.timeout - 0 (should be > 0)	org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.<init>(StorageLocationChecker.java:98), org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2818), org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2734), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1755), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks.testWithReplicationFactorAsOne(TestProcessCorruptBlocks.java:168), org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks.testWithReplicationFactorAsOne$$CONFUZZ(TestProcessCorruptBlocks.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks/testWithReplicationFactorAsOne/campaign/failures/debug_000000	{"dfs.datanode.disk.check.timeout": "0m"}	["debug_000000"]
	Bug-196	BUG	2	org.apache.hadoop.hdfs.server.namenode.TestFavoredNodesEndToEnd#testWhenSomeNodesAreNotGood	java.lang.IllegalArgumentException		java.base/java.util.concurrent.LinkedBlockingQueue.<init>(LinkedBlockingQueue.java:254), java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490), org.apache.hadoop.ipc.CallQueueManager.createCallQueueInstance(CallQueueManager.java:168), org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:81), org.apache.hadoop.ipc.Server.<init>(Server.java:3115), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371), org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:853), org.apache.hadoop.hdfs.server.datanode.DataNode.initIpcServer(DataNode.java:1027), org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424), org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507), org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2828), org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2734), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1755), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.TestFavoredNodesEndToEnd.setUpBeforeClass(TestFavoredNodesEndToEnd.java:71), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestFavoredNodesEndToEnd/testWhenSomeNodesAreNotGood/campaign/failures/debug_000003	{"dfs.namenode.handler.count": "4631", "ipc.server.handler.queue.size": "304660495"}	["debug_000003"]	org.apache.hadoop.hdfs.TestDecommission#testDecommission	java.lang.IllegalArgumentException		java.base/java.util.concurrent.LinkedBlockingQueue.<init>(LinkedBlockingQueue.java:254), java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490), org.apache.hadoop.ipc.CallQueueManager.createCallQueueInstance(CallQueueManager.java:168), org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:81), org.apache.hadoop.ipc.Server.<init>(Server.java:3115), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371), org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:853), org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:466), org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:865), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:771), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.AdminStatesBaseTest.startCluster(AdminStatesBaseTest.java:414), org.apache.hadoop.hdfs.AdminStatesBaseTest.startCluster(AdminStatesBaseTest.java:392), org.apache.hadoop.hdfs.AdminStatesBaseTest.startCluster(AdminStatesBaseTest.java:424), org.apache.hadoop.hdfs.TestDecommission.testDecommission(TestDecommission.java:369), org.apache.hadoop.hdfs.TestDecommission.testDecommission(TestDecommission.java:184), org.apache.hadoop.hdfs.TestDecommission.testDecommission$$CONFUZZ(TestDecommission.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommission/testDecommission/campaign/failures/debug_000007	{"dfs.namenode.handler.count": "2560", "ipc.server.handler.queue.size": "1015551"}	["debug_000007"]
should throw permission exception	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDistributedFileSystem#testFileChecksum	java.lang.AssertionError		org.junit.Assert.fail(Assert.java:87), org.junit.Assert.fail(Assert.java:96), org.apache.hadoop.hdfs.TestDistributedFileSystem.testFileChecksum(TestDistributedFileSystem.java:1322), org.apache.hadoop.hdfs.TestDistributedFileSystem.testFileChecksum$$CONFUZZ(TestDistributedFileSystem.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDistributedFileSystem/testFileChecksum/campaign/failures/debug_000009	{"dfs.permissions.enabled": "false"}	["debug_000009"]											
similar to L8	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoad	org.junit.ComparisonFailure	expected:<IP-[1-3]> but was:<IP-[2-4]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad(TestDatanodeManager.java:578), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderLoad/campaign/failures/debug_000002	{"dfs.namenode.stale.datanode.minimum.interval": "928053426", "dfs.namenode.stale.datanode.interval": "13", "dfs.heartbeat.interval": "1922470578"}	["debug_000002", "debug_000001"]											
header size is too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testHttpServer	java.lang.AssertionError	expected:<200> but was:<431>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.hdfs.DFSTestUtil.urlGetBytes(DFSTestUtil.java:1003), org.apache.hadoop.hdfs.DFSTestUtil.urlGet(DFSTestUtil.java:993), org.apache.hadoop.hdfs.qjournal.server.TestJournalNode.testHttpServer(TestJournalNode.java:318), org.apache.hadoop.hdfs.qjournal.server.TestJournalNode.testHttpServer$$CONFUZZ(TestJournalNode.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.server.TestJournalNode/testHttpServer/campaign/failures/debug_000003	{"hadoop.http.max.request.header.size": "63"}	["debug_000003"]											
L37 of DirectoryDiffFactory, enabling skip interval enables skip list	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles	java.lang.AssertionError	expected:<2> but was:<1>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.hdfs.DFSTestUtil.verifySnapshotDiffReport(DFSTestUtil.java:2511), org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.verifyDiffReport(TestSnapshotDiffReport.java:197), org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithOpenFiles(TestSnapshotDiffReport.java:1106), org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithOpenFiles$$CONFUZZ(TestSnapshotDiffReport.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport/testDiffReportWithOpenFiles/campaign/failures/debug_000003	{"dfs.namenode.snapshot.skiplist.max.levels": "539008767"}	["debug_000003"]											
	Hardcode Assertion	FP	2	org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testNodeUsageWhileDecommissioining	java.lang.AssertionError		org.junit.Assert.fail(Assert.java:87), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertTrue(Assert.java:53), org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageWhileDecommissioining(TestDecommission.java:1510), org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor.testNodeUsageWhileDecommissioining$$CONFUZZ(TestDecommissionWithBackoffMonitor.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor/testNodeUsageWhileDecommissioining/campaign/failures/debug_000002	{"dfs.namenode.fs-limits.min-block-size": "2141261824"}	["debug_000002"]	org.apache.hadoop.hdfs.TestDecommission#testNodeUsageAfterDecommissioned	java.lang.AssertionError		org.junit.Assert.fail(Assert.java:87), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertTrue(Assert.java:53), org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned(TestDecommission.java:1498), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned$$CONFUZZ(TestDecommission.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommission/testNodeUsageAfterDecommissioned/campaign/failures/debug_000002	{"dfs.namenode.fs-limits.min-block-size": "8224"}	["debug_000002"]
	Bug-124	BUG	1	org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testNodeUsageWhileDecommissioining	java.lang.NullPointerException		org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageWhileDecommissioining(TestDecommission.java:1510), org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor.testNodeUsageWhileDecommissioining$$CONFUZZ(TestDecommissionWithBackoffMonitor.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor/testNodeUsageWhileDecommissioining/campaign/failures/debug_000001	{"dfs.namenode.replication.min": "687"}	["debug_000001"]											
ScriptBasedMapping.java:228 checks the value	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoadWithNodesOfSameDistance	java.lang.AssertionError	expected:<2> but was:<4>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoadWithNodesOfSameDistance(TestDatanodeManager.java:669), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoadWithNodesOfSameDistance$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderLoadWithNodesOfSameDistance/campaign/failures/debug_000001	{"net.topology.script.number.args": "0"}	["debug_000001"]											
	Bug-197	BUG	1	org.apache.hadoop.hdfs.TestDFSShell#testRemoteException	java.lang.AssertionError	permission denied printed	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.hdfs.TestDFSShell$8.run(TestDFSShell.java:1948), java.base/java.security.AccessController.doPrivileged(Native Method), java.base/javax.security.auth.Subject.doAs(Subject.java:423), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878), org.apache.hadoop.hdfs.TestDFSShell.testRemoteException(TestDFSShell.java:1935), org.apache.hadoop.hdfs.TestDFSShell.testRemoteException$$CONFUZZ(TestDFSShell.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDFSShell/testRemoteException/campaign/failures/debug_000001	{"ipc.maximum.response.length": "575"}	["debug_000001"]											
	Bug-12	BUG	1	org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade#testDatanodeRollingUpgradeWithRollback	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.ipc.Server.start(Server.java:3418), org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.start(NameNodeRpcServer.java:579), org.apache.hadoop.hdfs.server.namenode.NameNode.startCommonServices(NameNode.java:880), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:791), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.startCluster(TestDataNodeRollingUpgrade.java:77), org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.testDatanodeRollingUpgradeWithRollback(TestDataNodeRollingUpgrade.java:263), org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.testDatanodeRollingUpgradeWithRollback$$CONFUZZ(TestDataNodeRollingUpgrade.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade/testDatanodeRollingUpgradeWithRollback/campaign/failures/debug_000002	{"dfs.namenode.blocks.per.postponedblocks.rescan": "237418903", "dfs.namenode.handler.count": "1214110464"}	["debug_000002"]											
	Bug-198	BUG	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOutOfSyncAtBeginningOfSegment2	java.lang.IllegalArgumentException		java.base/java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1293), java.base/java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:1215), org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel.createParallelExecutor(IPCLoggerChannel.java:283), org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel.<init>(IPCLoggerChannel.java:189), org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$1.createLogger(IPCLoggerChannel.java:161), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.createLoggers(QuorumJournalManager.java:424), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.createLoggers(QuorumJournalManager.java:199), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.<init>(QuorumJournalManager.java:147), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.<init>(QuorumJournalManager.java:121), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster$1.get(MiniJournalCluster.java:251), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster$1.get(MiniJournalCluster.java:245), org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:449), org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:421), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.waitActive(MiniJournalCluster.java:245), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.setup(TestQuorumJournalManager.java:112), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testOutOfSyncAtBeginningOfSegment2/campaign/failures/debug_000003	{"dfs.qjournal.parallel-read.num-threads": "0"}	["debug_000003"]											
timeout is too small, 73ms and cause FP	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.TestMalformedURLs#testTryStartingCluster	org.apache.hadoop.util.DiskChecker$DiskErrorException	Too many failed volumes - current valid volumes: 0, volumes configured: 2, volumes failed: 2, volume failures tolerated: 0	org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.check(StorageLocationChecker.java:233), org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2821), org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2734), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1755), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.TestMalformedURLs.testTryStartingCluster(TestMalformedURLs.java:49), org.apache.hadoop.hdfs.server.namenode.TestMalformedURLs.testTryStartingCluster$$CONFUZZ(TestMalformedURLs.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestMalformedURLs/testTryStartingCluster/campaign/failures/debug_000005	{"dfs.datanode.disk.check.timeout": "73ms", "dfs.namenode.gc.time.monitor.enable": "false", "ipc.server.read.threadpool.size": "18674", "dfs.namenode.edits.asynclogging": "true", "dfs.bytes-per-checksum": "27806", "dfs.client.failover.sleep.base.millis": "1367352593", "dfs.permissions.superusergroup": "vcdqliqztzxwilpmogcrmezpgsftmhdculyvzhiotcxfmidvdhnprllmtkdvrxuezfonqbnxmgroup", "dfs.datanode.fsdatasetasyncdisk.max.threads.per.volume": "3074"}	["debug_000005"]											
timeout too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testReaderWhileAnotherWrites	java.io.IOException	Timed out waiting 3ms for a quorum of nodes to respond.	org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:138), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.selectStreamingInputStreams(QuorumJournalManager.java:619), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.selectInputStreams(QuorumJournalManager.java:535), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.selectInputStreams(QuorumJournalManager.java:510), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testReaderWhileAnotherWrites(TestQuorumJournalManager.java:174), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testReaderWhileAnotherWrites$$CONFUZZ(TestQuorumJournalManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testReaderWhileAnotherWrites/campaign/failures/debug_000006	{"dfs.qjournal.select-input-streams.timeout.ms": "3"}	["debug_000006"]											
	Bug-167	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs#testNameEditsConfigsFailure	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs.testNameEditsConfigsFailure(TestNameEditsConfigs.java:450), org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs.testNameEditsConfigsFailure$$CONFUZZ(TestNameEditsConfigs.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs/testNameEditsConfigsFailure/campaign/failures/debug_000000	{"dfs.datanode.balance.bandwidthPerSec": "153358982h"}	["debug_000000"]											
not allow anonymous auth	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashBetweenSyncLogAndPersistPaxosData	org.apache.hadoop.hdfs.qjournal.client.QuorumException	Got too many exceptions to achieve quorum size 2/3. 1 successful responses:127.0.0.1:41623: null [success]2 exceptions thrown:127.0.0.1:35317: Call From 596a60c918df/172.17.0.2 to localhost:35317 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused127.0.0.1:38681: Image transfer servlet at http://localhost:39195/getJournal?jid=test-journal&segmentTxId=1&storageInfo=-66%3A12345%3A0%3Amycluster&inProgressOk=true failed with status code 401Response message:Authentication required        at org.apache.hadoop.hdfs.server.common.Util.doGetUrl(Util.java:168)        at org.apache.hadoop.hdfs.server.namenode.TransferFsImage.doGetUrl(TransferFsImage.java:441)        at org.apache.hadoop.hdfs.qjournal.server.Journal$1.run(Journal.java:1007)        at org.apache.hadoop.hdfs.qjournal.server.Journal$1.run(Journal.java:997)        at java.base/java.security.AccessController.doPrivileged(Native Method)        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)        at org.apache.hadoop.security.SecurityUtil.doAsUser(SecurityUtil.java:536)        at org.apache.hadoop.security.SecurityUtil.doAsLoginUser(SecurityUtil.java:517)        at org.apache.hadoop.hdfs.qjournal.server.Journal.syncLog(Journal.java:996)        at org.apache.hadoop.hdfs.qjournal.server.Journal.acceptRecovery(Journal.java:935)        at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.acceptRecovery(JournalNodeRpcServer.java:257)        at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.acceptRecovery(QJournalProtocolServerSideTranslatorPB.java:268)        at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:31940)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)        at java.base/java.security.AccessController.doPrivileged(Native Method)        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81), org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:305), org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:143), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.recoverUnclosedSegment(QuorumJournalManager.java:395), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.recoverUnfinalizedSegments(QuorumJournalManager.java:497), org.apache.hadoop.hdfs.qjournal.QJMTestUtil.recoverAndReturnLastTxn(QJMTestUtil.java:164), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testCrashBetweenSyncLogAndPersistPaxosData(TestQuorumJournalManager.java:857), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testCrashBetweenSyncLogAndPersistPaxosData$$CONFUZZ(TestQuorumJournalManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testCrashBetweenSyncLogAndPersistPaxosData/campaign/failures/debug_000003	{"hadoop.http.authentication.simple.anonymous.allowed": "false"}	["debug_000003"]											
max txn too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testInProgressRecovery	java.lang.AssertionError	Expected to find txid 2, but no more streams available to read from	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.hdfs.qjournal.QJMTestUtil.verifyEdits(QJMTestUtil.java:132), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testInProgressRecovery(TestQuorumJournalManager.java:974), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testInProgressRecovery$$CONFUZZ(TestQuorumJournalManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testInProgressRecovery/campaign/failures/debug_000001	{"dfs.ha.tail-edits.qjm.rpc.max-txns": "1"}	["debug_000001"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations#testGetBlockLocationsRacingWithDelete	java.io.IOException	Unexpected configuration parameters: dfs.namenode.maintenance.replication.min = 1 > dfs.replication = 0	org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:580), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.setupFileSystem(TestGetBlockLocations.java:135), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testGetBlockLocationsRacingWithDelete(TestGetBlockLocations.java:65), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testGetBlockLocationsRacingWithDelete$$CONFUZZ(TestGetBlockLocations.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations/testGetBlockLocationsRacingWithDelete/campaign/failures/debug_000001	{"dfs.replication": "0"}	["debug_000001"]											
timeout too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsInSync	java.io.IOException	Timed out waiting 3ms for a quorum of nodes to respond.	org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:138), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.createNewUniqueEpoch(QuorumJournalManager.java:233), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.recoverUnfinalizedSegments(QuorumJournalManager.java:478), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testChangeWritersLogsInSync(TestQuorumJournalManager.java:411), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testChangeWritersLogsInSync$$CONFUZZ(TestQuorumJournalManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testChangeWritersLogsInSync/campaign/failures/debug_000002	{"dfs.qjournal.get-journal-state.timeout.ms": "3"}	["debug_000002"]											
header too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsInSync	java.lang.AssertionError	Time out while waiting for journal node 0 to start.	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.waitActive(MiniJournalCluster.java:262), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.setup(TestQuorumJournalManager.java:112), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testChangeWritersLogsInSync/campaign/failures/debug_000003	{"ipc.maximum.response.length": "15"}	["debug_000003"]											
	Bug-59	BUG	1	org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock#testAviodStaleAndSlowDatanodes	java.lang.NullPointerException		java.base/java.util.Comparator.lambda$comparing$77a9974f$1(Comparator.java:469), java.base/java.util.TimSort.countRunAndMakeAscending(TimSort.java:355), java.base/java.util.TimSort.sort(TimSort.java:220), java.base/java.util.Arrays.sort(Arrays.java:1515), java.base/java.util.ArrayList.sort(ArrayList.java:1750), java.base/java.util.Collections.sort(Collections.java:179), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.lambda$createSecondaryNodeSorter$0(DatanodeManager.java:654), org.apache.hadoop.net.NetworkTopology.sortByDistance(NetworkTopology.java:983), org.apache.hadoop.net.NetworkTopology.sortByDistanceUsingNetworkLocation(NetworkTopology.java:946), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlock(DatanodeManager.java:637), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:554), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleAndSlowDatanodes(TestSortLocatedBlock.java:144), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleAndSlowDatanodes$$CONFUZZ(TestSortLocatedBlock.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock/testAviodStaleAndSlowDatanodes/campaign/failures/debug_000003	{"dfs.namenode.stale.datanode.minimum.interval": "34561", "dfs.namenode.read.considerStorageType": "true"}	["debug_000003"]											
	Bug-124	BUG	1	org.apache.hadoop.hdfs.TestDecommission#testNodeUsageAfterDecommissioned	java.lang.NullPointerException		org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned(TestDecommission.java:1498), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned$$CONFUZZ(TestDecommission.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommission/testNodeUsageAfterDecommissioned/campaign/failures/debug_000000	{"dfs.namenode.resource.checked.volumes.minimum": "34559"}	["debug_000000"]											
	Bug-10,Bug-11	BUG	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashAtBeginningOfSegment	java.lang.OutOfMemoryError	Java heap space	org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534), org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:511), org.apache.hadoop.hdfs.qjournal.server.JournalNodeHttpServer.start(JournalNodeHttpServer.java:81), org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:238), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.<init>(MiniJournalCluster.java:120), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.<init>(MiniJournalCluster.java:47), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster$Builder.build(MiniJournalCluster.java:79), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.setup(TestQuorumJournalManager.java:111), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testCrashAtBeginningOfSegment/campaign/failures/debug_000000	{"hadoop.http.acceptor.count": "627849093", "hadoop.http.selector.count": "1227789720"}	["debug_000000"]											
Mockito parameter is fixed	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcWithoutDurableTransactions	org.mockito.exceptions.verification.junit.ArgumentsAreDifferent	Argument(s) are different! Wanted:iPCLoggerChannel.getJournaledEdits(    1L,    5000);-> at org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testSelectViaRpcWithoutDurableTransactions(TestQuorumJournalManager.java:1014)Actual invocations have different arguments:iPCLoggerChannel.format(    lv=-66;cid=mycluster;nsid=12345;c=0;bpid=my-bp,    false);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.format(AsyncLoggerSet.java:321)iPCLoggerChannel.getProxy(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$9.call(IPCLoggerChannel.java:519)iPCLoggerChannel.createProxy(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel.getProxy(IPCLoggerChannel.java:224)iPCLoggerChannel.getJournalState(    );-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.getJournalState(AsyncLoggerSet.java:212)iPCLoggerChannel.getProxy(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$5.call(IPCLoggerChannel.java:360)iPCLoggerChannel.newEpoch(    1L);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.newEpoch(AsyncLoggerSet.java:231)iPCLoggerChannel.getProxy(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$6.call(IPCLoggerChannel.java:373)iPCLoggerChannel.setEpoch(    1L);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.setEpoch(AsyncLoggerSet.java:66)iPCLoggerChannel.startLogSegment(    1L,    -66);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.startLogSegment(AsyncLoggerSet.java:240)iPCLoggerChannel.getProxy(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$10.call(IPCLoggerChannel.java:531)iPCLoggerChannel.sendEdits(    1L,    1L,    3,    [(byte) 0x03, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x44, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x01, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x04, (byte) 0x74, (byte) 0x78, (byte) 0x20, (byte) 0x31, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x08, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x73, (byte) 0x65, (byte) 0x72, (byte) 0x09, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x67, (byte) 0x72, (byte) 0x6F, (byte) 0x75, (byte) 0x70, (byte) 0x01, (byte) 0xFF, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x5E, (byte) 0xAF, (byte) 0xA6, (byte) 0xA0, (byte) 0x03, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x44, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x02, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x04, (byte) 0x74, (byte) 0x78, (byte) 0x20, (byte) 0x32, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x08, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x73, (byte) 0x65, (byte) 0x72, (byte) 0x09, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x67, (byte) 0x72, (byte) 0x6F, (byte) 0x75, (byte) 0x70, (byte) 0x01, (byte) 0xFF, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0xBE, (byte) 0x64, (byte) 0xCB, (byte) 0x01, (byte) 0x03, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x44, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x03, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x04, (byte) 0x74, (byte) 0x78, (byte) 0x20, (byte) 0x33, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x08, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x73, (byte) 0x65, (byte) 0x72, (byte) 0x09, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x67, (byte) 0x72, (byte) 0x6F, (byte) 0x75, (byte) 0x70, (byte) 0x01, (byte) 0xFF, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0xE1, (byte) 0xDD, (byte) 0xEF, (byte) 0x9E]);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.sendEdits(AsyncLoggerSet.java:259)iPCLoggerChannel.isOutOfSync(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel.throwIfOutOfSync(IPCLoggerChannel.java:462)iPCLoggerChannel.getProxy(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:401)iPCLoggerChannel.setCommittedTxId(    3L);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.setCommittedTxId(AsyncLoggerSet.java:77)iPCLoggerChannel.sendEdits(    1L,    4L,    1,    [(byte) 0x03, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x44, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x04, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x04, (byte) 0x74, (byte) 0x78, (byte) 0x20, (byte) 0x34, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x08, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x73, (byte) 0x65, (byte) 0x72, (byte) 0x09, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x67, (byte) 0x72, (byte) 0x6F, (byte) 0x75, (byte) 0x70, (byte) 0x01, (byte) 0xFF, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0xA4, (byte) 0x83, (byte) 0x16, (byte) 0x02]);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.sendEdits(AsyncLoggerSet.java:259)iPCLoggerChannel.setCommittedTxId(    4L);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.setCommittedTxId(AsyncLoggerSet.java:77)iPCLoggerChannel.sendEdits(    1L,    5L,    1,    [(byte) 0x03, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x44, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x05, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x04, (byte) 0x74, (byte) 0x78, (byte) 0x20, (byte) 0x35, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x08, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x75, (byte) 0x73, (byte) 0x65, (byte) 0x72, (byte) 0x09, (byte) 0x74, (byte) 0x65, (byte) 0x73, (byte) 0x74, (byte) 0x67, (byte) 0x72, (byte) 0x6F, (byte) 0x75, (byte) 0x70, (byte) 0x01, (byte) 0xFF, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0x00, (byte) 0xFB, (byte) 0x3A, (byte) 0x32, (byte) 0x9D]);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.sendEdits(AsyncLoggerSet.java:259)iPCLoggerChannel.isOutOfSync(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel.throwIfOutOfSync(IPCLoggerChannel.java:462)iPCLoggerChannel.getProxy(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$7.call(IPCLoggerChannel.java:401)iPCLoggerChannel.getJournaledEdits(    1L,    18208);-> at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.getJournaledEdits(AsyncLoggerSet.java:272)iPCLoggerChannel.getProxy(    );-> at org.apache.hadoop.hdfs.qjournal.client.IPCLoggerChannel$13.call(IPCLoggerChannel.java:578)	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testSelectViaRpcWithoutDurableTransactions(TestQuorumJournalManager.java:1014), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testSelectViaRpcWithoutDurableTransactions$$CONFUZZ(TestQuorumJournalManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testSelectViaRpcWithoutDurableTransactions/campaign/failures/debug_000002	{"dfs.ha.tail-edits.qjm.rpc.max-txns": "18208"}	["debug_000002"]											
Authentication	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA#testRollbackWithNfs	org.apache.hadoop.hdfs.server.common.HttpGetFailedException	Image transfer servlet at http://localhost:39083/imagetransfer?getimage=1&txid=3&storageInfo=-66:378155989:1690653392623:testClusterID&bootstrapstandby=true failed with status code 401Response message:Authentication required	org.apache.hadoop.hdfs.server.common.Util.doGetUrl(Util.java:168), org.apache.hadoop.hdfs.server.namenode.TransferFsImage.doGetUrl(TransferFsImage.java:441), org.apache.hadoop.hdfs.server.namenode.TransferFsImage.getFileClient(TransferFsImage.java:436), org.apache.hadoop.hdfs.server.namenode.TransferFsImage.downloadImageToStorage(TransferFsImage.java:123), org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.downloadImage(BootstrapStandby.java:357), org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.doRun(BootstrapStandby.java:239), org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.access$000(BootstrapStandby.java:82), org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$1.run(BootstrapStandby.java:125), org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby$1.run(BootstrapStandby.java:121), org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:503), org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.run(BootstrapStandby.java:121), org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:81), org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:95), org.apache.hadoop.hdfs.server.namenode.ha.BootstrapStandby.run(BootstrapStandby.java:544), org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithNfs(TestDFSUpgradeWithHA.java:589), org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA.testRollbackWithNfs$$CONFUZZ(TestDFSUpgradeWithHA.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA/testRollbackWithNfs/campaign/failures/debug_000003	{"hadoop.http.authentication.simple.anonymous.allowed": "false"}	["debug_000003"]											
	Bug-33	BUG	1	org.apache.hadoop.hdfs.TestDecommission#testDecommissionFederation	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.ipc.Server.start(Server.java:3418), org.apache.hadoop.hdfs.server.datanode.DataNode.runDatanodeDaemon(DataNode.java:2686), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1789), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.AdminStatesBaseTest.startCluster(AdminStatesBaseTest.java:414), org.apache.hadoop.hdfs.AdminStatesBaseTest.startCluster(AdminStatesBaseTest.java:392), org.apache.hadoop.hdfs.AdminStatesBaseTest.startCluster(AdminStatesBaseTest.java:424), org.apache.hadoop.hdfs.TestDecommission.testDecommission(TestDecommission.java:369), org.apache.hadoop.hdfs.TestDecommission.testDecommissionFederation(TestDecommission.java:243), org.apache.hadoop.hdfs.TestDecommission.testDecommissionFederation$$CONFUZZ(TestDecommission.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommission/testDecommissionFederation/campaign/failures/debug_000004	{"ipc.server.handler.queue.size": "21090", "dfs.datanode.handler.count": "1610711359"}	["debug_000004"]											
checked number too large	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDFSMkdirs#testMkdirRpcNonCanonicalPath	org.apache.hadoop.hdfs.server.namenode.SafeModeException	Cannot create directory //test1. Name node is in safe mode.Resources are low on NN. Please add or free up more resourcesthen turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the NN will immediately return to safe mode. Use "hdfs dfsadmin -safemode leave" to turn safe mode off. NamenodeHostName:localhost	org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException(FSNamesystem.java:1577), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkNameNodeSafeMode(FSNamesystem.java:1564), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:3404), org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1159), org.apache.hadoop.hdfs.TestDFSMkdirs.testMkdirRpcNonCanonicalPath(TestDFSMkdirs.java:144), org.apache.hadoop.hdfs.TestDFSMkdirs.testMkdirRpcNonCanonicalPath$$CONFUZZ(TestDFSMkdirs.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDFSMkdirs/testMkdirRpcNonCanonicalPath/campaign/failures/debug_000003	{"dfs.namenode.resource.checked.volumes.minimum": "7268"}	["debug_000003"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.TestTrashWithSecureEncryptionZones#testTrashExpunge	java.lang.IllegalArgumentException	dfs.namenode.reencrypt.throttle.limit.handler.ratio is not positive.	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.<init>(ReencryptionHandler.java:215), org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager.<init>(EncryptionZoneManager.java:250), org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:411), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:977), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestTrashWithSecureEncryptionZones.init(TestTrashWithSecureEncryptionZones.java:214), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestTrashWithSecureEncryptionZones/testTrashExpunge/campaign/failures/debug_000010	{"dfs.namenode.reencrypt.throttle.limit.handler.ratio": "0.0"}	["debug_000010"]											
max op size too small	Hardcode Assertion	FP	2	org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testEnterSafeModeInANNShouldNotThrowNPE	java.io.IOException	Op 24 has size 17, but maxOpSize = 7	org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LengthPrefixedReader.decodeOpFrame(FSEditLogOp.java:5253), org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LengthPrefixedReader.decodeOp(FSEditLogOp.java:5198), org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Reader.readOp(FSEditLogOp.java:5071), org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.nextOpImpl(EditLogFileInputStream.java:229), org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.nextOp(EditLogFileInputStream.java:276), org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85), org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:201), org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85), org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:243), org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:182), org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:915), org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:762), org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:339), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1201), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:779), org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:681), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:768), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.restartNameNode(MiniDFSCluster.java:2246), org.apache.hadoop.hdfs.MiniDFSCluster.restartNameNode(MiniDFSCluster.java:2227), org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode.restartActive(TestHASafeMode.java:252), org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode.testEnterSafeModeInANNShouldNotThrowNPE(TestHASafeMode.java:198), org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode.testEnterSafeModeInANNShouldNotThrowNPE$$CONFUZZ(TestHASafeMode.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode/testEnterSafeModeInANNShouldNotThrowNPE/campaign/failures/debug_000004	{"dfs.namenode.max.op.size": "7"}	["debug_000004"]	org.apache.hadoop.hdfs.server.namenode.snapshot.TestXAttrWithSnapshot#testXAttrForSnapshotRootAfterRemove	java.io.IOException	Op 3 has size 69, but maxOpSize = 64	org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LengthPrefixedReader.decodeOpFrame(FSEditLogOp.java:5253), org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$LengthPrefixedReader.decodeOp(FSEditLogOp.java:5198), org.apache.hadoop.hdfs.server.namenode.FSEditLogOp$Reader.readOp(FSEditLogOp.java:5071), org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.nextOpImpl(EditLogFileInputStream.java:229), org.apache.hadoop.hdfs.server.namenode.EditLogFileInputStream.nextOp(EditLogFileInputStream.java:276), org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85), org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream.nextOp(RedundantEditLogInputStream.java:201), org.apache.hadoop.hdfs.server.namenode.EditLogInputStream.readOp(EditLogInputStream.java:85), org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:243), org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:182), org.apache.hadoop.hdfs.server.namenode.FSImage.loadEdits(FSImage.java:915), org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:762), org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:339), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1201), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:779), org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:681), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:768), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.snapshot.TestXAttrWithSnapshot.initCluster(TestXAttrWithSnapshot.java:401), org.apache.hadoop.hdfs.server.namenode.snapshot.TestXAttrWithSnapshot.restart(TestXAttrWithSnapshot.java:419), org.apache.hadoop.hdfs.server.namenode.snapshot.TestXAttrWithSnapshot.testXAttrForSnapshotRootAfterRemove(TestXAttrWithSnapshot.java:276), org.apache.hadoop.hdfs.server.namenode.snapshot.TestXAttrWithSnapshot.testXAttrForSnapshotRootAfterRemove$$CONFUZZ(TestXAttrWithSnapshot.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.snapshot.TestXAttrWithSnapshot/testXAttrForSnapshotRootAfterRemove/campaign/failures/debug_000004	{"dfs.namenode.max.op.size": "64"}	["debug_000004"]
token expired	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens#testSaveNamespace	java.lang.AssertionError	Could not renew or cancel the token	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens.testSaveNamespace(TestCheckPointForSecurityTokens.java:125), org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens.testSaveNamespace$$CONFUZZ(TestCheckPointForSecurityTokens.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens/testSaveNamespace/campaign/failures/debug_000009	{"dfs.namenode.delegation.token.max-lifetime": "1056"}	["debug_000009"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles	java.io.IOException	Unable to close file because the last block BP-217263406-172.17.0.2-1690651587942:blk_1073741826_1002 does not have enough number of replicas.	org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:969), org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:909), org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:892), org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:847), org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77), org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106), org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:489), org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:447), org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:440), org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.createFile(TestSnapshotDiffReport.java:1032), org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithOpenFiles(TestSnapshotDiffReport.java:1068), org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport.testDiffReportWithOpenFiles$$CONFUZZ(TestSnapshotDiffReport.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport/testDiffReportWithOpenFiles/campaign/failures/debug_000005	{"dfs.client.block.write.locateFollowingBlock.initial.delay.ms": "3", "dfs.blockreport.incremental.intervalMsec": "1025"}	["debug_000005"]											
group is gibberish	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.web.TestWebHDFSAcl#testRemoveAclEntriesMustBeOwnerOrSuper	org.apache.hadoop.ipc.RemoteException	Permission denied. user=super is not the owner of inode=/p5/bruce/file	org.apache.hadoop.hdfs.web.JsonUtilClient.toRemoteException(JsonUtilClient.java:89), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:522), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$300(WebHdfsFileSystem.java:145), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:739), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:814), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:637), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:675), java.base/java.security.AccessController.doPrivileged(Native Method), java.base/javax.security.auth.Subject.doAs(Subject.java:423), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:671), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.removeAclEntries(WebHdfsFileSystem.java:1307), org.apache.hadoop.hdfs.server.namenode.FSAclBaseTest.testRemoveAclEntriesMustBeOwnerOrSuper(FSAclBaseTest.java:1347), org.apache.hadoop.hdfs.web.TestWebHDFSAcl.testRemoveAclEntriesMustBeOwnerOrSuper$$CONFUZZ(TestWebHDFSAcl.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.web.TestWebHDFSAcl/testRemoveAclEntriesMustBeOwnerOrSuper/campaign/failures/debug_000002	{"dfs.permissions.superusergroup": "zndvtvcahnksnoaasttdnnomjipjrrffuivqjcsioiomxpadfleltwpknnbqluhqbaakixoqcvqxgroup"}	["debug_000002"]											
expired and also without cache, nothing code can do	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS#testBasicOperations	org.apache.hadoop.ipc.RemoteException	java.util.concurrent.ExecutionException: java.net.SocketTimeoutException: Read timed out        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.warmUpEncryptedKeys(KMSClientProvider.java:953)        at org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.warmUpEncryptedKeys(LoadBalancingKMSClientProvider.java:295)        at org.apache.hadoop.crypto.key.KeyProviderCryptoExtension.warmUpEncryptedKeys(KeyProviderCryptoExtension.java:493)        at org.apache.hadoop.hdfs.server.namenode.FSDirEncryptionZoneOp.ensureKeyIsInitialized(FSDirEncryptionZoneOp.java:138)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createEncryptionZone(FSNamesystem.java:7779)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.createEncryptionZone(NameNodeRpcServer.java:2202)        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.createEncryptionZone(ClientNamenodeProtocolServerSideTranslatorPB.java:1587)        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)        at java.base/java.security.AccessController.doPrivileged(Native Method)        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)Caused by: java.util.concurrent.ExecutionException: java.net.SocketTimeoutException: Read timed out        at org.apache.hadoop.thirdparty.com.google.common.util.concurrent.AbstractFuture.getDoneValue(AbstractFuture.java:566)        at org.apache.hadoop.thirdparty.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:527)        at org.apache.hadoop.thirdparty.com.google.common.util.concurrent.AbstractFuture$TrustedFuture.get(AbstractFuture.java:104)        at org.apache.hadoop.thirdparty.com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:240)        at org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2313)        at org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2279)        at org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2155)        at org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2045)        at org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.get(LocalCache.java:3962)        at org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3985)        at org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4946)        at org.apache.hadoop.crypto.key.kms.ValueQueue.initializeQueuesForKeys(ValueQueue.java:276)        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.warmUpEncryptedKeys(KMSClientProvider.java:951)        ... 17 moreCaused by: java.net.SocketTimeoutException: Read timed out        at java.base/java.net.SocketInputStream.socketRead0(Native Method)        at java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)        at java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)        at java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)        at java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)        at java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:292)        at java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:351)        at java.base/sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:787)        at java.base/sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:722)        at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1615)        at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1520)        at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:564)        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.call(KMSClientProvider.java:540)        at org.apache.hadoop.crypto.key.kms.KMSClientProvider.access$200(KMSClientProvider.java:97)        at org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller.fillQueueForKey(KMSClientProvider.java:155)        at org.apache.hadoop.crypto.key.kms.ValueQueue$1.load(ValueQueue.java:249)        at org.apache.hadoop.crypto.key.kms.ValueQueue$1.load(ValueQueue.java:243)        at org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3529)        at org.apache.hadoop.thirdparty.com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2278)        ... 24 more	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy79.createEncryptionZone(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.createEncryptionZone(ClientNamenodeProtocolTranslatorPB.java:1606), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy80.createEncryptionZone(Unknown Source), org.apache.hadoop.hdfs.DFSClient.createEncryptionZone(DFSClient.java:2782), org.apache.hadoop.hdfs.DistributedFileSystem$53.doCall(DistributedFileSystem.java:2720), org.apache.hadoop.hdfs.DistributedFileSystem$53.doCall(DistributedFileSystem.java:2717), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.createEncryptionZone(DistributedFileSystem.java:2737), org.apache.hadoop.hdfs.client.HdfsAdmin.createEncryptionZone(HdfsAdmin.java:325), org.apache.hadoop.hdfs.TestEncryptionZones.testBasicOperations(TestEncryptionZones.java:446), org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS.testBasicOperations$$CONFUZZ(TestEncryptionZonesWithKMS.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS/testBasicOperations/campaign/failures/debug_000004	{"hadoop.security.kms.encrypted.key.cache.expiry": "1008", "hadoop.kms.cache.enable": "false"}	["debug_000004"]											
	Bug-123	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA#testHarUriWithHaUriWithNoPort	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA.testHarUriWithHaUriWithNoPort(TestHarFileSystemWithHA.java:60), org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA.testHarUriWithHaUriWithNoPort$$CONFUZZ(TestHarFileSystemWithHA.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA/testHarUriWithHaUriWithNoPort/campaign/failures/debug_000000	{"dfs.namenode.edits.dir": "file:/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-3,file:/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-4"}	["debug_000000"]											
	Bug-205	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor#testDecommissionStatusAfterDNRestart	java.lang.ArrayIndexOutOfBoundsException	Index 1 out of bounds for length 1	org.apache.hadoop.hdfs.util.HostsFileWriter.initOutOfServiceHosts(HostsFileWriter.java:110), org.apache.hadoop.hdfs.util.HostsFileWriter.initExcludeHosts(HostsFileWriter.java:87), org.apache.hadoop.hdfs.util.HostsFileWriter.initExcludeHost(HostsFileWriter.java:82), org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus.testDecommissionStatusAfterDNRestart(TestDecommissioningStatus.java:463), org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor.testDecommissionStatusAfterDNRestart$$CONFUZZ(TestDecommissioningStatusWithBackoffMonitor.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor/testDecommissionStatusAfterDNRestart/campaign/failures/debug_000007	{"dfs.namenode.hosts.provider.classname": "org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager"}	["debug_000007"]											
value of 9,223,372,036,854,775,80 can trigger a bug - BUG-238, but 35 is FP	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDecommission#testDecommission	java.lang.AssertionError	Checked if block was replicated after decommission, tried 21 times.	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.hdfs.TestDecommission.testDecommission(TestDecommission.java:414), org.apache.hadoop.hdfs.TestDecommission.testDecommission(TestDecommission.java:184), org.apache.hadoop.hdfs.TestDecommission.testDecommission$$CONFUZZ(TestDecommission.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommission/testDecommission/campaign/failures/debug_000004	{"dfs.datanode.data.transfer.bandwidthPerSec": "35"}	["debug_000004"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testComplexFailoverIntoSafemode	java.lang.AssertionError	Bad safemode status: 'Safe mode is ON. The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The minimum number of live datanodes is not required. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.'	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode.assertSafeMode(TestHASafeMode.java:514), org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode.testComplexFailoverIntoSafemode(TestHASafeMode.java:563), org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode.testComplexFailoverIntoSafemode$$CONFUZZ(TestHASafeMode.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode/testComplexFailoverIntoSafemode/campaign/failures/debug_000008	{"dfs.ha.tail-edits.in-progress": "true"}	["debug_000008"]											
polling time too long	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDatanodeConfig#testDataDirectories	java.lang.AssertionError	Data-node should startup.	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.hdfs.TestDatanodeConfig.testDataDirectories(TestDatanodeConfig.java:109), org.apache.hadoop.hdfs.TestDatanodeConfig.testDataDirectories$$CONFUZZ(TestDatanodeConfig.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDatanodeConfig/testDataDirectories/campaign/failures/debug_000002	{"dfs.datanode.cache.revocation.polling.ms": "1853214028"}	["debug_000002"]											
replication too large	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDecommission#testDecommission2	org.apache.hadoop.ipc.RemoteException	Requested replication factor of 4 is less than the required minimum of 9096 for /user/root/testDecommission2.dat, clientName=127.0.0.1        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.verifyReplication(BlockManager.java:1611)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2679)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)        at java.base/java.security.AccessController.doPrivileged(Native Method)        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy32.create(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy35.create(Unknown Source), org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280), org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271), org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250), org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232), org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170), org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556), org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567), org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.hdfs.AdminStatesBaseTest.writeFile(AdminStatesBaseTest.java:136), org.apache.hadoop.hdfs.AdminStatesBaseTest.writeFile(AdminStatesBaseTest.java:129), org.apache.hadoop.hdfs.AdminStatesBaseTest.writeFile(AdminStatesBaseTest.java:124), org.apache.hadoop.hdfs.TestDecommission.testDecommission2(TestDecommission.java:212), org.apache.hadoop.hdfs.TestDecommission.testDecommission2$$CONFUZZ(TestDecommission.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommission/testDecommission2/campaign/failures/debug_000001	{"dfs.replication.max": "12072", "dfs.namenode.replication.min": "9096"}	["debug_000001"]											
	Bug-206	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testBlocksRemovedWhileInSafeModeEditsArriveFirst	java.lang.IllegalArgumentException	timeout can't be negative	java.base/sun.nio.ch.SocketAdaptor.setSoTimeout(SocketAdaptor.java:338), org.apache.hadoop.hdfs.DataStreamer.createSocketForPipeline(DataStreamer.java:256), org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1774), org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1728), org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode/testBlocksRemovedWhileInSafeModeEditsArriveFirst/campaign/failures/debug_000001	{"dfs.client.socket-timeout": "2147483646"}	["debug_000001"]											
	Bug-124	BUG	1	org.apache.hadoop.hdfs.TestDecommission#testNodeUsageAfterDecommissioned	java.lang.NullPointerException		org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned(TestDecommission.java:1498), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned$$CONFUZZ(TestDecommission.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommission/testNodeUsageAfterDecommissioned/campaign/failures/debug_000000	{"dfs.namenode.gc.time.monitor.observation.window.ms": "044897756s"}	["debug_000000"]											
wait time in L102 of TestDataNodeRollingUpgrade.java is too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade#testDatanodeRollingUpgradeWithRollback	java.lang.AssertionError		org.junit.Assert.fail(Assert.java:87), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertTrue(Assert.java:53), org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.deleteAndEnsureInTrash(TestDataNodeRollingUpgrade.java:141), org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.testDatanodeRollingUpgradeWithRollback(TestDataNodeRollingUpgrade.java:274), org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.testDatanodeRollingUpgradeWithRollback$$CONFUZZ(TestDataNodeRollingUpgrade.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade/testDatanodeRollingUpgradeWithRollback/campaign/failures/debug_000002	{"dfs.heartbeat.interval": "1949799550"}	["debug_000002"]											
	Bug-1	BUG	1	org.apache.hadoop.hdfs.TestHDFSFileSystemContract#testAppend	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371), org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:853), org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:466), org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:865), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:771), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestHDFSFileSystemContract.setUp(TestHDFSFileSystemContract.java:46), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestHDFSFileSystemContract/testAppend/campaign/failures/debug_000000	{"dfs.namenode.blockreport.queue.size": "611881599", "ipc.server.read.threadpool.size": "1124040447"}	["debug_000000"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock#testAviodStaleAndSlowDatanodes	org.junit.ComparisonFailure	expected:<[5.5.5.5]> but was:<[3.3.3.3]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleAndSlowDatanodes(TestSortLocatedBlock.java:153), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleAndSlowDatanodes$$CONFUZZ(TestSortLocatedBlock.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock/testAviodStaleAndSlowDatanodes/campaign/failures/debug_000000	{"dfs.namenode.stale.datanode.minimum.interval": "2202", "dfs.heartbeat.interval": "501"}	["debug_000000", "debug_000004", "debug_000001"]											
delay too large for the assertion	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.TestFsck#testFsckReplicaDetails	java.lang.AssertionError		org.junit.Assert.fail(Assert.java:87), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertTrue(Assert.java:53), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckReplicaDetails(TestFsck.java:959), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckReplicaDetails$$CONFUZZ(TestFsck.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestFsck/testFsckReplicaDetails/campaign/failures/debug_000005	{"dfs.blockreport.initialDelay": "944"}	["debug_000005"]											
length too small for the big string	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl#testRemoveAclMustBeOwnerOrSuper	org.apache.hadoop.ipc.RpcException	RPC response exceeds maximum data length	org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936), org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238), org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl/testRemoveAclMustBeOwnerOrSuper/campaign/failures/debug_000011	{"ipc.maximum.response.length": "255", "dfs.permissions.superusergroup": "twvwarwwnedrdkduojshmvppbazyrprfawszwflzrbnrvrormovqvxltqxshdeuqomnolpstasdpzxsnktrgfbgroup"}	["debug_000011"]											
	Bug-124	BUG	1	org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testNodeUsageWhileDecommissioining	java.lang.NullPointerException		org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageWhileDecommissioining(TestDecommission.java:1510), org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor.testNodeUsageWhileDecommissioining$$CONFUZZ(TestDecommissionWithBackoffMonitor.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor/testNodeUsageWhileDecommissioining/campaign/failures/debug_000000	{"dfs.namenode.safemode.min.datanodes": "225"}	["debug_000000"]											
	Bug-125	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testUnreachableObserverWithMultiple	org.apache.hadoop.ipc.RemoteException	No reads!	org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider$NameNodeAnswer$ClientProtocolAnswer.answer(TestObserverReadProxyProvider.java:404), org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39), org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96), org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29), org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:61), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptAbstract(MockMethodInterceptor.java:126), org.apache.hadoop.hdfs.protocol.ClientProtocol$MockitoMock$830028894.checkAccess(Unknown Source), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider$ObserverReadInvocationHandler.invoke(ObserverReadProxyProvider.java:519), com.sun.proxy.$Proxy36.checkAccess(Unknown Source), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.doRead(TestObserverReadProxyProvider.java:345), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.doRead(TestObserverReadProxyProvider.java:328), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple(TestObserverReadProxyProvider.java:212), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple$$CONFUZZ(TestObserverReadProxyProvider.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider/testUnreachableObserverWithMultiple/campaign/failures/debug_000003	{"dfs.client.failover.random.order": "true"}	["debug_000003"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.TestEncryptionZones#testGetTrashRoots	java.lang.IllegalArgumentException	Minimum value of buffer size is 512.	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.CryptoStreamUtils.checkBufferSize(CryptoStreamUtils.java:70), org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:104), org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:134), org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:128), org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:1009), org.apache.hadoop.hdfs.DFSClient.createWrappedOutputStream(DFSClient.java:983), org.apache.hadoop.hdfs.DistributedFileSystem.safelyCreateWrappedOutputStream(DistributedFileSystem.java:721), org.apache.hadoop.hdfs.DistributedFileSystem.access$300(DistributedFileSystem.java:147), org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:559), org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1242), org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:466), org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:447), org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:440), org.apache.hadoop.hdfs.DFSTestUtil.createFile(DFSTestUtil.java:433), org.apache.hadoop.hdfs.TestEncryptionZones.testGetTrashRoots(TestEncryptionZones.java:1910), org.apache.hadoop.hdfs.TestEncryptionZones.testGetTrashRoots$$CONFUZZ(TestEncryptionZones.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestEncryptionZones/testGetTrashRoots/campaign/failures/debug_000003	{"hadoop.security.crypto.buffer.size": "469"}	["debug_000003"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcTwoDeadJNs	java.lang.IllegalArgumentException	numLevels must be at least 1	org.apache.hadoop.ipc.CallQueueManager.parseNumLevels(CallQueueManager.java:355), org.apache.hadoop.ipc.CallQueueManager.<init>(CallQueueManager.java:78), org.apache.hadoop.ipc.Server.<init>(Server.java:3115), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371), org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:853), org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.<init>(JournalNodeRpcServer.java:101), org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:242), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.<init>(MiniJournalCluster.java:120), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.<init>(MiniJournalCluster.java:47), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster$Builder.build(MiniJournalCluster.java:79), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.setup(TestQuorumJournalManager.java:111), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testSelectViaRpcTwoDeadJNs/campaign/failures/debug_000001	{"ipc.0.scheduler.priority.levels": "0"}	["debug_000001"]											
	Bug-124	BUG	1	org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testNodeUsageWhileDecommissioining	java.lang.NullPointerException		org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageWhileDecommissioining(TestDecommission.java:1510), org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor.testNodeUsageWhileDecommissioining$$CONFUZZ(TestDecommissionWithBackoffMonitor.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor/testNodeUsageWhileDecommissioining/campaign/failures/debug_000000	{"dfs.namenode.edits.dir.minimum": "26861"}	["debug_000000"]											
	Bug-188	BUG	1	org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testMoveBlockFailure	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.testMoveBlockFailure(TestFsDatasetImpl.java:1005), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.testMoveBlockFailure$$CONFUZZ(TestFsDatasetImpl.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl/testMoveBlockFailure/campaign/failures/debug_000000	{"ipc.server.handler.queue.size": "135", "dfs.datanode.handler.count": "272664331"}	["debug_000000"]											
	Bug-210	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRename	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.lambda$testClientRetryWithFailover$2(TestRetryCacheWithHA.java:1346), org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:449), org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:421), org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testClientRetryWithFailover(TestRetryCacheWithHA.java:1344), org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testRename(TestRetryCacheWithHA.java:1190), org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA.testRename$$CONFUZZ(TestRetryCacheWithHA.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA/testRename/campaign/failures/debug_000001	{"dfs.namenode.enable.retrycache": "false"}	["debug_000001"]											
length too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testPurgeLogs	org.apache.hadoop.hdfs.qjournal.client.QuorumException	Got too many exceptions to achieve quorum size 2/3. 3 exceptions thrown:127.0.0.1:38047: RPC response exceeds maximum data length127.0.0.1:42241: RPC response exceeds maximum data length127.0.0.1:35805: RPC response exceeds maximum data length	org.apache.hadoop.hdfs.qjournal.client.QuorumException.create(QuorumException.java:81), org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException(QuorumCall.java:305), org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:143), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.createNewUniqueEpoch(QuorumJournalManager.java:233), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.recoverUnfinalizedSegments(QuorumJournalManager.java:478), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.setup(TestQuorumJournalManager.java:118), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testPurgeLogs/campaign/failures/debug_000002	{"ipc.maximum.response.length": "33"}	["debug_000002"]											
	Bug-211	BUG	1	org.apache.hadoop.hdfs.TestAppendDifferentChecksum#testAlgoSwitchRandomized	java.lang.IllegalArgumentException	Buffer size <= 0	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.hdfs.DataStreamer.createBlockOutputStream(DataStreamer.java:1785), org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1728), org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestAppendDifferentChecksum/testAlgoSwitchRandomized/campaign/failures/debug_000000	{"io.file.buffer.size": "1"}	["debug_000000"]											
timeout too small (in fact a negative number can also work)	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestAppendDifferentChecksum#testAlgoSwitchRandomized	java.io.InterruptedIOException	No ack received after 0s and a timeout of 0s	org.apache.hadoop.hdfs.DataStreamer.waitForAckedSeqno(DataStreamer.java:931), org.apache.hadoop.hdfs.DFSOutputStream.flushInternal(DFSOutputStream.java:778), org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:888), org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:847), org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77), org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106), org.apache.hadoop.hdfs.TestAppendDifferentChecksum.testAlgoSwitchRandomized(TestAppendDifferentChecksum.java:125), org.apache.hadoop.hdfs.TestAppendDifferentChecksum.testAlgoSwitchRandomized$$CONFUZZ(TestAppendDifferentChecksum.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestAppendDifferentChecksum/testAlgoSwitchRandomized/campaign/failures/debug_000007	{"dfs.datanode.socket.write.timeout": "0"}	["debug_000007"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.TestEncryptionZones#testListEncryptionZonesWithSnapshots	java.lang.IllegalArgumentException	dfs.namenode.reencrypt.sleep.interval is not positive.	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.<init>(ReencryptionHandler.java:199), org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager.<init>(EncryptionZoneManager.java:250), org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:411), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:977), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestEncryptionZones.setup(TestEncryptionZones.java:198), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestEncryptionZones/testListEncryptionZonesWithSnapshots/campaign/failures/debug_000006	{"dfs.namenode.reencrypt.sleep.interval": "0s"}	["debug_000006"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl#testProvidedVolumeImpl	java.lang.RuntimeException	configured value 32831for dfs.datanode.cache.revocation.polling.ms is too high.  It must not be more than half of the value of dfs.datanode.cache.revocation.timeout.ms.  Reconfigure this to 7007	org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetCache.<init>(FsDatasetCache.java:174), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:375), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl.setUp(TestProvidedImpl.java:342), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl/testProvidedVolumeImpl/campaign/failures/debug_000002	{"dfs.datanode.cache.revocation.polling.ms": "32831", "dfs.datanode.cache.revocation.timeout.ms": "14015"}	["debug_000002"]											
length limit for the name is too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDisableConnCache#testDisableCache	org.apache.hadoop.ipc.RemoteException	The maximum path component name limit of testConnCache.dat in directory / is exceeded: limit=1 length=17        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxComponentLength(FSDirectory.java:1258)        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1360)        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:1184)        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFile(FSDirWriteFileOp.java:579)        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:398)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799)        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494)        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)        at java.base/java.security.AccessController.doPrivileged(Native Method)        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy29.create(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:382), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy30.create(Unknown Source), org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:280), org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1271), org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1250), org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1232), org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1170), org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:556), org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:553), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:567), org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:494), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.hdfs.client.impl.BlockReaderTestUtil.writeFile(BlockReaderTestUtil.java:127), org.apache.hadoop.hdfs.TestDisableConnCache.testDisableCache(TestDisableConnCache.java:54), org.apache.hadoop.hdfs.TestDisableConnCache.testDisableCache$$CONFUZZ(TestDisableConnCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDisableConnCache/testDisableCache/campaign/failures/debug_000003	{"dfs.namenode.fs-limits.max-component-length": "1"}	["debug_000003"]											
	Bug-123	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA#testHarUriWithHaUriWithNoPort	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA.testHarUriWithHaUriWithNoPort(TestHarFileSystemWithHA.java:60), org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA.testHarUriWithHaUriWithNoPort$$CONFUZZ(TestHarFileSystemWithHA.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA/testHarUriWithHaUriWithNoPort/campaign/failures/debug_000000	{"dfs.datanode.cache.revocation.timeout.ms": "503"}	["debug_000000"]											
0 is not a good number	Hardcode Assertion	FP	2	org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerForXAttr#testWebImageViewerForGetXAttrsWithOutParameters	java.io.IOException	Timed out waiting for Mini HDFS Cluster to start	org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp(MiniDFSCluster.java:1503), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:973), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerForXAttr.createOriginalFSImage(TestOfflineImageViewerForXAttr.java:75), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.tools.offlineImageViewer.TestOfflineImageViewerForXAttr/testWebImageViewerForGetXAttrsWithOutParameters/campaign/failures/debug_000003	{"dfs.datanode.fsdatasetasyncdisk.max.threads.per.volume": "0"}	["debug_000003"]	org.apache.hadoop.hdfs.TestDFSShell#testGetFAttrErrors	java.io.IOException	Timed out waiting for Mini HDFS Cluster to start	org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp(MiniDFSCluster.java:1503), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:973), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestDFSShell.setup(TestDFSShell.java:123), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDFSShell/testGetFAttrErrors/campaign/failures/debug_000001	{"dfs.datanode.fsdatasetasyncdisk.max.threads.per.volume": "0"}	["debug_000001"]
heartbeat is too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot#testDatanodeRestarts	java.lang.AssertionError	expected:<5> but was:<0>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot.testDatanodeRestarts(TestStandbyIsHot.java:170), org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot.testDatanodeRestarts$$CONFUZZ(TestStandbyIsHot.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot/testDatanodeRestarts/campaign/failures/debug_000003	{"dfs.namenode.heartbeat.recheck-interval": "7"}	["debug_000003"]											
redundancy too long	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade#testDatanodeRollingUpgradeWithRollback	java.lang.AssertionError		org.junit.Assert.fail(Assert.java:87), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertTrue(Assert.java:53), org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.deleteAndEnsureInTrash(TestDataNodeRollingUpgrade.java:141), org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.testDatanodeRollingUpgradeWithRollback(TestDataNodeRollingUpgrade.java:274), org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade.testDatanodeRollingUpgradeWithRollback$$CONFUZZ(TestDataNodeRollingUpgrade.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade/testDatanodeRollingUpgradeWithRollback/campaign/failures/debug_000003	{"dfs.namenode.redundancy.interval.seconds": "260145153"}	["debug_000003"]											
	Bug-125	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testUnreachableObserverWithMultiple	java.io.IOException	Unavailable	org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider$NameNodeAnswer$ClientProtocolAnswer.answer(TestObserverReadProxyProvider.java:370), org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39), org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96), org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29), org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:61), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptAbstract(MockMethodInterceptor.java:126), org.apache.hadoop.hdfs.protocol.ClientProtocol$MockitoMock$1295320945.checkAccess(Unknown Source), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider$ObserverReadInvocationHandler.invoke(ObserverReadProxyProvider.java:519), com.sun.proxy.$Proxy36.checkAccess(Unknown Source), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.doRead(TestObserverReadProxyProvider.java:345), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.doRead(TestObserverReadProxyProvider.java:328), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple(TestObserverReadProxyProvider.java:212), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple$$CONFUZZ(TestObserverReadProxyProvider.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider/testUnreachableObserverWithMultiple/campaign/failures/debug_000002	{"dfs.client.failover.random.order": "true"}	["debug_000002"]											
KERBERO requires a key tab	invalid value	FP	1	org.apache.hadoop.hdfs.server.namenode.TestStartup#testChkpointStartup1	java.lang.AssertionError	java.io.IOException: Running in secure mode, but config doesn't have a keytab        at org.apache.hadoop.security.SecurityUtil.login(SecurityUtil.java:308)        at org.apache.hadoop.hdfs.server.namenode.NameNode.loginAsNameNodeUser(NameNode.java:719)        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:738)        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020)        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995)        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769)        at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374)        at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143)        at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016)        at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948)        at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576)        at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518)        at org.apache.hadoop.hdfs.server.namenode.TestStartup.createCheckPoint(TestStartup.java:151)        at org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1(TestStartup.java:348)        at org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1$$CONFUZZ(TestStartup.java)        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.base/java.lang.reflect.Method.invoke(Method.java:566)        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)        at edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59)        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)        at edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65)        at edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101)        at edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144)        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)        at org.junit.runner.JUnitCore.run(JUnitCore.java:137)        at edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208)        at edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41)	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.hdfs.server.namenode.TestStartup.createCheckPoint(TestStartup.java:173), org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1(TestStartup.java:348), org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1$$CONFUZZ(TestStartup.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestStartup/testChkpointStartup1/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]											
	Bug-1	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl#testDefaultAclNewFileWithMode	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371), org.apache.hadoop.ipc.RPC$Builder.build(RPC.java:853), org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.<init>(NameNodeRpcServer.java:466), org.apache.hadoop.hdfs.server.namenode.NameNode.createRpcServer(NameNode.java:865), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:771), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.FSAclBaseTest.startCluster(FSAclBaseTest.java:91), org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl.init(TestFileContextAcl.java:41), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl/testDefaultAclNewFileWithMode/campaign/failures/debug_000002	{"dfs.namenode.blocks.per.postponedblocks.rescan": "537974527", "ipc.server.read.threadpool.size": "1124797696"}	["debug_000002"]											
for Windows	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testLoadingDfsUsedForVolumes	java.lang.AssertionError	expected:<1024> but was:<18>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.testLoadingDfsUsedForVolumes(TestFsDatasetImpl.java:684), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.testLoadingDfsUsedForVolumes$$CONFUZZ(TestFsDatasetImpl.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl/testLoadingDfsUsedForVolumes/campaign/failures/debug_000003	{"fs.getspaceused.jitterMillis": "2130640638", "fs.local.block.size": "229119", "fs.getspaceused.classname": "org.apache.hadoop.fs.WindowsGetSpaceUsed"}	["debug_000003"]											
	Bug-125	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testUnreachableObserverWithMultiple	java.io.IOException	Unavailable	org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider$NameNodeAnswer$ClientProtocolAnswer.answer(TestObserverReadProxyProvider.java:370), org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39), org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96), org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29), org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:61), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptAbstract(MockMethodInterceptor.java:126), org.apache.hadoop.hdfs.protocol.ClientProtocol$MockitoMock$436683570.checkAccess(Unknown Source), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider$ObserverReadInvocationHandler.invoke(ObserverReadProxyProvider.java:519), com.sun.proxy.$Proxy36.checkAccess(Unknown Source), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.doRead(TestObserverReadProxyProvider.java:345), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.doRead(TestObserverReadProxyProvider.java:328), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple(TestObserverReadProxyProvider.java:212), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple$$CONFUZZ(TestObserverReadProxyProvider.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider/testUnreachableObserverWithMultiple/campaign/failures/debug_000003	{"dfs.client.failover.random.order": "true", "dfs.client.failover.connection.retries.on.timeouts": "767"}	["debug_000003"]											
should not open auth	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDFSClientRetries#testClientDNProtocolTimeout	org.apache.hadoop.ipc.RemoteException	Protocol interface org.apache.hadoop.hdfs.protocol.ClientDatanodeProtocol is not known.	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy22.getReplicaVisibleLength(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientDatanodeProtocolTranslatorPB.getReplicaVisibleLength(ClientDatanodeProtocolTranslatorPB.java:201), org.apache.hadoop.hdfs.TestDFSClientRetries.testClientDNProtocolTimeout(TestDFSClientRetries.java:894), org.apache.hadoop.hdfs.TestDFSClientRetries.testClientDNProtocolTimeout$$CONFUZZ(TestDFSClientRetries.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDFSClientRetries/testClientDNProtocolTimeout/campaign/failures/debug_000002	{"hadoop.security.authorization": "true"}	["debug_000002"]											
		Non-Reproducible	1	org.apache.hadoop.hdfs.server.namenode.TestStartup#testChkpointStartup1	java.lang.AssertionError	java.io.IOException: Unexpected configuration parameters: dfs.namenode.replication.min = 5888 > dfs.replication.max = 105        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:532)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796)        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256)        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450)        at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261)        at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)        at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016)        at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948)        at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576)        at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518)        at org.apache.hadoop.hdfs.server.namenode.TestStartup.createCheckPoint(TestStartup.java:151)        at org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1(TestStartup.java:348)        at org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1$$CONFUZZ(TestStartup.java)        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.base/java.lang.reflect.Method.invoke(Method.java:566)        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)        at edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59)        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)        at edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65)        at edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101)        at edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144)        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)        at org.junit.runner.JUnitCore.run(JUnitCore.java:137)        at edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208)        at edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41)	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.hdfs.server.namenode.TestStartup.createCheckPoint(TestStartup.java:173), org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1(TestStartup.java:348), org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1$$CONFUZZ(TestStartup.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestStartup/testChkpointStartup1/campaign/failures/debug_000000	{"dfs.datanode.disk.check.timeout": "2ms"}	["debug_000000"]											
length too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestQuota#testSetAndClearSpaceQuotaPathIsFile	java.lang.AssertionError	Expected: is (a string containing "setSpaceQuota" and a string containing "/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/ag53Cfj888/TestQuota/testSetAndClearSpaceQuotaPathIsFile/path-is-file" and a string containing "Is not a directory")     but: a string containing "/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/ag53Cfj888/TestQuota/testSetAndClearSpaceQuotaPathIsFile/path-is-file" was "setSpaceQuota: RPC response exceeds maximum data length"	org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20), org.junit.Assert.assertThat(Assert.java:964), org.junit.Assert.assertThat(Assert.java:930), org.apache.hadoop.hdfs.TestQuota.testSetAndClearSpaceQuotaPathIsFileInternal(TestQuota.java:1511), org.apache.hadoop.hdfs.TestQuota.testSetAndClearSpaceQuotaPathIsFile(TestQuota.java:1480), org.apache.hadoop.hdfs.TestQuota.testSetAndClearSpaceQuotaPathIsFile$$CONFUZZ(TestQuota.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestQuota/testSetAndClearSpaceQuotaPathIsFile/campaign/failures/debug_000006	{"ipc.maximum.response.length": "695"}	["debug_000006"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testDecommissionWithMissingBlock	java.io.IOException	Failed: the number of failed blocks = 4 > the number of parity blocks = 3	org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:410), org.apache.hadoop.hdfs.DFSStripedOutputStream.handleStreamerFailure(DFSStripedOutputStream.java:435), org.apache.hadoop.hdfs.DFSStripedOutputStream.handleCurrentStreamerFailure(DFSStripedOutputStream.java:427), org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:571), org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:218), org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:126), org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:112), org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:62), java.base/java.io.DataOutputStream.write(DataOutputStream.java:107), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96), org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:903), org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:924), org.apache.hadoop.hdfs.TestDecommissionWithStriped.writeStripedFile(TestDecommissionWithStriped.java:570), org.apache.hadoop.hdfs.TestDecommissionWithStriped.testDecommissionWithMissingBlock(TestDecommissionWithStriped.java:844), org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor.testDecommissionWithMissingBlock$$CONFUZZ(TestDecommissionWithStripedBackoffMonitor.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor/testDecommissionWithMissingBlock/campaign/failures/debug_000006	{"dfs.client.socket-timeout": "939", "dfs.client-write-packet-size": "10096", "dfs.datanode.data.write.bandwidthPerSec": "767"}	["debug_000006"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestEncryptionZones#testBasicOperations	java.io.IOException	Timed out waiting for Mini HDFS Cluster to start	org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp(MiniDFSCluster.java:1503), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:973), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestEncryptionZones.setup(TestEncryptionZones.java:198), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestEncryptionZones/testBasicOperations/campaign/failures/debug_000001	{"dfs.datanode.fsdatasetcache.max.threads.per.volume": "0"}	["debug_000001"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.web.TestWebHDFSAcl#testSkipAclEnforcementSuper	java.lang.AssertionError	expected AccessControlException for user diana (auth:SIMPLE), path = /p15/bruce/file	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.hdfs.server.namenode.AclTestHelpers.assertFilePermissionDenied(AclTestHelpers.java:120), org.apache.hadoop.hdfs.server.namenode.FSAclBaseTest.testSkipAclEnforcementSuper(FSAclBaseTest.java:1315), org.apache.hadoop.hdfs.web.TestWebHDFSAcl.testSkipAclEnforcementSuper$$CONFUZZ(TestWebHDFSAcl.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.web.TestWebHDFSAcl/testSkipAclEnforcementSuper/campaign/failures/debug_000003	{"dfs.permissions.enabled": "false"}	["debug_000003"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testWithKeytabs	org.apache.hadoop.ipc.RemoteException	Failed to set storage policy since dfs.storage.policy.enabled is set to false.        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkStoragePolicyEnabled(FSNamesystem.java:2375)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setStoragePolicy(FSNamesystem.java:2395)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setStoragePolicy(NameNodeRpcServer.java:868)        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setStoragePolicy(ClientNamenodeProtocolServerSideTranslatorPB.java:1765)        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)        at java.base/java.security.AccessController.doPrivileged(Native Method)        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy32.setStoragePolicy(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setStoragePolicy(ClientNamenodeProtocolTranslatorPB.java:1805), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy33.setStoragePolicy(Unknown Source), org.apache.hadoop.hdfs.DFSClient.setStoragePolicy(DFSClient.java:1511), org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:762), org.apache.hadoop.hdfs.DistributedFileSystem$13.doCall(DistributedFileSystem.java:759), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.setStoragePolicy(DistributedFileSystem.java:771), org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.testWhenStoragePolicySetToALLSSD(TestExternalStoragePolicySatisfier.java:455), org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier$1.run(TestExternalStoragePolicySatisfier.java:311), org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier$1.run(TestExternalStoragePolicySatisfier.java:307), java.base/java.security.AccessController.doPrivileged(Native Method), java.base/javax.security.auth.Subject.doAs(Subject.java:423), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878), org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.testWithKeytabs(TestExternalStoragePolicySatisfier.java:307), org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier.testWithKeytabs$$CONFUZZ(TestExternalStoragePolicySatisfier.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier/testWithKeytabs/campaign/failures/debug_000006	{"dfs.storage.policy.enabled": "false"}	["debug_000006"]											
	Bug-8	BUG	1	org.apache.hadoop.hdfs.TestPread#testPreadLocalFS	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.read(ChecksumFileSystem.java:210), org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:124), org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:117), org.apache.hadoop.hdfs.TestPread.writeFile(TestPread.java:96), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS(TestPread.java:503), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS$$CONFUZZ(TestPread.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestPread/testPreadLocalFS/campaign/failures/debug_000000	{"file.stream-buffer-size": "2130640638"}	["debug_000000"]											
length too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.web.TestWebHDFSAcl#testDefaultAclRenamedDir	org.apache.hadoop.ipc.RemoteException	The maximum path component name limit of p23 in directory / is exceeded: limit=2 length=3	org.apache.hadoop.hdfs.web.JsonUtilClient.toRemoteException(JsonUtilClient.java:89), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:522), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$300(WebHdfsFileSystem.java:145), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:739), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:814), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.access$100(WebHdfsFileSystem.java:637), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner$1.run(WebHdfsFileSystem.java:675), java.base/java.security.AccessController.doPrivileged(Native Method), java.base/javax.security.auth.Subject.doAs(Subject.java:423), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.run(WebHdfsFileSystem.java:671), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.mkdirs(WebHdfsFileSystem.java:1142), org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:2388), org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:750), org.apache.hadoop.hdfs.server.namenode.FSAclBaseTest.testDefaultAclRenamedDir(FSAclBaseTest.java:1268), org.apache.hadoop.hdfs.web.TestWebHDFSAcl.testDefaultAclRenamedDir$$CONFUZZ(TestWebHDFSAcl.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.web.TestWebHDFSAcl/testDefaultAclRenamedDir/campaign/failures/debug_000004	{"dfs.namenode.fs-limits.max-component-length": "2"}	["debug_000004"]											
interval	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.web.TestWebHDFSAcl#testDefaultAclRenamedDir	java.io.IOException	Timed out waiting for Mini HDFS Cluster to start	org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp(MiniDFSCluster.java:1503), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:973), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.FSAclBaseTest.startCluster(FSAclBaseTest.java:91), org.apache.hadoop.hdfs.web.TestWebHDFSAcl.init(TestWebHDFSAcl.java:34), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.web.TestWebHDFSAcl/testDefaultAclRenamedDir/campaign/failures/debug_000003	{"dfs.datanode.directoryscan.interval": "0"}	["debug_000003"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocations	org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException	Unresolved topology mapping for host null	org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.resolveNetworkLocation(DatanodeManager.java:1030), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:1250), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocations(TestDatanodeManager.java:485), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocations$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocations/campaign/failures/debug_000002	{"dfs.namenode.reject-unresolved-dn-topology-mapping": "true", "net.topology.script.number.args": "0"}	["debug_000002"]											
	Bug-123	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA#testHarUriWithHaUriWithNoPort	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA.testHarUriWithHaUriWithNoPort(TestHarFileSystemWithHA.java:60), org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA.testHarUriWithHaUriWithNoPort$$CONFUZZ(TestHarFileSystemWithHA.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA/testHarUriWithHaUriWithNoPort/campaign/failures/debug_000000	{"dfs.datanode.max.locked.memory": "2534"}	["debug_000000"]											
dfs.namenode.ec.policies.max.cellsize is too small (> 65536)	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure#testIdempotentCloseWithFailedStreams	org.apache.hadoop.ipc.RemoteException	The policy name RS-6-3-64k does not exist        at org.apache.hadoop.hdfs.server.namenode.ErasureCodingPolicyManager.enablePolicy(ErasureCodingPolicyManager.java:433)        at org.apache.hadoop.hdfs.server.namenode.FSDirErasureCodingOp.enableErasureCodingPolicy(FSDirErasureCodingOp.java:285)        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.enableErasureCodingPolicy(FSNamesystem.java:8070)        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.enableErasureCodingPolicy(NameNodeRpcServer.java:2559)        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.enableErasureCodingPolicy(ClientNamenodeProtocolServerSideTranslatorPB.java:1916)        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572)        at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556)        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043)        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971)        at java.base/java.security.AccessController.doPrivileged(Native Method)        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy30.enableErasureCodingPolicy(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.enableErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1910), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy33.enableErasureCodingPolicy(Unknown Source), org.apache.hadoop.hdfs.DFSClient.enableErasureCodingPolicy(DFSClient.java:2993), org.apache.hadoop.hdfs.DistributedFileSystem.enableErasureCodingPolicy(DistributedFileSystem.java:3228), org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:214), org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testIdempotentCloseWithFailedStreams(TestDFSStripedOutputStreamWithFailure.java:165), org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testIdempotentCloseWithFailedStreams$$CONFUZZ(TestDFSStripedOutputStreamWithFailure.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure/testIdempotentCloseWithFailedStreams/campaign/failures/debug_000001	{"dfs.namenode.ec.policies.max.cellsize": "767"}	["debug_000001"]											
	Bug-197	BUG	1	org.apache.hadoop.hdfs.TestDFSShell#testSetXAttrPermission	java.lang.AssertionError	Permission denied printed	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.hdfs.TestDFSShell$11.run(TestDFSShell.java:3066), java.base/java.security.AccessController.doPrivileged(Native Method), java.base/javax.security.auth.Subject.doAs(Subject.java:423), org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878), org.apache.hadoop.hdfs.TestDFSShell.testSetXAttrPermission(TestDFSShell.java:3059), org.apache.hadoop.hdfs.TestDFSShell.testSetXAttrPermission$$CONFUZZ(TestDFSShell.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDFSShell/testSetXAttrPermission/campaign/failures/debug_000004	{"ipc.maximum.response.length": "767"}	["debug_000004"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashAtBeginningOfSegment	java.io.IOException	Timed out waiting 3ms for a quorum of nodes to respond.	org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:138), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.createNewUniqueEpoch(QuorumJournalManager.java:244), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.recoverUnfinalizedSegments(QuorumJournalManager.java:478), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testCrashAtBeginningOfSegment(TestQuorumJournalManager.java:297), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testCrashAtBeginningOfSegment$$CONFUZZ(TestQuorumJournalManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testCrashAtBeginningOfSegment/campaign/failures/debug_000004	{"dfs.qjournal.new-epoch.timeout.ms": "3"}	["debug_000004"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashAtBeginningOfSegment	java.io.IOException	Timed out waiting 2ms for a quorum of nodes to respond.	org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:138), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.startLogSegment(QuorumJournalManager.java:436), org.apache.hadoop.hdfs.qjournal.QJMTestUtil.writeSegment(QJMTestUtil.java:84), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testCrashAtBeginningOfSegment(TestQuorumJournalManager.java:300), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.testCrashAtBeginningOfSegment$$CONFUZZ(TestQuorumJournalManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testCrashAtBeginningOfSegment/campaign/failures/debug_000003	{"dfs.qjournal.start-segment.timeout.ms": "2"}	["debug_000003"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens#testSaveNamespace	java.lang.AssertionError	In-progress log EditLogFile(file=/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000005,first=0000000000000000005,last=0000000000000000007,inProgress=true,hasCorruptHeader=false) should have 5 transactions expected:<5> but was:<3>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens.testSaveNamespace(TestCheckPointForSecurityTokens.java:93), org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens.testSaveNamespace$$CONFUZZ(TestCheckPointForSecurityTokens.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens/testSaveNamespace/campaign/failures/debug_000001	{"dfs.namenode.edit.log.autoroll.multiplier.threshold": "-0.4923848509788513"}	["debug_000001"]											
kerberos need keytab	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.ha.TestPendingCorruptDnMessages#testChangedStorageId	javax.servlet.ServletException	Keytab does not exist: /root/hadoop.keytab	org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:156), org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194), org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180), org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter.init(ProxyUserAuthenticationFilter.java:57), org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:140), org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:731), java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948), java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734), java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:658), org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:755), org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379), org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:910), org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117), org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117), org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.server.Server.start(Server.java:423), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110), org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97), org.eclipse.jetty.server.Server.doStart(Server.java:387), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:170), org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:954), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:765), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.ha.TestPendingCorruptDnMessages.testChangedStorageId(TestPendingCorruptDnMessages.java:60), org.apache.hadoop.hdfs.server.namenode.ha.TestPendingCorruptDnMessages.testChangedStorageId$$CONFUZZ(TestPendingCorruptDnMessages.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestPendingCorruptDnMessages/testChangedStorageId/campaign/failures/debug_000004	{"dfs.namenode.gc.time.monitor.enable": "false", "dfs.replication.max": "23191", "dfs.namenode.top.enabled": "false", "hadoop.http.authentication.type": "kerberos"}	["debug_000004"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.TestCrcCorruption#testCrcCorruption	java.lang.AssertionError	Test resulted in an unexpected exit	org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2115), org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2102), org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2095), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:979), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestCrcCorruption.thistest(TestCrcCorruption.java:161), org.apache.hadoop.hdfs.TestCrcCorruption.testCrcCorruption(TestCrcCorruption.java:233), org.apache.hadoop.hdfs.TestCrcCorruption.testCrcCorruption$$CONFUZZ(TestCrcCorruption.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestCrcCorruption/testCrcCorruption/campaign/failures/debug_000001	{"dfs.client.read.shortcircuit": "true", "fs.trash.interval": "26960"}	["debug_000001"]											
	Bug-6	BUG	1	org.apache.hadoop.fs.contract.hdfs.TestHDFSContractMultipartUploader#testMultipartUploadEmptyPart	java.lang.OutOfMemoryError	Java heap space	java.base/java.util.concurrent.ArrayBlockingQueue.<init>(ArrayBlockingQueue.java:270), java.base/java.util.concurrent.ArrayBlockingQueue.<init>(ArrayBlockingQueue.java:254), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.<init>(BlockManager.java:5243), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:613), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:771), org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:681), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:768), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.fs.contract.hdfs.HDFSContract.createCluster(HDFSContract.java:58), org.apache.hadoop.fs.contract.hdfs.TestHDFSContractMultipartUploader.createCluster(TestHDFSContractMultipartUploader.java:42), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.fs.contract.hdfs.TestHDFSContractMultipartUploader/testMultipartUploadEmptyPart/campaign/failures/debug_000004	{"dfs.namenode.gc.time.monitor.enable": "false", "dfs.replication.max": "20897", "dfs.namenode.blockreport.queue.size": "866272653"}	["debug_000004"]											
	Bug-59	BUG	1	org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock#testAviodStaleAndSlowDatanodes	java.lang.NullPointerException		java.base/java.util.Comparator.lambda$comparing$77a9974f$1(Comparator.java:469), java.base/java.util.TimSort.countRunAndMakeAscending(TimSort.java:355), java.base/java.util.TimSort.sort(TimSort.java:220), java.base/java.util.Arrays.sort(Arrays.java:1515), java.base/java.util.ArrayList.sort(ArrayList.java:1750), java.base/java.util.Collections.sort(Collections.java:179), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.lambda$createSecondaryNodeSorter$0(DatanodeManager.java:654), org.apache.hadoop.net.NetworkTopology.sortByDistance(NetworkTopology.java:983), org.apache.hadoop.net.NetworkTopology.sortByDistanceUsingNetworkLocation(NetworkTopology.java:946), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlock(DatanodeManager.java:637), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks(DatanodeManager.java:554), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleAndSlowDatanodes(TestSortLocatedBlock.java:144), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleAndSlowDatanodes$$CONFUZZ(TestSortLocatedBlock.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock/testAviodStaleAndSlowDatanodes/campaign/failures/debug_000002	{"dfs.namenode.stale.datanode.minimum.interval": "999", "dfs.namenode.read.considerStorageType": "true", "dfs.heartbeat.interval": "351"}	["debug_000002"]											
stales too fast with the recheck	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoad	org.junit.ComparisonFailure	expected:<IP-[1-3]> but was:<IP-[2-4]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad(TestDatanodeManager.java:578), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderLoad/campaign/failures/debug_000001	{"dfs.namenode.stale.datanode.interval": "17", "dfs.heartbeat.interval": "0"}	["debug_000001"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testObserverToActive	java.net.UnknownHostException	namenode0.test: Name or service not known	java.base/java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method), java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:930), java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543), java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848), java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533), java.base/java.net.InetAddress.getAllByName(InetAddress.java:1386), java.base/java.net.InetAddress.getAllByName(InetAddress.java:1307), org.apache.hadoop.net.DNSDomainNameResolver.getAllByDomainName(DNSDomainNameResolver.java:33), org.apache.hadoop.net.DNSDomainNameResolver.getAllResolvedHostnameByDomainName(DNSDomainNameResolver.java:49), org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider.getResolvedHostsIfNecessary(AbstractNNFailoverProxyProvider.java:240), org.apache.hadoop.hdfs.server.namenode.ha.AbstractNNFailoverProxyProvider.getProxyAddresses(AbstractNNFailoverProxyProvider.java:183), org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.<init>(ConfiguredFailoverProxyProvider.java:51), org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider.<init>(ConfiguredFailoverProxyProvider.java:45), org.apache.hadoop.hdfs.server.namenode.ha.ObserverReadProxyProvider.<init>(ObserverReadProxyProvider.java:170), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider$2.<init>(TestObserverReadProxyProvider.java:116), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.setupProxyProvider(TestObserverReadProxyProvider.java:106), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testObserverToActive(TestObserverReadProxyProvider.java:219), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testObserverToActive$$CONFUZZ(TestObserverReadProxyProvider.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider/testObserverToActive/campaign/failures/debug_000003	{"dfs.client.failover.resolve-needed.testcluster": "true", "hadoop.security.token.service.use_ip": "false"}	["debug_000003"]											
	Bug-124	BUG	1	org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testNodeUsageWhileDecommissioining	java.lang.NullPointerException		org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageWhileDecommissioining(TestDecommission.java:1510), org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor.testNodeUsageWhileDecommissioining$$CONFUZZ(TestDecommissionWithBackoffMonitor.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor/testNodeUsageWhileDecommissioining/campaign/failures/debug_000000	{"ipc.server.handler.queue.size": "318226274"}	["debug_000000"]											
	Bug-12	BUG	1	org.apache.hadoop.hdfs.TestFileLengthOnClusterRestart#testFileLengthWithHSyncAndClusterRestartWithOutDNsRegister	java.lang.OutOfMemoryError	Java heap space	java.base/java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:771), org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:681), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:768), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestFileLengthOnClusterRestart.testFileLengthWithHSyncAndClusterRestartWithOutDNsRegister(TestFileLengthOnClusterRestart.java:43), org.apache.hadoop.hdfs.TestFileLengthOnClusterRestart.testFileLengthWithHSyncAndClusterRestartWithOutDNsRegister$$CONFUZZ(TestFileLengthOnClusterRestart.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestFileLengthOnClusterRestart/testFileLengthWithHSyncAndClusterRestartWithOutDNsRegister/campaign/failures/debug_000004	{"dfs.namenode.blocks.per.postponedblocks.rescan": "699407230", "dfs.namenode.blockreport.queue.size": "454756540"}	["debug_000004"]											
		FP	2	org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes#testFullBlockReportAfterRemovingVolumes	java.lang.AssertionError	Test resulted in an unexpected exit	org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2115), org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2102), org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2095), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:979), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.testFullBlockReportAfterRemovingVolumes(TestDataNodeHotSwapVolumes.java:1100), org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes.testFullBlockReportAfterRemovingVolumes$$CONFUZZ(TestDataNodeHotSwapVolumes.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes/testFullBlockReportAfterRemovingVolumes/campaign/failures/debug_000004	{"fs.trash.interval": "788", "dfs.client.domain.socket.data.traffic": "true"}	["debug_000004"]	org.apache.hadoop.hdfs.web.TestWebHDFSAcl#testRemoveAclMustBeOwnerOrSuper	java.lang.AssertionError	Test resulted in an unexpected exit	org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2115), org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2102), org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2095), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:979), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.FSAclBaseTest.startCluster(FSAclBaseTest.java:91), org.apache.hadoop.hdfs.web.TestWebHDFSAcl.init(TestWebHDFSAcl.java:34), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:258), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.web.TestWebHDFSAcl/testRemoveAclMustBeOwnerOrSuper/campaign/failures/debug_000000	{"fs.trash.interval": "223268976", "dfs.client.domain.socket.data.traffic": "true"}	["debug_000000"]
should set to true	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.TestStartup#testChkpointStartup1	java.lang.AssertionError	java.io.IOException: The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem        at org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296)        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228)        at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450)        at org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261)        at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)        at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016)        at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948)        at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576)        at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518)        at org.apache.hadoop.hdfs.server.namenode.TestStartup.createCheckPoint(TestStartup.java:151)        at org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1(TestStartup.java:348)        at org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1$$CONFUZZ(TestStartup.java)        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)        at java.base/java.lang.reflect.Method.invoke(Method.java:566)        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)        at edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59)        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)        at edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65)        at edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101)        at edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144)        at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)        at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)        at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)        at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)        at org.junit.runners.ParentRunner.run(ParentRunner.java:413)        at org.junit.runner.JUnitCore.run(JUnitCore.java:137)        at edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208)        at edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41)	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.hdfs.server.namenode.TestStartup.createCheckPoint(TestStartup.java:173), org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1(TestStartup.java:348), org.apache.hadoop.hdfs.server.namenode.TestStartup.testChkpointStartup1$$CONFUZZ(TestStartup.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestStartup/testChkpointStartup1/campaign/failures/debug_000000	{"dfs.namenode.safemode.min.datanodes": "35"}	["debug_000000"]											
	Bug-167	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs#testNameEditsConfigsFailure	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs.testNameEditsConfigsFailure(TestNameEditsConfigs.java:450), org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs.testNameEditsConfigsFailure$$CONFUZZ(TestNameEditsConfigs.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs/testNameEditsConfigsFailure/campaign/failures/debug_000000	{"ipc.server.handler.queue.size": "1645801136"}	["debug_000000"]											
	Bug-124	BUG	1	org.apache.hadoop.hdfs.TestDecommission#testNodeUsageAfterDecommissioned	java.lang.NullPointerException		org.apache.hadoop.hdfs.AdminStatesBaseTest.cleanupFile(AdminStatesBaseTest.java:459), org.apache.hadoop.hdfs.TestDecommission.nodeUsageVerification(TestDecommission.java:1575), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned(TestDecommission.java:1498), org.apache.hadoop.hdfs.TestDecommission.testNodeUsageAfterDecommissioned$$CONFUZZ(TestDecommission.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommission/testNodeUsageAfterDecommissioned/campaign/failures/debug_000000	{"dfs.client.read.shortcircuit": "true"}	["debug_000000"]											
	Bug-171	BUG	1	org.apache.hadoop.hdfs.TestPread#testPreadLocalFS	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.read(ChecksumFileSystem.java:210), org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:124), org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:117), org.apache.hadoop.hdfs.TestPread.writeFile(TestPread.java:96), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS(TestPread.java:503), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS$$CONFUZZ(TestPread.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestPread/testPreadLocalFS/campaign/failures/debug_000001	{"file.bytes-per-checksum": "2146036837"}	["debug_000001"]											
	Bug-6	BUG	1	org.apache.hadoop.hdfs.TestDecommissionWithStriped#testDecommissionWithBusyNode	java.lang.OutOfMemoryError	Java heap space	java.base/java.util.concurrent.ArrayBlockingQueue.<init>(ArrayBlockingQueue.java:270), java.base/java.util.concurrent.ArrayBlockingQueue.<init>(ArrayBlockingQueue.java:254), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.<init>(BlockManager.java:5243), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:613), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestDecommissionWithStriped.setup(TestDecommissionWithStriped.java:151), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDecommissionWithStriped/testDecommissionWithBusyNode/campaign/failures/debug_000002	{"file.stream-buffer-size": "945834638", "dfs.namenode.blockreport.queue.size": "467568147", "dfs.namenode.blocks.per.postponedblocks.rescan": "1065849381"}	["debug_000002"]											
	Bug-170	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testMultipleExistingUsers	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:114), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:456), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:447), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:447), org.apache.hadoop.hdfs.MiniDFSCluster.copyNameDirs(MiniDFSCluster.java:1326), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1121), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA.setUpNameNode(TestGetGroupsWithHA.java:41), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA/testMultipleExistingUsers/campaign/failures/debug_000000	{"io.file.buffer.size": "1885408251", "dfs.namenode.blocks.per.postponedblocks.rescan": "1032060624"}	["debug_000000"]											
should set to true	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.TestHDFSFileSystemContract#testAppend	org.apache.hadoop.hdfs.server.namenode.NameNodeFormatException	NameNode format aborted as reformat is disabled for this cluster.	org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1267), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestHDFSFileSystemContract.setUp(TestHDFSFileSystemContract.java:46), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestHDFSFileSystemContract/testAppend/campaign/failures/debug_000009	{"dfs.namenode.edits.dir": "file:/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/LYG6doDKLM/name-0-1,file:/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/LYG6doDKLM/name-0-2", "dfs.reformat.disabled": "true"}	["debug_000009"]											
		Non-Reproducible	1	org.apache.hadoop.fs.contract.hdfs.TestHDFSContractRename#testRenameNewFileSameDir	java.io.IOException	Failed to save in any storage directories while saving namespace.	org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImageInAllDirs(FSImage.java:1243), org.apache.hadoop.hdfs.server.namenode.FSImage.saveFSImageInAllDirs(FSImage.java:1200), org.apache.hadoop.hdfs.server.namenode.FSImage.format(FSImage.java:191), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1278), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.fs.contract.hdfs.HDFSContract.createCluster(HDFSContract.java:58), org.apache.hadoop.fs.contract.hdfs.TestHDFSContractRename.createCluster(TestHDFSContractRename.java:33), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.fs.contract.hdfs.TestHDFSContractRename/testRenameNewFileSameDir/campaign/failures/debug_000002	{"dfs.namenode.blocks.per.postponedblocks.rescan": "778928574", "dfs.image.compress": "true", "io.file.buffer.size": "2052039959"}	["debug_000002"]											
	Bug-175	BUG	1	org.apache.hadoop.hdfs.TestPread#testPreadLocalFS	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.read(ChecksumFileSystem.java:210), org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:124), org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:117), org.apache.hadoop.hdfs.TestPread.writeFile(TestPread.java:96), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS(TestPread.java:503), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS$$CONFUZZ(TestPread.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestPread/testPreadLocalFS/campaign/failures/debug_000001	{"file.bytes-per-checksum": "1623667365"}	["debug_000001"]											
timeout too small	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.namenode.ha.TestUpdateBlockTailing#testStandbyAppendNewBlock	java.lang.AssertionError	Time out while waiting for journal node 0 to start.	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.waitActive(MiniJournalCluster.java:262), org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:117), org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster.<init>(MiniQJMHACluster.java:38), org.apache.hadoop.hdfs.qjournal.MiniQJMHACluster$Builder.build(MiniQJMHACluster.java:68), org.apache.hadoop.hdfs.server.namenode.ha.TestUpdateBlockTailing.startUpCluster(TestUpdateBlockTailing.java:77), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestUpdateBlockTailing/testStandbyAppendNewBlock/campaign/failures/debug_000001	{"ipc.client.rpc-timeout.ms": "654", "ipc.server.read.threadpool.size": "3632"}	["debug_000001"]											
Shuai: I can't reproduce this	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoadWithNodesOfSameDistance	java.lang.AssertionError	expected:<2> but was:<1>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoadWithNodesOfSameDistance(TestDatanodeManager.java:669), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoadWithNodesOfSameDistance$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderLoadWithNodesOfSameDistance/campaign/failures/debug_000001	{"dfs.heartbeat.interval": "615442863", "dfs.namenode.stale.datanode.minimum.interval": "1404710177", "dfs.namenode.stale.datanode.interval": "40"}	["debug_000001"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageTypeAndLoad	org.junit.ComparisonFailure	expected:<IP-[1-3]> but was:<IP-[2-4]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad(TestDatanodeManager.java:818), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderStorageTypeAndLoad/campaign/failures/debug_000001	{"dfs.namenode.stale.datanode.interval": "131", "dfs.heartbeat.interval": "1355466193", "dfs.namenode.stale.datanode.minimum.interval": "581603761", "dfs.namenode.heartbeat.recheck-interval": "17944", "dfs.namenode.write.stale.datanode.ratio": "0.9f", "dfs.datanode.fileio.profiling.sampling.percentage": "8787"}	["debug_000001"]											
	Bug-214	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings#testThatMatchingRPCandHttpPortsThrowException	java.lang.IllegalArgumentException		org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:130), org.apache.hadoop.util.GcTimeMonitor.<init>(GcTimeMonitor.java:135), org.apache.hadoop.util.GcTimeMonitor$Builder.build(GcTimeMonitor.java:90), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:759), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings.testThatMatchingRPCandHttpPortsThrowException(TestValidateConfigurationSettings.java:70), org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings.testThatMatchingRPCandHttpPortsThrowException$$CONFUZZ(TestValidateConfigurationSettings.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings/testThatMatchingRPCandHttpPortsThrowException/campaign/failures/debug_000001	{"dfs.replication": "9793", "dfs.namenode.gc.time.monitor.sleep.interval.ms": "54ms", "dfs.replication.max": "16354", "dfs.namenode.gc.time.monitor.observation.window.ms": "11414395d"}	["debug_000001"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageTypeAndLoad	org.junit.ComparisonFailure	expected:<IP-[0-1]> but was:<IP-[2-4]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad(TestDatanodeManager.java:839), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderStorageTypeAndLoad/campaign/failures/debug_000004	{"dfs.namenode.avoid.read.slow.datanode": "false", "hadoop.security.groups.cache.secs": "270", "dfs.hosts": "", "dfs.use.dfs.network.topology": "true", "dfs.namenode.hosts.provider.classname": "org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager", "dfs.namenode.max.full.block.report.leases": "1909070122", "hadoop.security.auth_to_local": "RULE:[1:$1] RULE:[2:$1]", "net.topology.node.switch.mapping.impl": "org.apache.hadoop.net.ScriptBasedMapping", "dfs.namenode.reject-unresolved-dn-topology-mapping": "false", "dfs.namenode.stale.datanode.minimum.interval": "112247541", "dfs.hosts.exclude": "", "hadoop.kerberos.min.seconds.before.relogin": "570", "net.topology.script.number.args": "699", "ssl.client.stores.reload.interval": "738455503", "dfs.namenode.datanode.registration.ip-hostname-check": "true", "hadoop.security.group.mapping": "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback", "hadoop.ssl.keystores.factory.class": "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory", "dfs.namenode.stale.datanode.interval": "146", "hadoop.security.groups.cache.warn.after.ms": "586", "hadoop.ssl.client.conf": "ssl-client.xml", "dfs.namenode.max.slowpeer.collect.nodes": "575", "hadoop.security.groups.cache.background.reload": "true", "dfs.datanode.outliers.report.interval": "6h", "hadoop.user.group.static.mapping.overrides": "dr.who=;", "dfs.datanode.ipc.address": "0.0.0.0:9867", "hadoop.ssl.hostname.verifier": "STRICT", "dfs.datanode.https.address": "0.0.0.0:9865", "dfs.net.topology.impl": "org.apache.hadoop.hdfs.net.DFSNetworkTopology", "dfs.namenode.enable.log.stale.datanode": "false", "hadoop.security.dns.log-slow-lookups.enabled": "true", "dfs.datanode.fileio.profiling.sampling.percentage": "768378785", "dfs.datanode.address": "0.0.0.0:9866", "dfs.namenode.heartbeat.recheck-interval": "918", "dfs.namenode.write.stale.datanode.ratio": "1.0f", "io.file.buffer.size": "536", "dfs.namenode.full.block.report.lease.length.ms": "1194498755", "dfs.heartbeat.interval": "1632918848", "dfs.namenode.blocks.per.postponedblocks.rescan": "1835325989"}	["debug_000004", "debug_000002", "debug_000001"]											
	Bug-12	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxComponentLength	java.lang.OutOfMemoryError	Java heap space	java.base/java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.lazyInitFSDirectory(TestFsLimits.java:296), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.mkdirs(TestFsLimits.java:251), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.testMaxComponentLength(TestFsLimits.java:88), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.testMaxComponentLength$$CONFUZZ(TestFsLimits.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestFsLimits/testMaxComponentLength/campaign/failures/debug_000002	{"dfs.namenode.heartbeat.recheck-interval": "50587368", "dfs.namenode.safemode.extension": "874", "dfs.namenode.delegation.key.update-interval": "1438845953", "dfs.namenode.redundancy.interval.seconds": "806", "dfs.namenode.fs-limits.max-xattr-size": "34", "dfs.datanode.http.address": "0.0.0.0:9864", "dfs.namenode.blocks.per.postponedblocks.rescan": "1604071531", "dfs.lock.suppress.warning.interval": "322363734s", "dfs.namenode.maintenance.replication.min": "1283365615", "dfs.namenode.fs-limits.max-blocks-per-file": "4962"}	["debug_000002"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.TestEncryptionZones#testBasicOperations	java.lang.IllegalArgumentException	dfs.namenode.reencrypt.throttle.limit.updater.ratio is not positive.	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.<init>(ReencryptionUpdater.java:231), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.<init>(ReencryptionHandler.java:246), org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager.<init>(EncryptionZoneManager.java:250), org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:411), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:977), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestEncryptionZones.setup(TestEncryptionZones.java:198), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestEncryptionZones/testBasicOperations/campaign/failures/debug_000000	{"dfs.namenode.reencrypt.throttle.limit.updater.ratio": "0.00"}	["debug_000000"]											
	Bug-123	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA#testHarUriWithHaUriWithNoPort	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA.testHarUriWithHaUriWithNoPort(TestHarFileSystemWithHA.java:60), org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA.testHarUriWithHaUriWithNoPort$$CONFUZZ(TestHarFileSystemWithHA.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestHarFileSystemWithHA/testHarUriWithHaUriWithNoPort/campaign/failures/debug_000000	{"dfs.client.read.shortcircuit": "true"}	["debug_000000"]											
	Bug-171	BUG	1	org.apache.hadoop.hdfs.TestPread#testPreadLocalFS	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.read(ChecksumFileSystem.java:210), org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:124), org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:117), org.apache.hadoop.hdfs.TestPread.writeFile(TestPread.java:96), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS(TestPread.java:503), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS$$CONFUZZ(TestPread.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestPread/testPreadLocalFS/campaign/failures/debug_000001	{"file.stream-buffer-size": "1686491479", "file.bytes-per-checksum": "1107160090"}	["debug_000001"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock#testAviodStaleDatanodes	java.lang.AssertionError		org.junit.Assert.fail(Assert.java:87), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertTrue(Assert.java:53), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleDatanodes(TestSortLocatedBlock.java:197), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleDatanodes$$CONFUZZ(TestSortLocatedBlock.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock/testAviodStaleDatanodes/campaign/failures/debug_000001	{"dfs.heartbeat.interval": "151", "dfs.namenode.stale.datanode.minimum.interval": "720"}	["debug_000001"]											
	invalid value	FP	1	org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashBetweenSyncLogAndPersistPaxosData	java.lang.IllegalStateException	Insufficient configured threads: required=1569269103 < max=1466230894 for QueuedThreadPool[qtp945370867]@385936f3{STARTED,8<=8<=1466230894,i=8,r=-1,q=0}[ReservedThreadExecutor@5722c149{s=0/2,p=0}]	org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:321), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81), org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:234), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.server.Server.doStart(Server.java:401), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.hdfs.qjournal.server.JournalNodeHttpServer.start(JournalNodeHttpServer.java:86), org.apache.hadoop.hdfs.qjournal.server.JournalNode.start(JournalNode.java:238), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.<init>(MiniJournalCluster.java:120), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster.<init>(MiniJournalCluster.java:47), org.apache.hadoop.hdfs.qjournal.MiniJournalCluster$Builder.build(MiniJournalCluster.java:79), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager.setup(TestQuorumJournalManager.java:111), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager/testCrashBetweenSyncLogAndPersistPaxosData/campaign/failures/debug_000002	{"dfs.edit.log.transfer.bandwidthPerSec": "487", "dfs.ha.tail-edits.qjm.rpc.max-txns": "30346", "ipc.0.backoff.enable": "false", "ipc.server.read.threadpool.size": "9305", "ipc.client.connect.max.retries.on.timeouts": "1979084840", "hadoop.ssl.keystores.factory.class": "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory", "dfs.qjm.operations.timeout": "6137s", "ipc.client.connection.idle-scan-interval.ms": "768567183", "dfs.namenode.edits.noeditlogchannelflush": "true", "hadoop.http.selector.count": "1569259701", "rpc.metrics.timeunit": "SECONDS", "hadoop.http.authentication.token.validity": "566155384"}	["debug_000002"]											
	Bug-12	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testHAStateInNamespaceInfo	java.lang.OutOfMemoryError	Java heap space	java.base/java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testHAStateInNamespaceInfo(TestFSNamesystem.java:170), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testHAStateInNamespaceInfo$$CONFUZZ(TestFSNamesystem.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem/testHAStateInNamespaceInfo/campaign/failures/debug_000001	{"dfs.namenode.read.considerLoad": "false", "dfs.lock.suppress.warning.interval": "59s", "dfs.use.dfs.network.topology": "false", "dfs.namenode.maintenance.replication.min": "683850693", "dfs.namenode.resource.check.interval": "15789", "dfs.datanode.peer.stats.enabled": "false", "dfs.namenode.fs-limits.max-blocks-per-file": "19814", "dfs.block.invalidate.limit": "318", "dfs.namenode.hosts.provider.classname": "org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager", "dfs.replication": "583", "dfs.namenode.snapshot.skiplist.max.levels": "287831635", "dfs.client.block.write.replace-datanode-on-failure.enable": "true", "hadoop.security.auth_to_local": "RULE:[1:$1] RULE:[2:$1]", "hadoop.caller.context.max.size": "1239761330", "dfs.namenode.audit.log.async": "false", "dfs.namenode.top.windows.minutes": "556,4723323,538836", "dfs.namenode.write-lock-reporting-threshold-ms": "308", "dfs.namenode.avoid.read.stale.datanode": "true", "dfs.namenode.startup": "REGULAR", "dfs.storage.policy.enabled": "false", "dfs.storage.policy.permissions.superuser-only": "false", "dfs.namenode.list.cache.pools.num.responses": "1213990387", "ssl.client.truststore.location": "", "dfs.namenode.lease-recheck-interval-ms": "14481", "dfs.namenode.ec.userdefined.policy.allowed": "false", "dfs.checksum.type": "CRC32C", "dfs.namenode.stale.datanode.minimum.interval": "973", "dfs.namenode.ec.policies.max.cellsize": "10502", "ssl.client.truststore.type": "jks", "hadoop.security.groups.cache.background.reload.threads": "32", "dfs.hosts.exclude": "", "dfs.namenode.replication.min": "181", "dfs.content-summary.sleep-microsec": "5889", "dfs.provided.storage.id": "DS-PROVIDED", "dfs.namenode.acls.enabled": "true", "net.topology.script.number.args": "23251", "hadoop.security.group.mapping": "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback", "dfs.namenode.fslock.fair": "false", "dfs.permissions.enabled": "false", "hadoop.ssl.keystores.factory.class": "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory", "dfs.permissions.ContentSummary.subAccess": "false", "hadoop.security.groups.cache.warn.after.ms": "10980", "dfs.namenode.stale.datanode.interval": "324", "dfs.namenode.lazypersist.file.scrub.interval.sec": "805", "hadoop.security.auth_to_local.mechanism": "MIT", "dfs.namenode.blockreport.max.lock.hold.time": "22600", "dfs.namenode.provided.enabled": "true", "dfs.namenode.safemode.min.datanodes": "856362901", "hadoop.ssl.enabled.protocols": "TLSv1.2", "dfs.namenode.snapshot.skip.capture.accesstime-only-change": "false", "dfs.replication.max": "687", "dfs.namenode.read-lock-reporting-threshold-ms": "1933823618", "fs.trash.interval": "1493", "dfs.datanode.https.address": "0.0.0.0:9865", "dfs.ha.standby.checkpoints": "false", "dfs.block.placement.ec.classname": "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant", "dfs.namenode.replqueue.threshold-pct": "0.26422828435897827", "dfs.namenode.enable.log.stale.datanode": "true", "dfs.block.access.token.enable": "false", "dfs.blocksize": "143", "hadoop.security.dns.log-slow-lookups.enabled": "false", "dfs.datanode.fileio.profiling.sampling.percentage": "14971", "dfs.namenode.edekcacheloader.initial.delay.ms": "560", "dfs.ls.limit": "883719821", "dfs.namenode.write.stale.datanode.ratio": "1.0f", "dfs.encrypt.data.transfer": "false", "io.file.buffer.size": "32252", "dfs.namenode.max-lock-hold-to-release-lease-ms": "29883", "dfs.corruptfilesreturned.max": "14300", "dfs.namenode.xattrs.enabled": "false", "dfs.client-write-packet-size": "3288", "dfs.namenode.checkpoint.txns": "31348", "dfs.namenode.corrupt.block.delete.immediately.enabled": "false", "dfs.protected.subdirectories.enable": "true", "dfs.namenode.list.openfiles.num.responses": "157", "dfs.namenode.safemode.threshold-pct": "0.26803940534591675", "dfs.hosts": "", "dfs.namenode.block-placement-policy.default.prefer-local-node": "false", "dfs.namenode.replication.max-streams": "190", "dfs.namenode.caching.enabled": "true", "dfs.namenode.quota.init-threads": "521", "hadoop.security.key.provider.path": "", "dfs.namenode.file.close.num-committed-allowed": "75", "dfs.quota.by.storage.type.enabled": "false", "dfs.namenode.reconstruction.pending.timeout-sec": "31030", "dfs.namenode.replication.work.multiplier.per.iteration": "12765", "dfs.namenode.lock.detailed-metrics.enabled": "false", "dfs.namenode.delegation.token.renew-interval": "649", "dfs.namenode.retrycache.heap.percent": "0.9f", "hadoop.security.dns.log-slow-lookups.threshold.ms": "358", "dfs.namenode.reject-unresolved-dn-topology-mapping": "true", "hadoop.security.authentication": "kerberos", "dfs.namenode.max-corrupt-file-blocks-returned": "1968657862", "dfs.namenode.replication.max-streams-hard-limit": "11883", "dfs.namenode.top.num.users": "202", "hadoop.security.token.service.use_ip": "false", "hadoop.kerberos.min.seconds.before.relogin": "1657223448", "dfs.namenode.edekcacheloader.interval.ms": "16430", "ssl.client.stores.reload.interval": "495639355", "dfs.namenode.fs-limits.max-xattrs-per-inode": "5345", "dfs.namenode.edit.log.autoroll.multiplier.threshold": "-0.33380961418151855", "dfs.namenode.edit.log.autoroll.check.interval.ms": "593080665", "dfs.namenode.top.window.num.buckets": "23535", "dfs.namenode.max-num-blocks-to-log": "302", "dfs.namenode.retrycache.expirytime.millis": "1112412301", "dfs.namenode.snapshot.max.limit": "821", "hadoop.ssl.client.conf": "ssl-client.xml", "dfs.namenode.max.slowpeer.collect.nodes": "4599", "dfs.content-summary.limit": "762412041", "dfs.namenode.fs-limits.max-component-length": "4437", "dfs.namenode.enable.retrycache": "false", "hadoop.security.groups.cache.background.reload": "false", "dfs.namenode.posix.acl.inheritance.enabled": "false", "dfs.datanode.outliers.report.interval": "10504h", "hadoop.user.group.static.mapping.overrides": "dr.who=;", "dfs.datanode.ipc.address": "0.0.0.0:9867", "dfs.bytes-per-checksum": "6085", "dfs.namenode.max.objects": "2127861269", "dfs.namenode.read.considerStorageType": "false", "dfs.namenode.path.based.cache.retry.interval.ms": "788370123", "dfs.block.misreplication.processing.limit": "29612", "dfs.client.block.write.replace-datanode-on-failure.policy": "NEVER", "hadoop.ssl.hostname.verifier": "STRICT", "dfs.storage.policy.satisfier.mode": "external", "dfs.namenode.tolerate.heartbeat.multiplier": "13571", "hadoop.security.groups.negative-cache.secs": "292", "hadoop.caller.context.enabled": "false", "dfs.namenode.audit.log.token.tracking.id": "false", "ssl.client.keystore.type": "jks", "dfs.permissions.superusergroup": "mpfwqgsflobicqxgruzlxouslgbklpbllreqvtebqxdootcftfbvzakokobkoljxnnloyfcidpevmgroup", "dfs.namenode.path.based.cache.refresh.interval.ms": "23203", "dfs.namenode.safemode.replication.min": "308130438", "dfs.namenode.fs-limits.max-directory-items": "4423431", "dfs.namenode.snapshot.skiplist.interval": "21171", "dfs.namenode.safemode.extension": "678", "dfs.namenode.full.block.report.lease.length.ms": "24465", "dfs.heartbeat.interval": "496", "dfs.namenode.delegation.token.always-use": "false", "dfs.namenode.redundancy.interval.seconds": "621", "dfs.namenode.list.reencryption.status.num.responses": "411616124", "dfs.datanode.http.address": "0.0.0.0:9864", "dfs.permissions.allow.owner.set.quota": "false", "dfs.namenode.blocks.per.postponedblocks.rescan": "1585230118"}	["debug_000001"]											
	Bug-125	BUG	1	org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testUnreachableObserverWithMultiple	org.junit.ComparisonFailure	expected:<namenode[2].test:8020> but was:<namenode[3].test:8020>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.assertHandledBy(TestObserverReadProxyProvider.java:336), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple(TestObserverReadProxyProvider.java:193), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple$$CONFUZZ(TestObserverReadProxyProvider.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider/testUnreachableObserverWithMultiple/campaign/failures/debug_000000	{"dfs.client.failover.random.order": "true"}	["debug_000000"]											
	Bug-11	BUG	1	org.apache.hadoop.hdfs.TestDFSMkdirs#testMkdirRpcNonCanonicalPath	java.lang.OutOfMemoryError	Java heap space	org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534), org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:511), org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:152), org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:954), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:765), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestDFSMkdirs.testMkdirRpcNonCanonicalPath(TestDFSMkdirs.java:138), org.apache.hadoop.hdfs.TestDFSMkdirs.testMkdirRpcNonCanonicalPath$$CONFUZZ(TestDFSMkdirs.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestDFSMkdirs/testMkdirRpcNonCanonicalPath/campaign/failures/debug_000002	{"dfs.namenode.blocks.per.postponedblocks.rescan": "1120636424", "hadoop.http.selector.count": "732159788"}	["debug_000002"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageTypeAndLoad	org.junit.ComparisonFailure	expected:<IP-[0-1]> but was:<IP-[2-4]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad(TestDatanodeManager.java:823), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testGetBlockLocationConsiderStorageTypeAndLoad/campaign/failures/debug_000001	{"dfs.namenode.stale.datanode.interval": "98", "dfs.heartbeat.interval": "1991920600", "dfs.block.invalidate.limit": "390133698", "dfs.namenode.stale.datanode.minimum.interval": "1916456597"}	["debug_000001"]											
	Bug-29	BUG	1	org.apache.hadoop.hdfs.TestEncryptionZones#testListEncryptionZonesWithSnapshots	java.lang.OutOfMemoryError	Java heap space	java.base/java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionBatch.<init>(ReencryptionHandler.java:488), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.<init>(ReencryptionHandler.java:248), org.apache.hadoop.hdfs.server.namenode.EncryptionZoneManager.<init>(EncryptionZoneManager.java:250), org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:411), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:977), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestEncryptionZones.setup(TestEncryptionZones.java:198), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299), org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293), java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264), java.base/java.lang.Thread.run(Thread.java:829),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestEncryptionZones/testListEncryptionZonesWithSnapshots/campaign/failures/debug_000002	{"dfs.namenode.reencrypt.batch.size": "1262602843", "dfs.namenode.blockreport.queue.size": "1329170972"}	["debug_000002"]											
	Bug-167	BUG	1	org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs#testNameEditsConfigsFailure	java.lang.NullPointerException		org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs.testNameEditsConfigsFailure(TestNameEditsConfigs.java:450), org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs.testNameEditsConfigsFailure$$CONFUZZ(TestNameEditsConfigs.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.namenode.TestNameEditsConfigs/testNameEditsConfigsFailure/campaign/failures/debug_000000	{"dfs.datanode.cache.revocation.polling.ms": "383819019"}	["debug_000000"]											
	Hardcode Assertion	FP	1	org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testgoodScript	java.lang.AssertionError	Expected: is "IP-4"     but: was "IP-2"	org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20), org.junit.Assert.assertThat(Assert.java:964), org.junit.Assert.assertThat(Assert.java:930), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:443), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testgoodScript(TestDatanodeManager.java:322), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testgoodScript$$CONFUZZ(TestDatanodeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:41),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager/testgoodScript/campaign/failures/debug_000001	{"dfs.namenode.avoid.read.stale.datanode": "true", "dfs.namenode.stale.datanode.minimum.interval": "170992440", "dfs.namenode.stale.datanode.interval": "5", "dfs.heartbeat.interval": "2134810349"}	["debug_000001"]											
	Bug-189	BUG	1	org.apache.hadoop.hdfs.TestPread#testPreadLocalFS	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.read(ChecksumFileSystem.java:210), org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:124), org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:117), org.apache.hadoop.hdfs.TestPread.writeFile(TestPread.java:96), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS(TestPread.java:503), org.apache.hadoop.hdfs.TestPread.testPreadLocalFS$$CONFUZZ(TestPread.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:101), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/meringue/org.apache.hadoop.hdfs.TestPread/testPreadLocalFS/campaign/failures/debug_000000	{"file.bytes-per-checksum": "1616848183", "io.file.buffer.size": "1587007752"}	["debug_000000"]											