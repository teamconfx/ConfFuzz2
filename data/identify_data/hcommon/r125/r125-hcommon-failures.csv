test;failure_id;config;exception;message;stacktrace
org.apache.hadoop.security.TestUserGroupInformation#testConstructorWithKerberos;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.crypto.key.TestKeyProviderFactory#testUserProvider;id_000000;[hadoop.security.key.default.bitlength=1751887613];java.io.IOException;Wrong key length. Required 1751887613, but got 128;[org.apache.hadoop.crypto.key.UserProvider.createKey(UserProvider.java:91), org.apache.hadoop.crypto.key.TestKeyProviderFactory.checkSpecificProvider(TestKeyProviderFactory.java:120), org.apache.hadoop.crypto.key.TestKeyProviderFactory.testUserProvider(TestKeyProviderFactory.java:204), org.apache.hadoop.crypto.key.TestKeyProviderFactory.testUserProvider$$CONFUZZ(TestKeyProviderFactory.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.crypto.key.TestKeyProviderFactory#testFactory;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;Invalid value of fs.creation.parallel.count: 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:415), org.apache.hadoop.fs.FileSystem$Cache.<init>(FileSystem.java:3514), org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:206), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.crypto.key.JavaKeyStoreProvider.<init>(JavaKeyStoreProvider.java:133)]
org.apache.hadoop.ipc.TestMultipleProtocolServer#testPBService;id_000000;[ipc.server.read.threadpool.size=1537083334];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.util.concurrent.LinkedBlockingQueue could not be constructed.),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.ipc.TestRPC#testErrorMsgForInsecureClient;id_000000;[ipc.server.read.threadpool.size=959158792];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371)]
org.apache.hadoop.ipc.TestRPC#testConnectionPing;id_000000;[ipc.server.read.threadpool.size=1088259942];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371)]
org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal#testExistingWrite;id_000000;[file.bytes-per-checksum=1958588727];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal#testExistingWrite2;id_000000;[file.bytes-per-checksum=911734008];java.lang.NegativeArraySizeException;-384328520;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.TestDoAsEffectiveUser#testRealUserGroupAuthorizationFailure;id_000000;[ipc.server.read.threadpool.size=1332981928];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371)]
org.apache.hadoop.security.TestDoAsEffectiveUser#testRealUserAuthorizationSuccess;id_000000;[ipc.server.read.threadpool.size=955210543];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371)]
org.apache.hadoop.fs.viewfs.TestViewFsOverloadSchemeListStatus#testViewFSOverloadSchemeWithoutAnyMountLinks;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme.initialize(ViewFileSystemOverloadScheme.java:161), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]
org.apache.hadoop.fs.viewfs.TestViewFsOverloadSchemeListStatus#testListStatusACL;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme.initialize(ViewFileSystemOverloadScheme.java:161), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeLocalFileSystem#testLocalFsLinkSlashMerge;id_000000;[file.stream-buffer-size=576274247, io.file.buffer.size=1499095007];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeLocalFileSystem#testLocalFsLinkSlashMerge;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestTrash#testExistingFileTrash;id_000000;[file.bytes-per-checksum=738606917];java.lang.NegativeArraySizeException;-1942472339;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestTrash#testExistingFileTrash;id_000001;[file.bytes-per-checksum=1480878323, io.file.buffer.size=1274251224];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestTrash#testExistingFileTrash;id_000002;[file.bytes-per-checksum=2097364921];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestTrash#testExistingFileTrash;id_000003;[file.bytes-per-checksum=2146302804];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestTrash#testExistingFileTrash;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestTrash#testExistingFileTrash;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestTrash#testExpungeWithFileSystem;id_000000;[file.stream-buffer-size=781002202, io.file.buffer.size=1494291429];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestTrash#testExpungeWithFileSystem;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestTrash#testExpungeWithFileSystem;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestTrash#testTrash;id_000000;[file.bytes-per-checksum=1843284096];java.lang.NegativeArraySizeException;-590312320;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestTrash#testTrash;id_000001;[file.bytes-per-checksum=689205179];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestTrash#testTrash;id_000002;[file.bytes-per-checksum=1487310987];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestTrash#testTrash;id_000003;[file.bytes-per-checksum=680956527];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestTrash#testTrash;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestTrash#testTrash;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig#testLocalFsCreateAndDelete;id_000000;[file.stream-buffer-size=2065329502, io.file.buffer.size=1699306219];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig#testLocalFsCreateAndDelete;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig#testLocalFsLinkSlashMerge;id_000000;[file.stream-buffer-size=2069528577, io.file.buffer.size=202143064];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig#testLocalFsLinkSlashMerge;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestArrayFile#testEmptyFile;id_000000;[io.file.buffer.size=506601292];java.lang.NegativeArraySizeException;-99968201;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestArrayFile#testEmptyFile;id_000002;[io.file.buffer.size=1427436756];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestViewfsFileStatus#testFileStatusSerialziation;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewfsFileStatus#testFileStatusSerialziation;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)]
org.apache.hadoop.io.TestSetFile#testSetFileAccessMethods;id_000000;[io.file.buffer.size=1661480239];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1307), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateUnderFileSubdir;id_000000;[file.bytes-per-checksum=404167900];java.lang.NegativeArraySizeException;-657456196;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateUnderFileSubdir;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateUnderFileSubdir;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateUnderFileSubdir;id_000004;[file.bytes-per-checksum=2022779952];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteDeepEmptyDir;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenFileExceptionallyTranslating;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestPath#testURI;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteEmptyDirNonRecursive;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testCreateDirWithExistingDir;id_000000;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=774827675, hadoop.kerberos.min.seconds.before.relogin=652671444, hadoop.security.groups.cache.secs=1468964876, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=50563724h, fs.permissions.umask-mode=277, fs.local.block.size=1315377007, fs.creation.parallel.count=1343671679, fs.file.impl.disable.cache=true, fs.automatic.close=true];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testInputStreamClosedTwice;id_000000;[file.bytes-per-checksum=2082852352];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testInputStreamClosedTwice;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=740060837, hadoop.security.auth_to_local.mechanism=MIT, hadoop.kerberos.min.seconds.before.relogin=301487871, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.service.shutdown.timeout=8563169h, fs.permissions.umask-mode=644, file.stream-buffer-size=832785283, fs.file.impl.disable.cache=true, io.file.buffer.size=826008851];java.lang.NegativeArraySizeException;-1780290444;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testInputStreamClosedTwice;id_000004;[hadoop.security.token.service.use_ip=true, hadoop.service.shutdown.timeout=0166785s, fs.permissions.umask-mode=644, file.stream-buffer-size=125, hadoop.security.groups.negative-cache.secs=8932096, fs.creation.parallel.count=369616231, fs.file.impl.disable.cache=true, io.file.buffer.size=826008851];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testInputStreamClosedTwice;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneBlock;id_000000;[io.file.buffer.size=2120136289];java.lang.NegativeArraySizeException;-729137901;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneBlock;id_000002;[file.bytes-per-checksum=1447185129];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneBlock;id_000004;[io.file.buffer.size=1734502960, file.bytes-per-checksum=113152096];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneBlock;id_000005;[io.file.buffer.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneBlock;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testRenameAcrossFs;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testFileStatusBlocksizeEmptyFile;id_000000;[io.file.buffer.size=2094162973];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testFileStatusBlocksizeEmptyFile;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testFileStatusBlocksizeEmptyFile;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileToFile;id_000000;[file.bytes-per-checksum=1798413011];java.lang.NegativeArraySizeException;-994152085;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileToFile;id_000001;[file.bytes-per-checksum=2069424844];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileToFile;id_000002;[file.bytes-per-checksum=2143037987];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileToFile;id_000003;[file.stream-buffer-size=1058810128, io.file.buffer.size=286168366, file.bytes-per-checksum=96398552];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileToFile;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileToFile;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testMkdirsWithUmask;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testInputStreamStatisticRead;id_000000;[file.bytes-per-checksum=66504073, io.file.buffer.size=1516942297];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testInputStreamStatisticRead;id_000002;[file.bytes-per-checksum=359393270];java.lang.NegativeArraySizeException;-1060427866;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameDirIntoExistingDir;id_000000;[io.file.buffer.size=2123583922];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameDirIntoExistingDir;id_000001;[io.file.buffer.size=916279730];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:419), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:409)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameDirIntoExistingDir;id_000002;[io.file.buffer.size=967987634];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testUsingWeightedTimeCostProviderNoRequests;id_000000;[decay-scheduler.metrics.top.user.count=0];java.lang.IllegalArgumentException;the number of top users for scheduler metrics must be at least 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:246), org.apache.hadoop.ipc.TestDecayRpcScheduler.getSchedulerWithWeightedTimeCostProvider(TestDecayRpcScheduler.java:390), org.apache.hadoop.ipc.TestDecayRpcScheduler.testUsingWeightedTimeCostProviderNoRequests(TestDecayRpcScheduler.java:374), org.apache.hadoop.ipc.TestDecayRpcScheduler.testUsingWeightedTimeCostProviderNoRequests$$CONFUZZ(TestDecayRpcScheduler.java)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testUsingWeightedTimeCostProviderNoRequests;id_000001;[decay-scheduler.metrics.top.user.count=0];java.lang.IllegalArgumentException;Decay Factor must be between 0 and 1;[org.apache.hadoop.ipc.DecayRpcScheduler.parseDecayFactor(DecayRpcScheduler.java:311), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:227), org.apache.hadoop.ipc.TestDecayRpcScheduler.getSchedulerWithWeightedTimeCostProvider(TestDecayRpcScheduler.java:390), org.apache.hadoop.ipc.TestDecayRpcScheduler.testUsingWeightedTimeCostProviderNoRequests(TestDecayRpcScheduler.java:374), org.apache.hadoop.ipc.TestDecayRpcScheduler.testUsingWeightedTimeCostProviderNoRequests$$CONFUZZ(TestDecayRpcScheduler.java)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testParseFactor;id_000000;[decay-scheduler.metrics.top.user.count=0];java.lang.IllegalArgumentException;the number of top users for scheduler metrics must be at least 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:246), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParseFactor(TestDecayRpcScheduler.java:89), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParseFactor$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testParseFactor;id_000002;[ipc.3.decay-scheduler.period-ms=0];java.lang.IllegalArgumentException;Decay Factor must be between 0 and 1;[org.apache.hadoop.ipc.DecayRpcScheduler.parseDecayFactor(DecayRpcScheduler.java:311), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:227), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParseFactor(TestDecayRpcScheduler.java:89), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParseFactor$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testParseFactor;id_000003;[ipc.3.decay-scheduler.period-ms=0];java.lang.IllegalArgumentException;Period millis must be >= 0;[org.apache.hadoop.ipc.DecayRpcScheduler.parseDecayPeriodMillis(DecayRpcScheduler.java:332), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:228), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParseFactor(TestDecayRpcScheduler.java:89), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParseFactor$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testUsingWeightedTimeCostProvider;id_000000;[decay-scheduler.metrics.top.user.count=0];java.lang.IllegalArgumentException;the number of top users for scheduler metrics must be at least 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:246), org.apache.hadoop.ipc.TestDecayRpcScheduler.getSchedulerWithWeightedTimeCostProvider(TestDecayRpcScheduler.java:390), org.apache.hadoop.ipc.TestDecayRpcScheduler.testUsingWeightedTimeCostProvider(TestDecayRpcScheduler.java:290), org.apache.hadoop.ipc.TestDecayRpcScheduler.testUsingWeightedTimeCostProvider$$CONFUZZ(TestDecayRpcScheduler.java)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testUsingWeightedTimeCostProvider;id_000001;[ipc.15.weighted-cost.lockshared=0];java.lang.IllegalArgumentException;Decay Factor must be between 0 and 1;[org.apache.hadoop.ipc.DecayRpcScheduler.parseDecayFactor(DecayRpcScheduler.java:311), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:227), org.apache.hadoop.ipc.TestDecayRpcScheduler.getSchedulerWithWeightedTimeCostProvider(TestDecayRpcScheduler.java:390), org.apache.hadoop.ipc.TestDecayRpcScheduler.testUsingWeightedTimeCostProvider(TestDecayRpcScheduler.java:290), org.apache.hadoop.ipc.TestDecayRpcScheduler.testUsingWeightedTimeCostProvider$$CONFUZZ(TestDecayRpcScheduler.java)]
org.apache.hadoop.ha.TestZKFailoverController#testFormatSetsAcls;id_000000;[ha.zookeeper.parent-znode=/uiuc];org.apache.zookeeper.KeeperException$NoNodeException;KeeperErrorCode = NoNode for /hadoop-ha;[org.apache.zookeeper.KeeperException.create(KeeperException.java:118), org.apache.zookeeper.KeeperException.create(KeeperException.java:54), org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:2131), org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:2160), org.apache.hadoop.ha.TestZKFailoverController.testFormatSetsAcls(TestZKFailoverController.java:209)]
org.apache.hadoop.ha.TestZKFailoverController#testFormatSetsAcls;id_000001;[hadoop.security.credential.clear-text-fallback=false];org.apache.zookeeper.KeeperException$NoAuthException;KeeperErrorCode = NoAuth for /hadoop-ha/dummy-cluster;[org.apache.zookeeper.KeeperException.create(KeeperException.java:120), org.apache.zookeeper.KeeperException.create(KeeperException.java:54), org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1041), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1038)]
org.apache.hadoop.ha.TestZKFailoverController#testFormatSetsAcls;id_000004;[ha.zookeeper.session-timeout.ms=0];java.lang.IllegalArgumentException;Invalid ZK session timeout 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:192), org.apache.hadoop.ha.ZKFailoverController.initZK(ZKFailoverController.java:367), org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:200), org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:63), org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:181)]
org.apache.hadoop.ha.TestZKFailoverController#testFencingMustBeConfigured;id_000000;[hadoop.security.credential.clear-text-fallback=false];org.apache.zookeeper.KeeperException$NoAuthException;KeeperErrorCode = NoAuth for /hadoop-ha/dummy-cluster;[org.apache.zookeeper.KeeperException.create(KeeperException.java:120), org.apache.zookeeper.KeeperException.create(KeeperException.java:54), org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1041), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1038)]
org.apache.hadoop.ha.TestZKFailoverController#testFencingMustBeConfigured;id_000002;[ha.zookeeper.session-timeout.ms=0];java.lang.IllegalArgumentException;Invalid ZK session timeout 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:192), org.apache.hadoop.ha.ZKFailoverController.initZK(ZKFailoverController.java:367), org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:200), org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:63), org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:181)]
org.apache.hadoop.io.TestSequenceFile#testCreateUsesFsArg;id_000000;[io.file.buffer.size=2121438875];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testCreateUsesFsArg;id_000001;[io.file.buffer.size=1493418224];java.lang.NegativeArraySizeException;-1307104176;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFile#testCreateUsesFsArg;id_000002;[file.bytes-per-checksum=1061290155];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.TestSequenceFile#testCreateUsesFsArg;id_000003;[io.file.buffer.size=1788761627];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testCancelTokenSingleManager;id_000000;[zk-dt-secret-manager.zkSessionTimeout=0];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[java.lang.Object.wait(Native Method), java.lang.Object.wait(Object.java:328), org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1529), org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1512), org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:2016)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testCancelTokenSingleManager;id_000001;[zk-dt-secret-manager.zkNumRetries=0];java.lang.ArithmeticException;/ by zero;[org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.<init>(ZKDelegationTokenSecretManager.java:210), org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager.<init>(DelegationTokenManager.java:99), org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.<init>(DelegationTokenManager.java:120), org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testCancelTokenSingleManager(TestZKDelegationTokenSecretManager.java:299), org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testCancelTokenSingleManager$$CONFUZZ(TestZKDelegationTokenSecretManager.java)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testMultiNodeCompeteForSeqNum;id_000000;[zk-dt-secret-manager.zkNumRetries=0];java.lang.ArithmeticException;/ by zero;[org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.<init>(ZKDelegationTokenSecretManager.java:210), org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager.<init>(DelegationTokenManager.java:99), org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.<init>(DelegationTokenManager.java:120), org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testMultiNodeCompeteForSeqNum(TestZKDelegationTokenSecretManager.java:228), org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testMultiNodeCompeteForSeqNum$$CONFUZZ(TestZKDelegationTokenSecretManager.java)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testMultiNodeCompeteForSeqNum;id_000001;[zk-dt-secret-manager.zkSessionTimeout=0];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[jdk.internal.misc.Unsafe.park(Native Method), java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234), java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1079), java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1369), java.util.concurrent.CountDownLatch.await(CountDownLatch.java:278)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testRenewTokenSingleManager;id_000000;[zk-dt-secret-manager.token.seqnum.batch.size=0];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[java.lang.Object.wait(Native Method), java.lang.Object.wait(Object.java:328), org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1529), org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1512), org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:2016)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testRenewTokenSingleManager;id_000001;[zk-dt-secret-manager.zkNumRetries=0];java.lang.ArithmeticException;/ by zero;[org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.<init>(ZKDelegationTokenSecretManager.java:210), org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager.<init>(DelegationTokenManager.java:99), org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.<init>(DelegationTokenManager.java:120), org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testRenewTokenSingleManager(TestZKDelegationTokenSecretManager.java:279), org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testRenewTokenSingleManager$$CONFUZZ(TestZKDelegationTokenSecretManager.java)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlob;id_000000;[file.bytes-per-checksum=2078686965];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlob;id_000001;[fs.file.impl.disable.cache=true];java.lang.NegativeArraySizeException;-360309307;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlob;id_000002;[file.bytes-per-checksum=1647644194];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlob;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlob;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFileSerialization#testJavaSerialization;id_000000;[file.bytes-per-checksum=1610850989];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.TestSequenceFileSerialization#testJavaSerialization;id_000001;[file.bytes-per-checksum=294965691];java.lang.NegativeArraySizeException;-1640276077;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFileSerialization#testJavaSerialization;id_000002;[io.file.buffer.size=698235198];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFileSerialization#testJavaSerialization;id_000003;[file.bytes-per-checksum=2087692171];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.TestSequenceFileSerialization#testJavaSerialization;id_000005;[io.file.buffer.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.TestSequenceFileSerialization#testJavaSerialization;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testBuilderCreateRecursive;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusWithMultipleWildCardMatches;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testOpenFileLazyFail;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testInputStreamClosedTwice;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testOutputStreamClosedTwice;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testWriteReadAndDeleteEmptyFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testBuilderCreateExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testCreateFlagAppendNonExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testOpenFileUnknownOption;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithMultiplePathWildcardsAndNonTrivialFilter;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithEmptyPathResults;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWorkingDirectory;id_000000;[hadoop.service.shutdown.timeout=55640s, fs.permissions.umask-mode=755, file.stream-buffer-size=489330601, fs.local.block.size=1082111316, fs.file.impl.disable.cache=true, io.file.buffer.size=197493940];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.FSMainOperationsBaseTest.testWorkingDirectory(FSMainOperationsBaseTest.java:169)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWorkingDirectory;id_000002;[file.stream-buffer-size=2086279596];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWorkingDirectory;id_000003;[file.bytes-per-checksum=1777174566];java.lang.NegativeArraySizeException;-1185298090;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWorkingDirectory;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWorkingDirectory;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusSomeMatchesInDirectories;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteTwoBlocks;id_000000;[io.file.buffer.size=1235509949];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.FileSystemContractBaseTest.writeAndRead(FileSystemContractBaseTest.java:937)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteTwoBlocks;id_000002;[hadoop.service.shutdown.timeout=856h, fs.permissions.umask-mode=027, fs.creation.parallel.count=584896383, fs.file.impl.disable.cache=true, io.file.buffer.size=392770999, fs.automatic.close=true, hadoop.security.groups.cache.warn.after.ms=8650886];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringFileSystem;id_000000;[file.bytes-per-checksum=1311721705];java.lang.NegativeArraySizeException;-1079406543;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.createNonRecursive(ChecksumFileSystem.java:565), org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder.build(FileSystem.java:4523)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringFileSystem;id_000002;[file.bytes-per-checksum=1602633720];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringFileSystem;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringFileSystem;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameFileToNonExistentDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameFileToDestinationWithParentFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneAndAHalfBlocks;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testInputStreamClosedTwice;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGetWrappedInputStream;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteTwoBlocks;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testOutputStreamClosedTwice;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameFileToItself;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteEmptyFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameFileAsExistingDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneBlock;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestLocalFSFileContextMainOperations#testCreateFlagAppendCreateOverwrite;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1825375633, hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1383624237, hadoop.security.authentication=simple, fs.client.resolve.remote.symlinks=true, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1763767710, fs.local.block.size=563009733, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=28977499, hadoop.kerberos.keytab.login.autorenewal.enabled=false, fs.permissions.umask-mode=755, hadoop.security.groups.negative-cache.secs=1378165279, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.creation.parallel.count=278268518, hadoop.security.groups.cache.warn.after.ms=440676086];org.apache.hadoop.HadoopIllegalArgumentException;[CREATE, OVERWRITE, APPEND]Both append and overwrite options cannot be enabled.;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:159), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)]
org.apache.hadoop.fs.TestLocalFSFileContextMainOperations#testNullCreateFlag;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1772794971, hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=757310896, hadoop.security.authentication=simple, fs.client.resolve.remote.symlinks=true, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1539307689, fs.local.block.size=847095269, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1735725013, hadoop.kerberos.keytab.login.autorenewal.enabled=false, fs.permissions.umask-mode=640, hadoop.security.groups.negative-cache.secs=641038703, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.creation.parallel.count=245111005, hadoop.security.groups.cache.warn.after.ms=1104806822];org.apache.hadoop.HadoopIllegalArgumentException;null does not specify any options;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:151), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)]
org.apache.hadoop.io.compress.lz4.TestLz4CompressorDecompressor#testLz4Compatibility;id_000000;[io.compression.codec.lz4.buffersize=1564476604];java.lang.OutOfMemoryError;Direct buffer memory;[java.nio.Bits.reserveMemory(Bits.java:175), java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118), java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317), org.apache.hadoop.io.compress.lz4.Lz4Decompressor.<init>(Lz4Decompressor.java:69), org.apache.hadoop.io.compress.Lz4Codec.createDecompressor(Lz4Codec.java:175)]
org.apache.hadoop.io.compress.lz4.TestLz4CompressorDecompressor#testLz4Compatibility;id_000002;[file.bytes-per-checksum=1125591493, io.file.buffer.size=1327544000];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.BlockDecompressorStream.<init>(BlockDecompressorStream.java:50), org.apache.hadoop.io.compress.Lz4Codec.createInputStream(Lz4Codec.java:150), org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:2055)]
org.apache.hadoop.io.compress.lz4.TestLz4CompressorDecompressor#testLz4Compatibility;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.io.compress.lz4.TestLz4CompressorDecompressor#testLz4Compatibility;id_000005;[io.compression.codec.lz4.buffersize=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:60), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.BlockDecompressorStream.<init>(BlockDecompressorStream.java:50), org.apache.hadoop.io.compress.Lz4Codec.createInputStream(Lz4Codec.java:150), org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:2055)]
org.apache.hadoop.io.compress.lz4.TestLz4CompressorDecompressor#testLz4Compatibility;id_000007;[io.compression.codec.lz4.buffersize=725351466];java.lang.OutOfMemoryError;Direct buffer memory;[java.nio.Bits.reserveMemory(Bits.java:175), java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118), java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317), org.apache.hadoop.io.compress.lz4.Lz4Decompressor.<init>(Lz4Decompressor.java:68), org.apache.hadoop.io.compress.Lz4Codec.createDecompressor(Lz4Codec.java:175)]
org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile;id_000000;[io.file.buffer.size=1929583239];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile;id_000001;[io.file.buffer.size=1446890665];java.lang.NegativeArraySizeException;-1035490332;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile;id_000002;[io.file.buffer.size=993779892];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile;id_000003;[io.compression.codec.snappy.buffersize=1771838905];java.lang.OutOfMemoryError;Direct buffer memory;[java.nio.Bits.reserveMemory(Bits.java:175), java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118), java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317), org.apache.hadoop.io.compress.snappy.SnappyCompressor.<init>(SnappyCompressor.java:60), org.apache.hadoop.io.compress.SnappyCodec.createCompressor(SnappyCodec.java:116)]
org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile;id_000004;[io.file.buffer.size=1848490608];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile;id_000008;[io.compression.codec.snappy.buffersize=2112495057];java.lang.OutOfMemoryError;Direct buffer memory;[java.nio.Bits.reserveMemory(Bits.java:175), java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118), java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317), org.apache.hadoop.io.compress.snappy.SnappyCompressor.<init>(SnappyCompressor.java:59), org.apache.hadoop.io.compress.SnappyCodec.createCompressor(SnappyCodec.java:116)]
org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile;id_000014;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.compress.TestCodec#testSnappyMapFile;id_000017;[io.compression.codec.snappy.buffersize=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:42), org.apache.hadoop.io.compress.BlockCompressorStream.<init>(BlockCompressorStream.java:56), org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:92), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1307), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194)]
org.apache.hadoop.io.TestSequenceFile#testClose;id_000000;[io.file.buffer.size=1670638360];java.lang.NegativeArraySizeException;-626297453;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFile#testClose;id_000001;[io.file.buffer.size=1546486607];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testClose;id_000002;[io.file.buffer.size=740723963];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testListLocatedStatus;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testListStatusIterator;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testAccumulate;id_000000;[decay-scheduler.metrics.top.user.count=0];java.lang.IllegalArgumentException;Decay Factor must be between 0 and 1;[org.apache.hadoop.ipc.DecayRpcScheduler.parseDecayFactor(DecayRpcScheduler.java:311), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:227), org.apache.hadoop.ipc.TestDecayRpcScheduler.testAccumulate(TestDecayRpcScheduler.java:139), org.apache.hadoop.ipc.TestDecayRpcScheduler.testAccumulate$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testAccumulate;id_000001;[decay-scheduler.metrics.top.user.count=0];java.lang.IllegalArgumentException;the number of top users for scheduler metrics must be at least 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:246), org.apache.hadoop.ipc.TestDecayRpcScheduler.testAccumulate(TestDecayRpcScheduler.java:139), org.apache.hadoop.ipc.TestDecayRpcScheduler.testAccumulate$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testPriority;id_000000;[decay-scheduler.metrics.top.user.count=0];java.lang.IllegalArgumentException;Decay Factor must be between 0 and 1;[org.apache.hadoop.ipc.DecayRpcScheduler.parseDecayFactor(DecayRpcScheduler.java:311), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:227), org.apache.hadoop.ipc.TestDecayRpcScheduler.testPriority(TestDecayRpcScheduler.java:215), org.apache.hadoop.ipc.TestDecayRpcScheduler.testPriority$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testPriority;id_000001;[decay-scheduler.metrics.top.user.count=0];java.lang.IllegalArgumentException;the number of top users for scheduler metrics must be at least 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:246), org.apache.hadoop.ipc.TestDecayRpcScheduler.testPriority(TestDecayRpcScheduler.java:215), org.apache.hadoop.ipc.TestDecayRpcScheduler.testPriority$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.http.TestAuthenticationSessionCookie#testPersistentCookie;id_000000;[hadoop.http.selector.count=1111660663];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.OutOfMemoryError(Java heap space),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testListLocatedStatus;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testListStatusIterator;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testClientRetriesIfMaxAttemptsNotSet;id_000000;[hadoop.security.kms.client.failover.max.retries=811725612];org.junit.runners.model.TestTimedOutException;test timed out after 30000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:220), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.createKey(LoadBalancingKMSClientProvider.java:481), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testClientRetriesIfMaxAttemptsNotSet(TestLoadBalancingKMSClientProvider.java:654), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testClientRetriesIfMaxAttemptsNotSet$$CONFUZZ(TestLoadBalancingKMSClientProvider.java)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testRandomSeeks;id_000000;[fs.defaultFS=file:///, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1368815781, fs.client.resolve.remote.symlinks=true, hadoop.service.shutdown.timeout=16068s, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.contract.supports-seek=true, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1428853170, fs.contract.test.random-seek-count=288113769, fs.creation.parallel.count=931252980, file.bytes-per-checksum=2022524631, fs.automatic.close=true, hadoop.security.dns.log-slow-lookups.threshold.ms=1620380075, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1521053142, fs.local.block.size=33554432, fs.file.impl.disable.cache=false, io.file.buffer.size=4096, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1917041499, fs.permissions.umask-mode=022, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.security.groups.cache.warn.after.ms=1286181430];org.junit.runners.model.TestTimedOutException;test timed out after 180000 milliseconds;[sun.nio.ch.NativeThread.current(Native Method), sun.nio.ch.NativeThreadSet.add(NativeThreadSet.java:46), sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:357), org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.seek(RawLocalFileSystem.java:157), org.apache.hadoop.fs.BufferedFSInputStream.seek(BufferedFSInputStream.java:102)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testRandomSeeks;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=1752609127, hadoop.security.groups.cache.secs=1287932226, fs.contract.supports-seek=true, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.service.shutdown.timeout=59244s, fs.contract.test.random-seek-count=625142782, fs.local.block.size=33554432];org.junit.runners.model.TestTimedOutException;test timed out after 180000 milliseconds;[java.io.FileInputStream.readBytes(Native Method), java.io.FileInputStream.read(FileInputStream.java:279), org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java:202), java.io.BufferedInputStream.read1(BufferedInputStream.java:290), java.io.BufferedInputStream.read(BufferedInputStream.java:351)]
org.apache.hadoop.io.file.tfile.TestTFileSeqFileComparison#testRunComparisons;id_000000;[io.seqfile.compress.blocksize=2139546938];java.lang.NegativeArraySizeException;-1254255442;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileSeqFileComparison#testRunComparisons;id_000001;[io.seqfile.compress.blocksize=1271556749];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.TestArrayFile#testArrayFileIteration;id_000000;[io.file.buffer.size=1814691821];java.lang.AssertionError;testArrayFileWriterConstruction error !!!;[org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.io.TestArrayFile.testArrayFileIteration(TestArrayFile.java:163), org.apache.hadoop.io.TestArrayFile.testArrayFileIteration$$CONFUZZ(TestArrayFile.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]
org.apache.hadoop.io.TestArrayFile#testArrayFileIteration;id_000001;[io.file.buffer.size=984946053];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.ipc.TestIPCServerResponder#testResponseBuffer;id_000001;[ipc.server.read.threadpool.size=1017212615];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.Server.<init>(Server.java:3046), org.apache.hadoop.ipc.TestIPCServerResponder$TestServer.<init>(TestIPCServerResponder.java:85), org.apache.hadoop.ipc.TestIPCServerResponder.checkServerResponder(TestIPCServerResponder.java:158)]
org.apache.hadoop.io.compress.TestCodec#testSequenceFileDefaultCodec;id_000000;[io.file.buffer.size=1935753517];java.lang.NegativeArraySizeException;-694489814;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.compress.TestCodec#testSequenceFileDefaultCodec;id_000001;[io.file.buffer.size=1168808724];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.compress.TestCodec#testSequenceFileDefaultCodec;id_000002;[io.file.buffer.size=810927474];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFileSystemCaching#testCacheEnabledWithInitializeForeverFS;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=2068195255, fs.localfs1.impl.disable.cache=false];org.junit.runners.model.TestTimedOutException;test timed out after 100000 milliseconds;[jdk.internal.misc.Unsafe.park(Native Method), java.util.concurrent.locks.LockSupport.park(LockSupport.java:194), java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:885), java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1039), java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)]
org.apache.hadoop.ha.TestZKFailoverControllerStress#testExpireBackAndForth;id_000000;[ipc.server.read.threadpool.size=1201973118];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(Deferred),  java.lang.RuntimeException(Deferred);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65)]
org.apache.hadoop.ha.TestZKFailoverControllerStress#testRandomHealthAndDisconnects;id_000000;[ipc.server.read.threadpool.size=1875248411];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(Deferred),  java.lang.RuntimeException(Deferred);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testRandomSeeks;id_000000;[file.bytes-per-checksum=1543079747];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testRandomSeeks;id_000001;[file.bytes-per-checksum=1400353366];java.lang.NegativeArraySizeException;-281721594;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.util.TestJsonSerialization#testFileSystemEmptyPath;id_000000;[io.file.buffer.size=2076761139];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemDelegationTokenSupport#testGetCanonicalServiceNameWithDefaultMountTable;id_000000;[hadoop.kerberos.min.seconds.before.relogin=1602884002, hadoop.kerberos.keytab.login.autorenewal.enabled=true, fs.viewfs.impl.disable.cache=true, hadoop.service.shutdown.timeout=2592786m, hadoop.security.groups.cache.background.reload.threads=731884361, fs.creation.parallel.count=1958069131];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemDelegationTokenSupport#testGetCanonicalServiceNameWithNonDefaultMountTable;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestUserGroupInformation#testUGITokens;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.util.TestGenericOptionsParser#testCreateWithOptions;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testOneDataEntry;id_000000;[io.file.buffer.size=950151908, tfile.fs.output.buffer.size=1187091564];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testOneDataEntry;id_000001;[file.bytes-per-checksum=1827522039];java.lang.NegativeArraySizeException;-732170833;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testOneDataEntry;id_000002;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testOneDataEntry;id_000000;[io.file.buffer.size=1422588268, tfile.fs.output.buffer.size=609774314];java.lang.NegativeArraySizeException;-1321457989;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testOneDataEntry;id_000002;[file.bytes-per-checksum=2086952122];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFile#testTFileFeatures;id_000000;[tfile.io.chunk.size=274844764, tfile.fs.output.buffer.size=1890279167];java.lang.NegativeArraySizeException;-175681285;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFile#testTFileFeatures;id_000002;[io.file.buffer.size=1375320644];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestFileSystemCaching#testCacheLargeSemaphoreConstruction;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneBlockPlusOneEntry;id_000000;[io.file.buffer.size=1330810559, tfile.fs.output.buffer.size=699273302];java.lang.NegativeArraySizeException;-901034490;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneBlockPlusOneEntry;id_000001;[io.file.buffer.size=2067767318, tfile.fs.output.buffer.size=428158892];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneBlockPlusOneEntry;id_000003;[file.bytes-per-checksum=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.security.alias.TestCredentialProviderFactory#testJksProvider;id_000000;[file.bytes-per-checksum=826309385];java.lang.NegativeArraySizeException;-1153150127;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.alias.TestCredentialProviderFactory#testJksProvider;id_000001;[file.bytes-per-checksum=1111791341];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.TestShellBasedUnixGroupsMapping#testFiniteGroupResolutionTime;id_000000;[hadoop.security.groups.shell.command.timeout=500];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestShellBasedUnixGroupsMapping.testFiniteGroupResolutionTime(TestShellBasedUnixGroupsMapping.java:321)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testThreeBlocks;id_000000;[file.bytes-per-checksum=1618799450];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testThreeBlocks;id_000001;[file.bytes-per-checksum=464802537];java.lang.NegativeArraySizeException;-111744463;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testThreeBlocks;id_000002;[file.stream-buffer-size=1371497841, tfile.fs.output.buffer.size=1131625935];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:431), org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642), org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testThreeBlocks;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testThreeBlocks;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testThreeBlocks;id_000005;[tfile.fs.output.buffer.size=0];java.lang.ArrayIndexOutOfBoundsException;Index 0 out of bounds for length 0;[org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream.write(SimpleBufferedOutputStream.java:50), java.io.DataOutputStream.writeByte(DataOutputStream.java:153), org.apache.hadoop.io.file.tfile.Utils.writeVLong(Utils.java:103), org.apache.hadoop.io.file.tfile.Utils.writeVInt(Utils.java:56), org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister.close(TFile.java:450)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testThreeBlocks;id_000000;[file.bytes-per-checksum=2118563007];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testThreeBlocks;id_000001;[io.file.buffer.size=2111710862];java.lang.NegativeArraySizeException;-693058503;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testThreeBlocks;id_000002;[file.bytes-per-checksum=2058933835];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.compress.TestCodec#testGzipCodecWithParam;id_000000;[io.file.buffer.size=2121551808];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:160), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:171)]
org.apache.hadoop.io.compress.TestCodec#testGzipCodecWithParam;id_000001;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:60), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:160), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:171)]
org.apache.hadoop.io.file.tfile.TestTFileSeek#testSeeks;id_000000;[file.bytes-per-checksum=2070922891];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileSeek#testSeeks;id_000001;[file.bytes-per-checksum=1453050438];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileSeek#testSeeks;id_000002;[file.bytes-per-checksum=1164238787];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileSeek#testSeeks;id_000003;[file.bytes-per-checksum=885348514];java.lang.NegativeArraySizeException;-621797966;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureGetNonExistentMetaBlock;id_000000;[io.file.buffer.size=1943326474];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureGetNonExistentMetaBlock;id_000001;[file.bytes-per-checksum=1282324561];java.lang.NegativeArraySizeException;-1343980839;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureGetNonExistentMetaBlock;id_000002;[io.file.buffer.size=1589008661];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureGetNonExistentMetaBlock;id_000003;[io.file.buffer.size=623783912, tfile.fs.output.buffer.size=1382030815];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testThreeBlocks;id_000000;[tfile.fs.input.buffer.size=1710436023, file.bytes-per-checksum=592505637];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testThreeBlocks;id_000001;[tfile.fs.input.buffer.size=1618243746, hadoop.security.groups.negative-cache.secs=1952886561, tfile.fs.output.buffer.size=2069147799, fs.creation.parallel.count=1521727873, io.file.buffer.size=4096, hadoop.security.groups.cache.warn.after.ms=2061698335];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testThreeBlocks;id_000002;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.http.TestServletFilter#testContextSpecificServletFilterWhenInitThrowsException;id_000000;[hadoop.http.selector.count=567204429];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.fs.TestFileSystemCaching#testCacheForUgi;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.http.TestHttpServer#testBindAddress;id_000000;[hadoop.http.selector.count=1877228975];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServer#testHttpResponseOverrideDefaultHeaders;id_000000;[hadoop.http.selector.count=1701171520];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServer#testPortRanges;id_000000;[hadoop.http.selector.count=1696854151];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestHttpServer#testHttpResonseContainsDeny;id_000000;[hadoop.prometheus.endpoint.enabled=true];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStartedServerWithRequestLog;id_000000;[hadoop.http.selector.count=1174394862];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStartedServerWithRequestLog;id_000002;[hadoop.http.selector.count=395176264];java.lang.IllegalStateException;Insufficient configured threads: required=479609814 < max=370726600 for QueuedThreadPool[qtp130527868]@7c7b27c{STARTED,8<=8<=370726600,i=8,r=-1,q=0}[ReservedThreadExecutor@218e28d9{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)]
org.apache.hadoop.http.TestHttpServerLifecycle#testWepAppContextAfterServerStop;id_000000;[hadoop.http.selector.count=2025045359];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.fs.shell.TestTextCommand#testDisplayForAvroFiles;id_000000;[io.file.buffer.size=2110393776];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.shell.TestTextCommand#testDisplayForAvroFiles;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.shell.TestTextCommand#testDisplayForAvroFiles;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.http.TestHttpServerLogs#testLogsEnabled;id_000000;[hadoop.http.selector.count=1928195404];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestHttpServerLogs#testLogsEnabled;id_000001;[hadoop.http.selector.count=864156267];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteNonEmptyDirRecursive;id_000000;[file.bytes-per-checksum=1917920316, io.file.buffer.size=94652721];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteNonEmptyDirRecursive;id_000001;[io.file.buffer.size=1204777625, file.bytes-per-checksum=166288409];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteNonEmptyDirRecursive;id_000002;[file.stream-buffer-size=2041300843, io.file.buffer.size=417094567];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteNonEmptyDirRecursive;id_000003;[file.bytes-per-checksum=943891449];java.lang.NegativeArraySizeException;-94911551;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testVerifyChecksum;id_000000;[file.bytes-per-checksum=731998018];java.lang.NegativeArraySizeException;-2001952430;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testVerifyChecksum;id_000002;[file.bytes-per-checksum=1189285671];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testVerifyChecksum;id_000003;[io.file.buffer.size=883593089];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestChecksumFileSystem#testVerifyChecksum;id_000004;[file.bytes-per-checksum=2146998462];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestChecksumFileSystem#testVerifyChecksum;id_000005;[io.file.buffer.size=1097998398];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.util.TestJsonSerialization#testFileSystemRoundTrip;id_000000;[file.bytes-per-checksum=897756228];java.lang.NegativeArraySizeException;-510128540;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.util.TestJsonSerialization#testFileSystemRoundTrip;id_000001;[file.bytes-per-checksum=681217449];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteDeepEmptyDir;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringsFileSystem;id_000000;[file.bytes-per-checksum=218126259];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringsFileSystem;id_000001;[file.bytes-per-checksum=1191159821];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.createNonRecursive(ChecksumFileSystem.java:565), org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder.build(FileSystem.java:4523)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringsFileSystem;id_000002;[file.bytes-per-checksum=1795139597];java.lang.NegativeArraySizeException;-1023612811;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.createNonRecursive(ChecksumFileSystem.java:565), org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder.build(FileSystem.java:4523)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringsFileSystem;id_000003;[file.stream-buffer-size=2116983301];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFileUtil#testWriteStringsFileSystem;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringsFileSystem;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir#testMkdirOverParentFile;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir#testMkdirOverParentFile;id_000001;[io.file.buffer.size=2112953608];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir#testMkdirOverParentFile;id_000002;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenamePopulatesFileAncestors;id_000000;[io.file.buffer.size=2078021337];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsFileAndDestinationIsNonExistentDirectory;id_000000;[file.bytes-per-checksum=1563198618];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsFileAndDestinationIsNonExistentDirectory;id_000002;[file.bytes-per-checksum=1176898958];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsFileAndDestinationIsNonExistentDirectory;id_000003;[file.bytes-per-checksum=1715018911];java.lang.NegativeArraySizeException;-1744698985;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsFileAndDestinationIsNonExistentDirectory;id_000004;[io.file.buffer.size=690552832];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.service.launcher.TestServiceConf#testConfPropagation;id_000000;[delay.time=759319346];org.junit.runners.model.TestTimedOutException;test timed out after 15000 milliseconds;[java.lang.Object.wait(Native Method), org.apache.hadoop.service.AbstractService.waitForServiceToStop(AbstractService.java:279), org.apache.hadoop.service.launcher.ServiceLauncher.coreServiceLaunch(ServiceLauncher.java:638), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:495), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:469)]
org.apache.hadoop.service.launcher.TestServiceConf#testConfPropagationOverInitBindings;id_000000;[delay.time=852499486];org.junit.runners.model.TestTimedOutException;test timed out after 15000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.service.launcher.testservices.LaunchableRunningService.execute(LaunchableRunningService.java:97), org.apache.hadoop.service.launcher.ServiceLauncher.coreServiceLaunch(ServiceLauncher.java:627), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:495), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:469)]
org.apache.hadoop.fs.viewfs.TestHCFSMountTableConfigLoader#testLoadWithNonExistentMountFile;id_000000;[file.stream-buffer-size=716671444, io.file.buffer.size=1280926315];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestHCFSMountTableConfigLoader#testLoadWithNonExistentMountFile;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestHCFSMountTableConfigLoader#testLoadWithMountFile;id_000000;[fs.local.block.size=700788742, fs.creation.parallel.count=1103265096, fs.client.resolve.remote.symlinks=false, fs.permissions.umask-mode=660, fs.viewfs.overload.scheme.target.file.impl=org.apache.hadoop.fs.LocalFileSystem];java.io.FileNotFoundException;File file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task132/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/5R7AyDNj82/Non-Existent-File.xml does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:597), org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972), org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014), org.apache.hadoop.fs.FileSystem$4.<init>(FileSystem.java:2180), org.apache.hadoop.fs.FileSystem.listLocatedStatus(FileSystem.java:2179)]
org.apache.hadoop.fs.viewfs.TestHCFSMountTableConfigLoader#testMountTableFileLoadingWhenMultipleFilesExist;id_000001;[io.file.buffer.size=2133269835];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestHCFSMountTableConfigLoader#testMountTableFileLoadingWhenMultipleFilesExist;id_000002;[file.stream-buffer-size=1623326742, io.file.buffer.size=1548252753];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestHCFSMountTableConfigLoader#testMountTableFileLoadingWhenMultipleFilesExist;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestHCFSMountTableConfigLoader#testMountTableFileWithInvalidFormat;id_000000;[file.stream-buffer-size=1366466974, io.file.buffer.size=845689304];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestHCFSMountTableConfigLoader#testMountTableFileWithInvalidFormat;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteHalfABlock;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testWorkingDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.security.authorize.TestProxyUsers#testNullUser;id_000000;[hadoop.proxyuser.proxier.hosts=10.222.0.0/16,10.113.221.221, hadoop.security.groups.cache.secs=1870858889, hadoop.security.groups.negative-cache.secs=1682574736, hadoop.security.groups.cache.background.reload.threads=277547773, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.proxyuser.proxier.groups=*, hadoop.security.groups.cache.warn.after.ms=1447880015];java.lang.IllegalArgumentException;user is null.;[org.apache.hadoop.security.authorize.DefaultImpersonationProvider.authorize(DefaultImpersonationProvider.java:113), org.apache.hadoop.security.authorize.ImpersonationProvider.authorize(ImpersonationProvider.java:54), org.apache.hadoop.security.authorize.ProxyUsers.authorize(ProxyUsers.java:101), org.apache.hadoop.security.authorize.TestProxyUsers.testNullUser(TestProxyUsers.java:357), org.apache.hadoop.security.authorize.TestProxyUsers.testNullUser$$CONFUZZ(TestProxyUsers.java)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testViewFileSystemInnerCache;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testViewFileSystemInnerCache;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1306482218;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testViewFileSystemInnerCache;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testViewFileSystemInnerCache;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.ipc.TestRPC#testServerAddress;id_000000;[ipc.server.read.threadpool.size=1707479971];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalRename2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalRename2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalRename2;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1697748816;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalRename2;id_000010;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-681599550;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathMountPoints;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathMountPoints;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-884501837;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalDeleteExisting2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-639559646;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalDeleteExisting2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalDeleteExisting2;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testgetFSonDanglingLink;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testgetFSonDanglingLink;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-578542795;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testgetFSonDanglingLink;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testgetFSonDanglingLink;id_000007;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir3;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1651626004;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir3;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir3;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir3;id_000021;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalRenameToSlash;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1310538019;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalRenameToSlash;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalRenameToSlash;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalRenameToSlash;id_000017;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testCreateNonRecursive;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-753115900;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testCreateNonRecursive;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testCreateNonRecursive;id_000012;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testFileStatusOnMountLink;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1911238564;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testFileStatusOnMountLink;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testFileStatusOnMountLink;id_000009;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testUsed;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testUsed;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1085147086;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testUsed;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testUsed;id_000005;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.util.TestJsonSerialization#testFileSystemEmptyStatus;id_000000;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=2763922, fs.file.impl.disable.cache=true];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestUserGroupInformation#testConstructorWithRules;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListLocatedStatusEmptyDirectory;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testAppendToEmptyFile;id_000000;[io.file.buffer.size=1957283669];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.ContractTestUtils.readDataset(ContractTestUtils.java:214)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesFile;id_000000;[file.stream-buffer-size=1769078176, io.file.buffer.size=1514528419];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesFile;id_000001;[file.stream-buffer-size=1771557408, io.file.buffer.size=648518016];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesFile;id_000002;[file.bytes-per-checksum=1429426787];java.lang.NegativeArraySizeException;-20060805;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesFile;id_000004;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesFile;id_000008;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testInitZeroLengthSequenceFile;id_000000;[file.bytes-per-checksum=1898014414];java.lang.NegativeArraySizeException;-97739458;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFile#testInitZeroLengthSequenceFile;id_000001;[file.stream-buffer-size=1755118988, io.file.buffer.size=1278505761];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.TestSequenceFile#testInitZeroLengthSequenceFile;id_000002;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.TestSequenceFile#testInitZeroLengthSequenceFile;id_000005;[fs.creation.parallel.count=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadNullBuffer;id_000000;[file.bytes-per-checksum=851326816];java.lang.NegativeArraySizeException;-927993248;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadNullBuffer;id_000001;[file.bytes-per-checksum=1173938467];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadNullBuffer;id_000006;[file.bytes-per-checksum=1517830413];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyZeroBytebufferPastEOF;id_000000;[file.bytes-per-checksum=333043637];java.lang.NegativeArraySizeException;-1297574563;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyZeroBytebufferPastEOF;id_000001;[file.bytes-per-checksum=680414392];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyZeroBytebufferPastEOF;id_000004;[file.bytes-per-checksum=1562355381];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyZeroBytebufferPastEOF;id_000014;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyZeroByteFile;id_000000;[file.bytes-per-checksum=900623921];java.lang.NegativeArraySizeException;-484319303;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyZeroByteFile;id_000002;[file.stream-buffer-size=2107620795];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyZeroByteFile;id_000007;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekBigFile;id_000000;[file.stream-buffer-size=1877627198];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekBigFile;id_000001;[hadoop.security.groups.cache.secs=1678352610, fs.contract.supports-seek=true, hadoop.service.shutdown.timeout=549807451h, fs.permissions.umask-mode=077, file.stream-buffer-size=493618829];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekBigFile;id_000002;[file.bytes-per-checksum=743899744];java.lang.NegativeArraySizeException;-1894836896;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekBigFile;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=1910726744, hadoop.kerberos.min.seconds.before.relogin=1246521494, hadoop.security.groups.cache.secs=1950785691, fs.contract.supports-seek=true, hadoop.service.shutdown.timeout=826m, fs.permissions.umask-mode=077, file.stream-buffer-size=261551490, hadoop.security.groups.negative-cache.secs=1604275024, hadoop.security.groups.cache.background.reload.threads=1245135149, fs.local.block.size=1029907254, hadoop.security.groups.cache.warn.after.ms=1614249680];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekBigFile;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureReadValueManyTimes;id_000000;[io.file.buffer.size=2065951856, tfile.fs.output.buffer.size=1145328829];java.lang.NegativeArraySizeException;-883692508;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureReadValueManyTimes;id_000001;[file.bytes-per-checksum=2102160060];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureReadValueManyTimes;id_000002;[file.bytes-per-checksum=1538034479];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureReadValueManyTimes;id_000003;[tfile.fs.output.buffer.size=2123958372];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testNoDataEntry;id_000000;[io.file.buffer.size=575338975, tfile.fs.output.buffer.size=1796099482];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testNoDataEntry;id_000001;[file.bytes-per-checksum=1633492705];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testNoDataEntry;id_000002;[io.file.buffer.size=1829003329, tfile.fs.output.buffer.size=240350447];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testNoDataEntry;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testNoDataEntry;id_000005;[file.bytes-per-checksum=920883226];java.lang.NegativeArraySizeException;-301985558;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCompressionNotWorking2;id_000000;[io.file.buffer.size=1943792456, tfile.fs.output.buffer.size=1791315829];java.lang.NegativeArraySizeException;-1298302222;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCompressionNotWorking2;id_000001;[io.file.buffer.size=1298302755, tfile.fs.output.buffer.size=747263445];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCompressionNotWorking2;id_000004;[tfile.fs.output.buffer.size=1886352471, file.bytes-per-checksum=478354855];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryMixedLengths2;id_000000;[tfile.fs.output.buffer.size=2146957893];java.lang.NegativeArraySizeException;-2003263208;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryMixedLengths2;id_000002;[file.bytes-per-checksum=590174632, io.file.buffer.size=1554219286];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testReadFullySmallFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestUserGroupInformation#testTokenRaceCondition;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusIteratorFile;id_000000;[file.bytes-per-checksum=1105324016];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusIteratorFile;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=772260303, hadoop.security.auth_to_local.mechanism=MIT, fs.permissions.umask-mode=007, fs.contract.supports-getfilestatus=true, io.file.buffer.size=939238531];java.lang.NegativeArraySizeException;-1617394004;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusIteratorFile;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusIteratorFile;id_000003;[file.bytes-per-checksum=1150797756];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekReadClosedFile;id_000000;[file.bytes-per-checksum=219763060];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekReadClosedFile;id_000002;[file.bytes-per-checksum=1786766692];java.lang.NegativeArraySizeException;-1098968956;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekReadClosedFile;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekPastEndOfFileThenReseekAndRead;id_000000;[file.bytes-per-checksum=404388196];java.lang.NegativeArraySizeException;-655473532;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekPastEndOfFileThenReseekAndRead;id_000002;[file.bytes-per-checksum=2046134799];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekPastEndOfFileThenReseekAndRead;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekPastEndOfFileThenReseekAndRead;id_000009;[file.bytes-per-checksum=1458557255];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteSingleFile;id_000000;[file.bytes-per-checksum=302132041];java.lang.NegativeArraySizeException;-1575778927;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteSingleFile;id_000001;[file.stream-buffer-size=2070178803, io.file.buffer.size=371006509];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteSingleFile;id_000002;[io.file.buffer.size=2138858480];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteSingleFile;id_000004;[io.file.buffer.size=2138858358];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenReadZeroByteFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileNullStatus;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testFileStatusBlocksizeNonEmptyFile;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=2134989791, hadoop.service.shutdown.timeout=637s, fs.permissions.umask-mode=755, file.stream-buffer-size=1485707684, hadoop.security.groups.negative-cache.secs=1361329858, fs.local.block.size=603360960, fs.creation.parallel.count=452089713, fs.file.impl.disable.cache=true, io.file.buffer.size=349085541, hadoop.security.groups.cache.warn.after.ms=191078568];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testFileStatusBlocksizeNonEmptyFile;id_000001;[file.bytes-per-checksum=2110439516];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testFileStatusBlocksizeNonEmptyFile;id_000002;[file.stream-buffer-size=326231644, io.file.buffer.size=1697370253];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testFileStatusBlocksizeNonEmptyFile;id_000003;[file.bytes-per-checksum=1887163671];java.lang.NegativeArraySizeException;-195396145;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenFileApplyRead;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestChecksumFileSystem#testStreamType;id_000000;[file.bytes-per-checksum=960039374, io.file.buffer.size=316229056];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.TestChecksumFileSystem.testStreamType(TestChecksumFileSystem.java:195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testStreamType;id_000001;[io.file.buffer.size=1453579475];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestChecksumFileSystem#testStreamType;id_000002;[file.bytes-per-checksum=1352686786];java.lang.NegativeArraySizeException;-710720814;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testStreamType;id_000004;[io.file.buffer.size=1165607368];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestChecksumFileSystem#testStreamType;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestChecksumFileSystem#testStreamType;id_000006;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestChecksumFileSystem#testCorruptedChecksum;id_000000;[file.bytes-per-checksum=1670671928];java.lang.NegativeArraySizeException;-2143821832;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testCorruptedChecksum;id_000001;[file.bytes-per-checksum=1999187421];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestChecksumFileSystem#testCorruptedChecksum;id_000002;[file.bytes-per-checksum=2006666714];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testDeleteEmptyDirectory;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteEmptyDirRecursive;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenReadZeroByteFile;id_000000;[file.bytes-per-checksum=1716756062];java.lang.NegativeArraySizeException;-1729064626;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenReadZeroByteFile;id_000001;[file.bytes-per-checksum=209884217];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenReadZeroByteFile;id_000003;[file.bytes-per-checksum=1940565811];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testLocate;id_000000;[io.file.buffer.size=1542771801, tfile.fs.output.buffer.size=1402191385];java.lang.NegativeArraySizeException;-1829642934;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testLocate;id_000001;[tfile.fs.output.buffer.size=2139748707];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testLocate;id_000003;[io.file.buffer.size=1647022833, tfile.fs.output.buffer.size=701229364];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testNoDataEntry;id_000000;[file.bytes-per-checksum=1409657185];java.lang.NegativeArraySizeException;-197987223;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testNoDataEntry;id_000001;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testNoDataEntry;id_000002;[file.stream-buffer-size=2049076361, tfile.fs.output.buffer.size=84144301];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.compress.TestCompressionStreamReuse#testGzipCompressStreamReuse;id_000000;[io.file.buffer.size=2057922237];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:160), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:171)]
org.apache.hadoop.io.compress.TestCompressionStreamReuse#testGzipCompressStreamReuse;id_000001;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:60), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:160), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:171)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongShort;id_000000;[io.file.buffer.size=1968040004];java.lang.NegativeArraySizeException;-106247122;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongShort;id_000001;[file.bytes-per-checksum=1442129918];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongShort;id_000002;[file.bytes-per-checksum=1152528232];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongShort;id_000004;[io.file.buffer.size=2015760538, hadoop.security.groups.cache.warn.after.ms=1279741720];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong8Bytes;id_000000;[file.bytes-per-checksum=2014574294];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong8Bytes;id_000001;[file.bytes-per-checksum=1761975925];java.lang.NegativeArraySizeException;-1322085859;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong6Bytes;id_000000;[file.bytes-per-checksum=1167806485];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong6Bytes;id_000001;[file.bytes-per-checksum=870823714];java.lang.NegativeArraySizeException;-752521166;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong6Bytes;id_000004;[file.bytes-per-checksum=1183240911];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureReadValueManyTimes;id_000000;[io.file.buffer.size=1555859498, tfile.fs.output.buffer.size=2049386201];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureReadValueManyTimes;id_000001;[file.bytes-per-checksum=797664068];java.lang.NegativeArraySizeException;-1410957980;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureReadValueManyTimes;id_000002;[io.file.buffer.size=1682821343, tfile.fs.output.buffer.size=1751555167];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureReadValueManyTimes;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testMultiByteFilesAreFiles;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testMultiByteFilesAreFiles;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testMultiByteFilesAreFiles;id_000002;[io.file.buffer.size=2146487295];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileUnknownOption;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestGroupsCaching#testNegativeGroupCaching;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.TestGroupsCaching.testNegativeGroupCaching(TestGroupsCaching.java:340), org.apache.hadoop.security.TestGroupsCaching.testNegativeGroupCaching$$CONFUZZ(TestGroupsCaching.java)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreatedFileIsEventuallyVisible;id_000000;[file.stream-buffer-size=2004239885, file.bytes-per-checksum=1460885911];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreatedFileIsEventuallyVisible;id_000001;[file.bytes-per-checksum=1796439762];java.lang.NegativeArraySizeException;-1011911326;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreatedFileIsEventuallyVisible;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=1189938438, hadoop.kerberos.min.seconds.before.relogin=1288343556, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=05862391s, fs.permissions.umask-mode=500, file.stream-buffer-size=889517667, hadoop.security.groups.negative-cache.secs=1889654541, hadoop.security.groups.cache.background.reload.threads=1035754134, fs.local.block.size=2108629115, fs.creation.parallel.count=2124806152, fs.file.impl.disable.cache=true, hadoop.security.groups.cache.warn.after.ms=184745756];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToNonExistentDirectory;id_000000;[file.bytes-per-checksum=2055359385];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToNonExistentDirectory;id_000001;[file.bytes-per-checksum=285458795];java.lang.NegativeArraySizeException;-1725838141;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToNonExistentDirectory;id_000004;[file.bytes-per-checksum=2040213259];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToNonExistentDirectory;id_000005;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusWithNoMatchesInPath;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenOldService;id_000000;[hadoop.ssl.require.client.cert=true];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:391), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenOldService(TestKMSClientProvider.java:90)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenOldService;id_000002;[hadoop.security.kms.client.encrypted.key.cache.size=0];java.lang.IllegalArgumentException;"numValues" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:224), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenOldService(TestKMSClientProvider.java:90)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenOldService;id_000003;[hadoop.security.kms.client.encrypted.key.cache.expiry=0];java.lang.IllegalArgumentException;"expiry" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:230), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenOldService(TestKMSClientProvider.java:90)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenOldService;id_000004;[hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=0];java.lang.IllegalArgumentException;"numFillerThreads" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:231), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenOldService(TestKMSClientProvider.java:90)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenOldService;id_000005;[hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.0f];java.lang.IllegalArgumentException;"lowWatermark" must be > 0 and <= 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:225), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenOldService(TestKMSClientProvider.java:90)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreatedFileIsImmediatelyVisible;id_000000;[file.bytes-per-checksum=443005738];java.lang.NegativeArraySizeException;-307915654;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreatedFileIsImmediatelyVisible;id_000001;[file.bytes-per-checksum=1994672570];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreatedFileIsImmediatelyVisible;id_000004;[file.bytes-per-checksum=1093507648];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteNonEmptyDirNonRecursive;id_000000;[file.bytes-per-checksum=1714746165];java.lang.NegativeArraySizeException;-1747153699;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteNonEmptyDirNonRecursive;id_000001;[file.bytes-per-checksum=1585390133];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteNonEmptyDirNonRecursive;id_000002;[file.bytes-per-checksum=2039537099];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteNonEmptyDirNonRecursive;id_000004;[file.bytes-per-checksum=2110165018];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteNonEmptyDirNonRecursive;id_000005;[hadoop.security.dns.log-slow-lookups.threshold.ms=132, hadoop.security.groups.cache.secs=68794080, hadoop.service.shutdown.timeout=22911039h, file.stream-buffer-size=367078866, hadoop.security.groups.cache.background.reload.threads=36352, fs.local.block.size=1006632960, fs.file.impl.disable.cache=true, io.file.buffer.size=1257646918, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=758028366, fs.permissions.umask-mode=777, fs.creation.parallel.count=1454617664, hadoop.security.groups.cache.warn.after.ms=5284];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileAsExistingDirectory;id_000000;[file.bytes-per-checksum=1158354649];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileAsExistingDirectory;id_000001;[file.bytes-per-checksum=887854345];java.lang.NegativeArraySizeException;-599245487;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileAsExistingDirectory;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testListStatus;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFileSystemPermission#testLocalFSDirsetPermission;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testWriteSingleByte;id_000000;[file.bytes-per-checksum=1727699752];java.lang.NegativeArraySizeException;-1630571416;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testWriteSingleByte;id_000002;[file.bytes-per-checksum=1946977930];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testWriteSingleByte;id_000003;[file.bytes-per-checksum=680689050];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileUnderFileSubdir;id_000000;[file.bytes-per-checksum=1613639219];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileUnderFileSubdir;id_000001;[file.bytes-per-checksum=639785593];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileUnderFileSubdir;id_000002;[file.bytes-per-checksum=1837127580];java.lang.NegativeArraySizeException;-645720964;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileUnderFileSubdir;id_000003;[file.stream-buffer-size=1300542540, io.file.buffer.size=1881954214];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileUnderFileSubdir;id_000004;[file.stream-buffer-size=1662344953, io.file.buffer.size=607857814];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.token.TestDtUtilShell#testFormatProtoFlag;id_000000;[file.bytes-per-checksum=1480736279, io.file.buffer.size=275628405];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.token.TestDtUtilShell#testFormatProtoFlag;id_000001;[file.bytes-per-checksum=101133319, io.file.buffer.size=1225038540];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.token.TestDtUtilShell#testFormatProtoFlag;id_000002;[hadoop.security.dns.log-slow-lookups.threshold.ms=777313604, hadoop.security.groups.cache.secs=619811483, hadoop.service.shutdown.timeout=18298m, file.stream-buffer-size=832636519, fs.local.block.size=1101378466, fs.file.impl.disable.cache=true, io.file.buffer.size=581012245, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=905213890, hadoop.kerberos.keytab.login.autorenewal.enabled=false, fs.permissions.umask-mode=644, hadoop.security.groups.negative-cache.secs=209602893, fs.automatic.close=false, hadoop.security.groups.cache.warn.after.ms=946157471];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetContentSummaryWithFileInLocalFS;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetContentSummaryWithFileInLocalFS;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-49506461;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetContentSummaryWithFileInLocalFS;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testDeleteOnExit;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testDeleteOnExit;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testDeleteOnExit;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testDeleteOnExit;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-30188941;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts4;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts4;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-973526997;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testConfLinkSlash;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-987801520;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testConfLinkSlash;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetContentSummary;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetContentSummary;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetContentSummary;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1298745670;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetContentSummary;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCloseChildrenFileSystem;id_000000;[fs.viewfs.enable.inner.cache=false];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCloseChildrenFileSystem;id_000001;[fs.viewfs.enable.inner.cache=false];java.lang.NegativeArraySizeException;-1431269302;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCloseChildrenFileSystem;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCloseChildrenFileSystem;id_000015;[fs.viewfs.enable.inner.cache=false];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testNflyInvalidMinReplication;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testNflyInvalidMinReplication;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-882032169;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testNflyInvalidMinReplication;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetContentSummaryWithFileInLocalFS;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetContentSummaryWithFileInLocalFS;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetContentSummaryWithFileInLocalFS;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-861745939;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testDeleteOnExit;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testDeleteOnExit;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-555293064;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testDeleteOnExit;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testDeleteOnExit;id_000018;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testChildrenFileSystemLeak;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-497264345;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testChildrenFileSystemLeak;id_000001;[fs.viewfs.enable.inner.cache=false];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testChildrenFileSystemLeak;id_000007;[fs.viewfs.enable.inner.cache=false];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetContentSummary;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetContentSummary;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-441296364;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testCloseChildrenFileSystem;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testCloseChildrenFileSystem;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-345171175;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testWithDirStringAndConf;id_000000;[fs.file.impl.disable.cache=true];java.lang.NegativeArraySizeException;-1810448220;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testWithDirStringAndConf;id_000001;[file.bytes-per-checksum=186290163, io.file.buffer.size=134840182];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestPathData#testWithDirStringAndConf;id_000002;[io.file.buffer.size=1387673741, file.bytes-per-checksum=165185428];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testWithDirStringAndConf;id_000003;[file.bytes-per-checksum=1961351386];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.shell.TestPathData#testWithDirStringAndConf;id_000004;[io.file.buffer.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testWithDirStringAndConf;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testDeleteEmptyDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameFileToDestinationWithParentFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusFilterWithMultiplePathWildcardsAndNonTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testWriteReadAndDeleteHalfABlock;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testBuilderCreateAppendExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testEmptyCreateFlag;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testListStatusFilterWithNoMatches;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testCreateFlagAppendExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusSomeMatchesInDirectories;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testListStatus;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusFilterWithSomePathMatchesAndTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathDanglingLink;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathDanglingLink;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1420235899;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathDanglingLink;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathDanglingLink;id_000005;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetDelegationTokens;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-2113685301;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetDelegationTokens;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testViewFileSystemInnerCache;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1842182180;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testViewFileSystemInnerCache;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testViewFileSystemInnerCache;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testViewFileSystemInnerCache;id_000013;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir2;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-491203488;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir2;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreateMissingDir2;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathThroughMountPoints;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathThroughMountPoints;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1747371266;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathThroughMountPoints;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathThroughMountPoints;id_000012;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetDelegationTokens;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-758310358;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetDelegationTokens;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetDelegationTokens;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetDelegationTokens;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetDelegationTokensWithCredentials;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-240078523;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetDelegationTokensWithCredentials;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetDelegationTokensWithCredentials;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathMissingThroughMountPoints;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1931541870;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathMissingThroughMountPoints;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathMissingThroughMountPoints;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathMissingThroughMountPoints;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetBlockLocations;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetBlockLocations;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1167736084;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetBlockLocations;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testGetBlockLocations;id_000014;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts1;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts1;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts1;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-412521731;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalGetAllStoragePolicies;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalGetAllStoragePolicies;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1690876646;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalGetAllStoragePolicies;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateFileUnderFile;id_000000;[io.file.buffer.size=2120162670];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.security.TestUserGroupInformation#testGettingGroups;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir#testNoMkdirOverFile;id_000000;[io.file.buffer.size=2107104316];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir#testMkDirRmDir;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestGroupsCaching#testNegativeCacheClearedOnRefresh;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestGroupsCaching.testNegativeCacheClearedOnRefresh(TestGroupsCaching.java:843)]
org.apache.hadoop.security.alias.TestCredentialProviderFactory#testFactory;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameDirectoryAsEmptyDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusFilterWithMultiplePathMatchesAndNonTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testOpen2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameDirectoryToNonExistentParent;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testWriteReadAndDeleteOneAndAHalfBlocks;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameDirectoryAsNonEmptyDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testCreateFlagOverwriteNonExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testBuilderCreateNonExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testListFiles;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameFileAsExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testMkdirsRecursiveWithExistingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testWithRename;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testUnqualifiedUriContents;id_000000;[io.file.buffer.size=2108191412];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testUnqualifiedUriContents;id_000001;[file.bytes-per-checksum=1861399407];java.lang.NegativeArraySizeException;-427274521;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testUnqualifiedUriContents;id_000002;[file.bytes-per-checksum=1558039215];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestPathData#testUnqualifiedUriContents;id_000003;[file.bytes-per-checksum=1066026181];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.shell.TestPathData#testUnqualifiedUriContents;id_000004;[fs.file.impl.disable.cache=true];org.apache.hadoop.fs.PathNotFoundException;`d1': No such file or directory;[org.apache.hadoop.fs.shell.PathData.checkIfExists(PathData.java:218), org.apache.hadoop.fs.shell.PathData.getDirectoryContents(PathData.java:270), org.apache.hadoop.fs.shell.TestPathData.testUnqualifiedUriContents(TestPathData.java:91), org.apache.hadoop.fs.shell.TestPathData.testUnqualifiedUriContents$$CONFUZZ(TestPathData.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.fs.shell.TestPathData#testUnqualifiedUriContents;id_000005;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testUnqualifiedUriContents;id_000007;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testAbsoluteGlob;id_000000;[file.bytes-per-checksum=950605393];java.lang.NegativeArraySizeException;-34486055;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testAbsoluteGlob;id_000001;[file.bytes-per-checksum=1537807953];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestPathData#testAbsoluteGlob;id_000002;[file.bytes-per-checksum=2142234693];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testAbsoluteGlob;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testAbsoluteGlob;id_000004;[fs.creation.parallel.count=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusWithMultipleMatchesOfSingleChar;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testSetVerifyChecksum;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testMkdirRecursiveWithExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testMkdirNonRecursiveWithNonExistingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testCreateNonRecursiveWithNonExistingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testCreateRecursiveWithExistingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testMkdirRecursiveWithExistingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testMkdirNonRecursiveWithExistingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.security.TestGroupFallback#testGroupShell;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestGroupFallback.testGroupShell(TestGroupFallback.java:43)]
org.apache.hadoop.fs.TestLocalFSFileContextMainOperations#testCreateFlagAppendOverwrite;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1556260089, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=193956092, hadoop.security.authentication=simple, fs.client.resolve.remote.symlinks=false, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=955085125, fs.local.block.size=1039393318, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=803067481, hadoop.kerberos.keytab.login.autorenewal.enabled=true, fs.permissions.umask-mode=750, hadoop.security.groups.negative-cache.secs=1441548106, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.creation.parallel.count=156309580, hadoop.security.groups.cache.warn.after.ms=1312638445];org.apache.hadoop.HadoopIllegalArgumentException;[OVERWRITE, APPEND]Both append and overwrite options cannot be enabled.;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:159), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureFileWriteNotAt0Position;id_000000;[io.file.buffer.size=2057394437, tfile.fs.output.buffer.size=59950102];java.lang.NegativeArraySizeException;-1051675469;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureFileWriteNotAt0Position;id_000001;[file.bytes-per-checksum=961285740, tfile.fs.output.buffer.size=1537713337];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareMetaBlock(BCFile.java:357), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareMetaBlock(BCFile.java:383), org.apache.hadoop.io.file.tfile.TFile$Writer.close(TFile.java:319)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureFileWriteNotAt0Position;id_000002;[io.file.buffer.size=1542505234, tfile.fs.output.buffer.size=705603304];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureFileWriteNotAt0Position;id_000010;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureFileWriteNotAt0Position;id_000011;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOutOfOrderKeys;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=344385564, hadoop.security.groups.cache.secs=629318540, hadoop.service.shutdown.timeout=261s, file.stream-buffer-size=452557300, hadoop.security.groups.cache.background.reload.threads=1487808911, fs.local.block.size=1250022089, fs.file.impl.disable.cache=true, io.file.buffer.size=1104963440, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1736303938, fs.permissions.umask-mode=666, hadoop.security.groups.negative-cache.secs=1777257328, fs.creation.parallel.count=1621232831];java.lang.NegativeArraySizeException;-622377980;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOutOfOrderKeys;id_000001;[file.bytes-per-checksum=1551533596];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOutOfOrderKeys;id_000005;[tfile.fs.output.buffer.size=1029311512, file.bytes-per-checksum=1024677923];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCloseKeyStreamManyTimesInWriter;id_000000;[file.stream-buffer-size=1593336902, file.bytes-per-checksum=173154991];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCloseKeyStreamManyTimesInWriter;id_000002;[file.stream-buffer-size=1593369362, io.file.buffer.size=1694492934];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCloseKeyStreamManyTimesInWriter;id_000003;[file.bytes-per-checksum=314167159];java.lang.NegativeArraySizeException;-1467462865;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testNegativeSeek;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testReadFullyZeroByteFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testBlockReadZeroByteFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyTooLong;id_000000;[file.bytes-per-checksum=1285236533];java.lang.NegativeArraySizeException;-1317773091;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyTooLong;id_000001;[file.stream-buffer-size=2072968281];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyTooLong;id_000002;[file.bytes-per-checksum=1582587595];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyTooLong;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=381968270, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1474343823, hadoop.service.shutdown.timeout=1012s, file.stream-buffer-size=289072692, fs.local.block.size=1309227242, fs.file.impl.disable.cache=true, io.file.buffer.size=141164713, hadoop.kerberos.min.seconds.before.relogin=258296348, hadoop.kerberos.keytab.login.autorenewal.enabled=false, fs.permissions.umask-mode=750, hadoop.security.groups.negative-cache.secs=1156172067, tfile.fs.output.buffer.size=798549182, fs.creation.parallel.count=942226607, hadoop.security.groups.cache.warn.after.ms=1800356160];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.OutOfMemoryError(Java heap space),  java.lang.OutOfMemoryError(Java heap space);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyTooLong;id_000004;[tfile.fs.output.buffer.size=1268288970, io.file.buffer.size=1375486436];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testAppendToExistingFile;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1193437705, hadoop.kerberos.min.seconds.before.relogin=1543680458, fs.client.resolve.remote.symlinks=true, hadoop.service.shutdown.timeout=632m, fs.permissions.umask-mode=777, hadoop.security.groups.cache.background.reload.threads=526857347, fs.local.block.size=1873504365, hadoop.security.groups.cache.background.reload=false, file.bytes-per-checksum=1727201374, io.file.buffer.size=1035350407];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.ContractTestUtils.readDataset(ContractTestUtils.java:214)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testAppendToExistingFile;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testAppendToExistingFile;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestFsShellReturnCode#testChownUserAndGroupValidity;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.shell.TestTextCommand#testTwoByteTextFil;id_000000;[io.file.buffer.size=2125444891];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.shell.TestTextCommand#testTwoByteTextFil;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.shell.TestTextCommand#testTwoByteTextFil;id_000003;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.shell.TestTextCommand#testOneByteTextFil;id_000000;[io.file.buffer.size=2120768083];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.shell.TestTextCommand#testOneByteTextFil;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.shell.TestTextCommand#testOneByteTextFil;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.security.TestGroupsCaching#testCachePreventsImplRequest;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestGroupsCaching.testCachePreventsImplRequest(TestGroupsCaching.java:388)]
org.apache.hadoop.security.TestGroupsCaching#testGroupsCaching;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestGroupsCaching.testGroupsCaching(TestGroupsCaching.java:229)]
org.apache.hadoop.http.TestServletFilter#testServletFilterWhenInitThrowsException;id_000000;[hadoop.http.selector.count=1799146302];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestServletFilter#testServletFilterWhenInitThrowsException;id_000001;[hadoop.http.selector.count=1166638875];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestServletFilter#testServletFilterWhenInitThrowsException;id_000002;[hadoop.http.max.threads=0];java.lang.IllegalStateException;Insufficient configured threads: required=0 < max=0 for QueuedThreadPool[qtp747601258]@2c8f7d6a{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.QueuedThreadPool.setMaxThreads(QueuedThreadPool.java:364), org.apache.hadoop.http.HttpServer2.initializeWebServer(HttpServer2.java:703), org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:687), org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:129)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteExistingFile;id_000000;[file.stream-buffer-size=341672658, io.file.buffer.size=1809600629];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteExistingFile;id_000001;[file.bytes-per-checksum=239715192];java.lang.NegativeArraySizeException;-2137530568;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteExistingFile;id_000003;[file.stream-buffer-size=341701330, io.file.buffer.size=1809600629];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testFsIsEncrypted;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteNonEmptyDirRecursive;id_000000;[io.file.buffer.size=2102226523];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteNonEmptyDirRecursive;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteNonEmptyDirRecursive;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFileSystemPermission#testLocalFSsetOwner;id_000000;[io.file.buffer.size=2111708216];java.lang.NegativeArraySizeException;-1560292554;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestLocalFileSystemPermission#testLocalFSsetOwner;id_000001;[file.bytes-per-checksum=959793764, io.file.buffer.size=1775800578];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestLocalFileSystemPermission#testLocalFSsetOwner;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestLocalFileSystemPermission#testLocalFSsetOwner;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testMkdirsDoesNotRemoveParentDirectories;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestUserGroupInformation#testConstructorWithKerberosRules;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreatedFileIsVisibleOnFlush;id_000000;[file.bytes-per-checksum=1257270755];java.lang.NegativeArraySizeException;-1569465093;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreatedFileIsVisibleOnFlush;id_000001;[file.bytes-per-checksum=2033788169];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testMkdirSlashHandling;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSrcIsDirWithOverwriteOptions;id_000000;[io.file.buffer.size=1407199535];java.lang.NegativeArraySizeException;-403712482;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSrcIsDirWithOverwriteOptions;id_000001;[io.file.buffer.size=1791896832];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSrcIsDirWithOverwriteOptions;id_000002;[file.stream-buffer-size=1526761464, file.bytes-per-checksum=77993553];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFsURIs#testURIEmptyPath;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testTokenServiceCreationWithUriFormat;id_000000;[hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.0f];java.lang.IllegalArgumentException;"lowWatermark" must be > 0 and <= 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:225), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testTokenServiceCreationWithUriFormat;id_000001;[hadoop.security.kms.client.encrypted.key.cache.expiry=0];java.lang.IllegalArgumentException;"numValues" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:224), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testTokenServiceCreationWithUriFormat;id_000002;[hadoop.security.kms.client.encrypted.key.cache.expiry=0];java.lang.IllegalArgumentException;"expiry" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:230), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testTokenServiceCreationWithUriFormat;id_000003;[hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=0];java.lang.IllegalArgumentException;"numFillerThreads" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:231), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testOverwriteNonEmptyDirectory;id_000000;[io.file.buffer.size=2142924135];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testOverwriteNonEmptyDirectory;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=1682092379, hadoop.kerberos.min.seconds.before.relogin=2060679897, hadoop.security.groups.cache.secs=910670705, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=793064506, hadoop.security.groups.cache.background.reload=true, file.bytes-per-checksum=38392909, fs.contract.supports-strict-exceptions=true, hadoop.security.groups.cache.warn.after.ms=1773460493];java.io.FileNotFoundException;/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task71/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/hSClQRPG19/testOverwriteNonEmptyDirectory (Is a directory);[java.io.FileOutputStream.open0(Native Method), java.io.FileOutputStream.open(FileOutputStream.java:298), java.io.FileOutputStream.<init>(FileOutputStream.java:237), org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:321), org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameFileOverExistingFile;id_000000;[fs.contract.rename-returns-false-if-dest-exists=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameFileNonexistentDir;id_000000;[io.file.buffer.size=1675543247];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameFileNonexistentDir;id_000001;[io.file.buffer.size=944225453];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:419), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:391)]
org.apache.hadoop.security.token.TestDtUtilShell#testRemove;id_000000;[io.file.buffer.size=314689014, file.bytes-per-checksum=1465203641];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.token.TestDtUtilShell#testRemove;id_000001;[file.bytes-per-checksum=1601673185];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.token.TestDtUtilShell#testRemove;id_000002;[file.bytes-per-checksum=689754591];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testSequentialRead;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestUserGroupInformation#testEnsureInitWithRules;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureNegativeLength;id_000000;[io.file.buffer.size=2046922873, tfile.fs.output.buffer.size=1089517010];java.lang.NegativeArraySizeException;-1450301776;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureNegativeLength;id_000001;[file.bytes-per-checksum=1943072085];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureNegativeLength;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureNegativeLength;id_000015;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureKeyLongerThan64K;id_000000;[file.stream-buffer-size=2113334097];java.lang.NegativeArraySizeException;-1888707473;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureKeyLongerThan64K;id_000002;[file.bytes-per-checksum=1089579192];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureKeyLongerThan64K;id_000005;[file.stream-buffer-size=1756422462, file.bytes-per-checksum=43948361];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureWriteRecordAfterMetaBlock;id_000000;[file.bytes-per-checksum=1134932144];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureWriteRecordAfterMetaBlock;id_000002;[file.bytes-per-checksum=1332391856];java.lang.NegativeArraySizeException;-893375184;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureWriteRecordAfterMetaBlock;id_000003;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.compress.TestCodec#testCodecInitWithCompressionLevel;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:42), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.TestCodec.codecTestWithNOCompression(TestCodec.java:447), org.apache.hadoop.io.compress.TestCodec.testCodecInitWithCompressionLevel(TestCodec.java:476), org.apache.hadoop.io.compress.TestCodec.testCodecInitWithCompressionLevel$$CONFUZZ(TestCodec.java)]
org.apache.hadoop.io.compress.TestCodec#testCodecInitWithCompressionLevel;id_000002;[io.file.buffer.size=2106348044];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.TestCodec.codecTestWithNOCompression(TestCodec.java:447), org.apache.hadoop.io.compress.TestCodec.testCodecInitWithCompressionLevel(TestCodec.java:476), org.apache.hadoop.io.compress.TestCodec.testCodecInitWithCompressionLevel$$CONFUZZ(TestCodec.java)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFiltering;id_000000;[hadoop.service.shutdown.timeout=191802015h, fs.permissions.umask-mode=770, fs.local.block.size=478460843, fs.contract.supports-getfilestatus=true, fs.automatic.close=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFiltering;id_000002;[file.bytes-per-checksum=1802195241];java.lang.NegativeArraySizeException;-960112015;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFiltering;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testPositionedBulkReadDoesntChangePosition;id_000000;[hadoop.security.groups.cache.secs=1186596293, fs.permissions.umask-mode=775, file.stream-buffer-size=575418990, hadoop.security.groups.negative-cache.secs=1277052821, hadoop.security.groups.cache.background.reload.threads=2133323746, fs.local.block.size=2090912859, fs.creation.parallel.count=1168864327, fs.file.impl.disable.cache=true, hadoop.security.groups.cache.warn.after.ms=6963109];java.lang.NegativeArraySizeException;-21371602;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testPositionedBulkReadDoesntChangePosition;id_000001;[file.bytes-per-checksum=1928885442];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testPositionedBulkReadDoesntChangePosition;id_000003;[file.bytes-per-checksum=672066126];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.security.TestGroupsCaching#testOnlyOneRequestWhenNoEntryIsCached;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestGroupsCaching.testOnlyOneRequestWhenNoEntryIsCached(TestGroupsCaching.java:440)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testSyncable;id_000001;[io.file.buffer.size=1851646365];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.AbstractContractCreateTest.validateSyncableSemantics(AbstractContractCreateTest.java:546)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testSyncable;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testSyncable;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCloseKeyStreamManyTimesInWriter;id_000000;[io.file.buffer.size=2065552443, tfile.fs.output.buffer.size=1629787600];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCloseKeyStreamManyTimesInWriter;id_000001;[file.bytes-per-checksum=731733043];java.lang.NegativeArraySizeException;-2004337205;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCloseKeyStreamManyTimesInWriter;id_000003;[tfile.fs.output.buffer.size=2076487490];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.shell.TestFsShellConcat#testUnsupportedFs;id_000000;[file.bytes-per-checksum=1081230672];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestFsShellConcat#testUnsupportedFs;id_000001;[file.bytes-per-checksum=567372754, io.file.buffer.size=1172201639];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestFsShellConcat#testUnsupportedFs;id_000002;[file.bytes-per-checksum=242128979];java.lang.NegativeArraySizeException;-2115806485;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestFsShellConcat#testUnsupportedFs;id_000003;[file.bytes-per-checksum=1081538991];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.shell.TestFsShellConcat#testConcat;id_000000;[file.stream-buffer-size=1695568945];java.lang.NegativeArraySizeException;-580274329;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestFsShellConcat#testConcat;id_000002;[file.bytes-per-checksum=1483397618];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestFsShellConcat#testConcat;id_000006;[file.stream-buffer-size=992649933, file.bytes-per-checksum=29973216];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testTwoBlocks;id_000000;[tfile.fs.output.buffer.size=2126912767];java.lang.NegativeArraySizeException;-979749493;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testTwoBlocks;id_000003;[file.bytes-per-checksum=1665477398];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureGetNonExistentMetaBlock;id_000000;[file.bytes-per-checksum=691028395];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureGetNonExistentMetaBlock;id_000002;[file.bytes-per-checksum=1818749742];java.lang.NegativeArraySizeException;-811121506;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureGetNonExistentMetaBlock;id_000003;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryKnownLength;id_000000;[io.file.buffer.size=1585462109, tfile.fs.output.buffer.size=888645311];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryKnownLength;id_000001;[io.file.buffer.size=1948200660, tfile.fs.input.buffer.size=347432382];java.lang.NegativeArraySizeException;-2092642851;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryKnownLength;id_000003;[io.file.buffer.size=2085496817];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryKnownLength;id_000004;[file.bytes-per-checksum=218705167];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryMixedLengths1;id_000000;[io.file.buffer.size=1490159853, tfile.fs.output.buffer.size=1331409052];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryMixedLengths1;id_000001;[tfile.io.chunk.size=1299290899, io.file.buffer.size=1062256648];java.lang.NegativeArraySizeException;-1074797328;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryMixedLengths1;id_000002;[tfile.io.chunk.size=1011152786, io.file.buffer.size=771620116, tfile.fs.output.buffer.size=924195777];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testFailureScannerWithKeys;id_000000;[file.bytes-per-checksum=1751201873];java.lang.NegativeArraySizeException;-1419052327;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testFailureScannerWithKeys;id_000001;[tfile.fs.output.buffer.size=2072039089];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testFailureScannerWithKeys;id_000003;[fs.permissions.umask-mode=664, file.stream-buffer-size=472612498, fs.local.block.size=1443843015, fs.creation.parallel.count=1120669835, fs.file.impl.disable.cache=true, io.file.buffer.size=551284765];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyTooShort;id_000000;[fs.local.block.size=299538237, io.file.buffer.size=407720418, hadoop.security.token.service.use_ip=true, tfile.fs.output.buffer.size=1560344281];java.lang.NegativeArraySizeException;-1220560869;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyTooShort;id_000005;[file.bytes-per-checksum=225279278];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyTooShort;id_000060;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureNegativeOffset;id_000000;[hadoop.security.token.service.use_ip=false, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.service.shutdown.timeout=6786287m, fs.permissions.umask-mode=750, file.stream-buffer-size=213449720, fs.local.block.size=1653558541, fs.file.impl.disable.cache=true, io.file.buffer.size=1034148756];java.lang.NegativeArraySizeException;-1218154301;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureNegativeOffset;id_000001;[file.bytes-per-checksum=2060763661];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryMixedLengths2;id_000000;[tfile.fs.output.buffer.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(duration must be positive: 0 MILLISECONDS),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryMixedLengths2;id_000001;[tfile.fs.output.buffer.size=1399304304, file.bytes-per-checksum=122756078];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryMixedLengths2;id_000003;[file.bytes-per-checksum=942172868];java.lang.NegativeArraySizeException;-110378780;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryMixedLengths2;id_000004;[tfile.fs.input.buffer.size=2147098432];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testComplexDirActions;id_000000;[file.bytes-per-checksum=467970815];java.lang.NegativeArraySizeException;-83229961;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testComplexDirActions;id_000001;[hadoop.security.token.service.use_ip=false, hadoop.security.groups.cache.secs=1723909981, hadoop.service.shutdown.timeout=37204s, fs.permissions.umask-mode=400, file.stream-buffer-size=559983163, hadoop.security.groups.negative-cache.secs=419523323, hadoop.security.groups.cache.background.reload.threads=1408628815, fs.local.block.size=1259376477, fs.creation.parallel.count=1636715352, fs.file.impl.disable.cache=true, io.file.buffer.size=243470552, fs.automatic.close=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testComplexDirActions;id_000003;[file.stream-buffer-size=1250436408, io.file.buffer.size=756535321];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsEmptyDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testWriteInNonExistentDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testOverwrite;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsNonEmptyDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameFileToExistingParent;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testCopyToLocalWithUseRawLocalFileSystemOption;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithEmptyPathResults;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testListStatusFilterWithNoMatches;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testMkdirsFailsForSubdirectoryOfExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testDeleteRecursively;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsNonExistentDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameFileAsExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriteMetaBlocksWithSameName;id_000000;[file.bytes-per-checksum=489343696, tfile.fs.output.buffer.size=2037550037];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:431), org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642), org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriteMetaBlocksWithSameName;id_000001;[tfile.fs.output.buffer.size=2094289636];java.lang.NegativeArraySizeException;-1273157967;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureFileWriteNotAt0Position;id_000000;[file.bytes-per-checksum=1966182932];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureFileWriteNotAt0Position;id_000001;[tfile.fs.output.buffer.size=2124153202];java.lang.NegativeArraySizeException;-1939174712;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOpenEmptyFile;id_000000;[io.file.buffer.size=1395213888];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOpenEmptyFile;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=184317375, hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1496785853, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.service.shutdown.timeout=8h, file.stream-buffer-size=108070417, hadoop.security.groups.cache.background.reload.threads=1762805512, fs.local.block.size=2018744744, hadoop.security.groups.cache.background.reload=true, fs.file.impl.disable.cache=true, io.file.buffer.size=271444672, hadoop.kerberos.min.seconds.before.relogin=1940889395, fs.permissions.umask-mode=750, hadoop.security.groups.negative-cache.secs=211397048, tfile.fs.output.buffer.size=1000444143, fs.creation.parallel.count=102170061, fs.automatic.close=false, hadoop.security.groups.cache.warn.after.ms=1937115221];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOpenEmptyFile;id_000002;[file.bytes-per-checksum=1032055449];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOpenEmptyFile;id_000003;[file.bytes-per-checksum=1810525295];java.lang.NegativeArraySizeException;-885141529;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOpenEmptyFile;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOpenRandomFile;id_000000;[file.bytes-per-checksum=1364136522];java.lang.NegativeArraySizeException;-607673190;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOpenRandomFile;id_000001;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOpenRandomFile;id_000003;[file.bytes-per-checksum=1042885974];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureOpenRandomFile;id_000007;[file.bytes-per-checksum=1595011670];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyLongerThan64K;id_000000;[io.file.buffer.size=1190011943, tfile.fs.output.buffer.size=1961065066];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyLongerThan64K;id_000001;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=669946293, hadoop.service.shutdown.timeout=934003399m, fs.permissions.umask-mode=644, file.stream-buffer-size=1214580255, hadoop.security.groups.cache.background.reload.threads=838527034, fs.local.block.size=603947734, hadoop.security.groups.cache.background.reload=false, fs.file.impl.disable.cache=true, io.file.buffer.size=711615920, hadoop.security.groups.cache.warn.after.ms=1348519569];java.lang.NegativeArraySizeException;-600221110;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyLongerThan64K;id_000002;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyLongerThan64K;id_000003;[file.bytes-per-checksum=1665515681];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureKeyLongerThan64K;id_000004;[file.bytes-per-checksum=1523505542];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithNoMatchingPathsAndNonTrivialFilter;id_000000;[hadoop.service.shutdown.timeout=2m, fs.permissions.umask-mode=755, fs.local.block.size=1761607680, fs.creation.parallel.count=410086378, fs.file.impl.disable.cache=true, fs.automatic.close=true];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteInNonExistentDirectory;id_000000;[file.bytes-per-checksum=1312485489];java.lang.NegativeArraySizeException;-1072532487;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteInNonExistentDirectory;id_000002;[file.stream-buffer-size=344347701, file.bytes-per-checksum=183995839];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteInNonExistentDirectory;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateMakesParentDirs;id_000000;[io.file.buffer.size=2093710055];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateMakesParentDirs;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateMakesParentDirs;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateMakesParentDirs;id_000007;[io.file.buffer.size=2119440767];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsFileAndDestinationIsDirectory;id_000000;[file.bytes-per-checksum=197597842];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsFileAndDestinationIsDirectory;id_000001;[io.file.buffer.size=808175020];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsFileAndDestinationIsDirectory;id_000002;[file.bytes-per-checksum=1380069288];java.lang.NegativeArraySizeException;-464278296;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalRename2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1428857941;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalRename2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testgetFSonDanglingLink;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-165760523;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testgetFSonDanglingLink;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testgetFSonDanglingLink;id_000015;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMissingThroughMountPoints2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1316298063;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMissingThroughMountPoints2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMissingThroughMountPoints2;id_000011;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetDelegationTokensWithCredentials;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetDelegationTokensWithCredentials;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1192941214;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreate2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-363359893;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreate2;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreate2;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts1;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts1;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-853865917;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts1;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts1;id_000005;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCreateNonRecursive;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-103160491;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCreateNonRecursive;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCreateNonRecursive;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testFileStatusOnMountLink;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-953818212;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testFileStatusOnMountLink;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testFileStatusOnMountLink;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testFileStatusOnMountLink;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testFileStatusOnMountLink;id_000018;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathDanglingLink;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-504194854;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathDanglingLink;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathDanglingLink;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreate2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreate2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1665825514;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreate2;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreate2;id_000015;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveListFilesNotEndInColon;id_000000;[io.file.buffer.size=1135665046];java.lang.NegativeArraySizeException;-1481241874;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveListFilesNotEndInColon;id_000001;[fs.har.metadatacache.entries=740103152];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveListFilesNotEndInColon;id_000002;[io.file.buffer.size=1866654988];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveListFilesNotEndInColon;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveListFilesNotEndInColon;id_000004;[io.file.buffer.size=1729183552];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveListFilesNotEndInColon;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeHarFsModifications;id_000000;[io.file.buffer.size=1592476234];java.lang.NegativeArraySizeException;-541133314;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeHarFsModifications;id_000001;[fs.har.metadatacache.entries=720278353];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeHarFsModifications;id_000002;[io.file.buffer.size=1458071229];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeHarFsModifications;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeHarFsModifications;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveNewHarFsOnTheSameUnderlyingFs;id_000000;[io.file.buffer.size=1082436766];java.lang.NegativeArraySizeException;-355944792;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveNewHarFsOnTheSameUnderlyingFs;id_000001;[fs.har.metadatacache.entries=749167366];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveNewHarFsOnTheSameUnderlyingFs;id_000002;[io.file.buffer.size=1598336289];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveNewHarFsOnTheSameUnderlyingFs;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveNewHarFsOnTheSameUnderlyingFs;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveLruMetadataCacheFs;id_000000;[io.file.buffer.size=1824383798];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveLruMetadataCacheFs;id_000002;[io.file.buffer.size=2122072718];java.lang.NegativeArraySizeException;-2088106510;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveLruMetadataCacheFs;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveInitWithoutUnderlyingFS;id_000000;[io.file.buffer.size=1042727585];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveInitWithoutUnderlyingFS;id_000002;[fs.har.metadatacache.entries=1014360543];java.lang.NegativeArraySizeException;-11283100;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveInitWithoutUnderlyingFS;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testPositiveInitWithoutUnderlyingFS;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testListLocatedStatus;id_000000;[io.file.buffer.size=1409993889];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testListLocatedStatus;id_000001;[io.file.buffer.size=867131570];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testListLocatedStatus;id_000002;[fs.har.metadatacache.entries=1137641868];java.lang.NegativeArraySizeException;-955350795;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testListLocatedStatus;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testListLocatedStatus;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithoutIndex;id_000000;[fs.har.metadatacache.entries=1136514647];java.lang.NegativeArraySizeException;-1740580568;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithoutIndex;id_000001;[fs.har.metadatacache.entries=812702909];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithoutIndex;id_000002;[file.bytes-per-checksum=1450726633];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithoutIndex;id_000003;[fs.har.metadatacache.entries=1988497719];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithoutIndex;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithoutIndex;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestAvroFSInput#testAFSInput;id_000000;[io.file.buffer.size=1837401168, file.bytes-per-checksum=969943164];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestAvroFSInput#testAFSInput;id_000001;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=61906266, hadoop.kerberos.min.seconds.before.relogin=1399805275, fs.permissions.umask-mode=775, file.stream-buffer-size=101948886, hadoop.security.groups.cache.background.reload.threads=1010056553, fs.creation.parallel.count=619992071, fs.file.impl.disable.cache=true, io.file.buffer.size=1837401168];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestAvroFSInput#testAFSInput;id_000002;[file.bytes-per-checksum=1537758495];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestAvroFSInput#testAFSInput;id_000003;[file.bytes-per-checksum=803390180];java.lang.NegativeArraySizeException;-1359422972;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestAvroFSInput#testAFSInput;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestAvroFSInput#testAFSInput;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileContextResolveAfs#testFileContextResolveAfs;id_000000;[file.stream-buffer-size=865813108];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.crypto.key.TestKeyProviderFactory#testJksProviderWithKeytoolKeys;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.crypto.key.TestKeyProviderFactory#testJksProviderWithKeytoolKeys;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.crypto.key.TestKeyProviderFactory#testJksProviderWithKeytoolKeys;id_000003;[io.file.buffer.size=2115692674];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.util.TestGenericOptionsParser#testFilesOption;id_000000;[file.bytes-per-checksum=1950467969];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.util.TestGenericOptionsParser#testFilesOption;id_000001;[file.bytes-per-checksum=811970017];java.lang.NegativeArraySizeException;-1282204439;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.util.TestGenericOptionsParser#testFilesOption;id_000002;[file.bytes-per-checksum=2021953103];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.util.TestGenericOptionsParser#testFilesOption;id_000003;[file.bytes-per-checksum=1011416780, io.file.buffer.size=836614178];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.util.TestGenericOptionsParser#testFilesOption;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.util.TestGenericOptionsParser#testFilesOption;id_000006;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureCompressionNotWorking;id_000000;[io.file.buffer.size=1243863654, tfile.fs.output.buffer.size=1410196908];java.lang.NegativeArraySizeException;-1233042973;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureCompressionNotWorking;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=527634396, fs.permissions.umask-mode=770, file.stream-buffer-size=1259998121, fs.local.block.size=1849189670, fs.creation.parallel.count=264463896, fs.file.impl.disable.cache=true, io.file.buffer.size=679205560];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureCompressionNotWorking;id_000002;[tfile.fs.output.buffer.size=1255764589, file.bytes-per-checksum=593289271];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureCompressionNotWorking;id_000004;[fs.creation.parallel.count=0];java.lang.ArrayIndexOutOfBoundsException;Index 0 out of bounds for length 0;[org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream.write(SimpleBufferedOutputStream.java:50), java.io.DataOutputStream.writeByte(DataOutputStream.java:153), org.apache.hadoop.io.file.tfile.Utils.writeVLong(Utils.java:103), org.apache.hadoop.io.file.tfile.Utils.writeVInt(Utils.java:56), org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister.close(TFile.java:450)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testReadNullBuffer;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testReadFullyPastEOFZeroByteFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testReadAtExactEOF;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testAppendMissingTarget;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testBuilderAppendToEmptyFile;id_000000;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testBuilderAppendToEmptyFile;id_000001;[io.file.buffer.size=1625432389];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.ContractTestUtils.readDataset(ContractTestUtils.java:214)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testBuilderAppendToEmptyFile;id_000003;[io.file.buffer.size=2130753793];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testBuilderAppendToEmptyFile;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesFileRecursive;id_000000;[file.bytes-per-checksum=1074320028];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesFileRecursive;id_000001;[file.bytes-per-checksum=415435712];java.lang.NegativeArraySizeException;-556045888;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesFileRecursive;id_000002;[io.file.buffer.size=2101834792];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesFileRecursive;id_000004;[io.file.buffer.size=1249284884, file.bytes-per-checksum=1013715135];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusEmptyDirectory;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=628205413, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.kerberos.min.seconds.before.relogin=1611558545, hadoop.security.groups.cache.secs=1572985872, hadoop.service.shutdown.timeout=431551m, fs.permissions.umask-mode=000, hadoop.security.groups.negative-cache.secs=243938437, fs.local.block.size=1576271872, fs.creation.parallel.count=275942818, fs.file.impl.disable.cache=true];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureOutOfOrderKeys;id_000000;[file.bytes-per-checksum=1369589411];java.lang.NegativeArraySizeException;-558597189;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureOutOfOrderKeys;id_000001;[io.file.buffer.size=2049734437, file.bytes-per-checksum=85682975];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOpenEmptyFile;id_000000;[io.file.buffer.size=2123314241];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOpenEmptyFile;id_000002;[file.bytes-per-checksum=1718164849];java.lang.NegativeArraySizeException;-1716385543;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOpenEmptyFile;id_000004;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOpenRandomFile;id_000000;[file.bytes-per-checksum=1663811501];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOpenRandomFile;id_000001;[io.file.buffer.size=1631641931];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOpenRandomFile;id_000002;[file.bytes-per-checksum=1386297776];java.lang.NegativeArraySizeException;-408221904;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOpenRandomFile;id_000005;[file.bytes-per-checksum=1613817051];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOpenEmptyFile;id_000000;[io.file.buffer.size=1725716026];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOpenEmptyFile;id_000002;[file.bytes-per-checksum=1978018968];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOpenEmptyFile;id_000003;[file.bytes-per-checksum=1751586136];java.lang.NegativeArraySizeException;-1415593960;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOpenEmptyFile;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListLocatedStatusFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListLocatedStatusFile;id_000001;[io.file.buffer.size=2079140705];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListLocatedStatusFile;id_000003;[io.file.buffer.size=2123134471];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListLocatedStatusFile;id_000008;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testReadFullyZeroBytebufferPastEOF;id_000000;[hadoop.security.groups.cache.secs=1999984475, fs.permissions.umask-mode=777, hadoop.security.groups.negative-cache.secs=4017, fs.file.impl.disable.cache=true];java.lang.IllegalArgumentException;Invalid value of fs.creation.parallel.count: 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:415), org.apache.hadoop.fs.FileSystem$Cache.<init>(FileSystem.java:3514), org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:206), org.apache.hadoop.fs.contract.rawlocal.RawlocalFSContract.getLocalFS(RawlocalFSContract.java:46), org.apache.hadoop.fs.contract.localfs.LocalFSContract.init(LocalFSContract.java:63)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListFilesEmptyDirectoryRecursive;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyPastEOF;id_000000;[file.bytes-per-checksum=542115781];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyPastEOF;id_000001;[file.stream-buffer-size=1820593308];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyPastEOF;id_000002;[file.bytes-per-checksum=1945603803];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyPastEOF;id_000003;[file.bytes-per-checksum=839324754];java.lang.NegativeArraySizeException;-1036011806;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyPastEOF;id_000008;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureKeyLongerThan64K;id_000000;[io.file.buffer.size=1596113707, tfile.fs.output.buffer.size=901524832];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureKeyLongerThan64K;id_000001;[tfile.fs.output.buffer.size=847709740, file.bytes-per-checksum=123223941];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureKeyLongerThan64K;id_000002;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureKeyLongerThan64K;id_000003;[file.bytes-per-checksum=367441848];java.lang.NegativeArraySizeException;-987990664;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriterNotClosed;id_000000;[file.bytes-per-checksum=664649492];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriterNotClosed;id_000001;[file.stream-buffer-size=1718567947];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriterNotClosed;id_000002;[file.bytes-per-checksum=1678574791];java.lang.NegativeArraySizeException;-2072696065;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriterNotClosed;id_000009;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testChainedFailureAwaitFuture;id_000000;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=60963, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=7715711s, fs.permissions.umask-mode=007, hadoop.security.groups.cache.background.reload.threads=1465669769, hadoop.security.groups.cache.background.reload=true, fs.creation.parallel.count=587656231, fs.file.impl.disable.cache=true, hadoop.security.groups.cache.warn.after.ms=381033571];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenFileUnknownOption;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteInNonExistentDirectory;id_000000;[io.file.buffer.size=2140807164];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteInNonExistentDirectory;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteEmptyFile;id_000000;[io.file.buffer.size=1878638037];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.FileSystemContractBaseTest.writeAndRead(FileSystemContractBaseTest.java:937)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteEmptyFile;id_000002;[io.file.buffer.size=1876346988];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteEmptyFile;id_000003;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteEmptyFile;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testInputStreamStatisticKeys;id_000000;[file.stream-buffer-size=1914680098, io.file.buffer.size=1692999360];java.lang.NegativeArraySizeException;-1578369433;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testInputStreamStatisticKeys;id_000002;[file.stream-buffer-size=1696108228, file.bytes-per-checksum=84031818];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testInputStreamStatisticKeys;id_000003;[file.bytes-per-checksum=238481565];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testMkDirRmRfDir;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameFileUnderFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;Invalid value of fs.creation.parallel.count: 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:415), org.apache.hadoop.fs.FileSystem$Cache.<init>(FileSystem.java:3514), org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:206), org.apache.hadoop.fs.contract.rawlocal.RawlocalFSContract.getLocalFS(RawlocalFSContract.java:46), org.apache.hadoop.fs.contract.localfs.LocalFSContract.init(LocalFSContract.java:63)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameFileUnderFile;id_000001;[io.file.buffer.size=2112001717];java.lang.NoClassDefFoundError;Could not initialize class org.apache.hadoop.fs.FileSystem;[org.apache.hadoop.fs.contract.rawlocal.RawlocalFSContract.getLocalFS(RawlocalFSContract.java:46), org.apache.hadoop.fs.contract.localfs.LocalFSContract.init(LocalFSContract.java:63), org.apache.hadoop.fs.contract.AbstractFSContractTestBase.setup(AbstractFSContractTestBase.java:187), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteTwoBlocks;id_000000;[file.bytes-per-checksum=2099111343];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteTwoBlocks;id_000002;[file.stream-buffer-size=1776269425];java.lang.NegativeArraySizeException;-1097597569;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteTwoBlocks;id_000003;[file.stream-buffer-size=2124264983];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteTwoBlocks;id_000004;[file.stream-buffer-size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteTwoBlocks;id_000005;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteTwoBlocks;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithSomePathMatchesAndTrivialFilter;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFileUtil#testWriteBytesFileSystem;id_000000;[file.bytes-per-checksum=1050986882, io.file.buffer.size=734217514];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFileUtil#testWriteBytesFileSystem;id_000001;[file.bytes-per-checksum=2139031208];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:56), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.createNonRecursive(ChecksumFileSystem.java:565), org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder.build(FileSystem.java:4523)]
org.apache.hadoop.fs.TestFileUtil#testWriteBytesFileSystem;id_000002;[hadoop.security.groups.cache.secs=1750231462, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.service.shutdown.timeout=0481s, file.stream-buffer-size=155017561, fs.local.block.size=892122526, hadoop.security.groups.cache.background.reload=true, fs.file.impl.disable.cache=true, io.file.buffer.size=1213023811, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1073344122, hadoop.kerberos.keytab.login.autorenewal.enabled=true, fs.permissions.umask-mode=027, hadoop.security.groups.negative-cache.secs=495359511, fs.creation.parallel.count=1918276468, hadoop.security.groups.cache.warn.after.ms=689594050];java.lang.NegativeArraySizeException;-511130600;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.createNonRecursive(ChecksumFileSystem.java:565), org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder.build(FileSystem.java:4523)]
org.apache.hadoop.fs.TestFileUtil#testWriteBytesFileSystem;id_000004;[file.bytes-per-checksum=2001551908];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFileUtil#testWriteBytesFileSystem;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileUtil#testWriteBytesFileSystem;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyDirectoryWithDelete;id_000000;[io.file.buffer.size=727762642];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:419), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:409)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureBadCompressionCodec;id_000000;[file.bytes-per-checksum=976670564, io.file.buffer.size=981896137];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureBadCompressionCodec;id_000001;[tfile.fs.output.buffer.size=0];java.lang.NegativeArraySizeException;-99437188;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureKeyLongerThan64K;id_000000;[file.bytes-per-checksum=1111512671];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureKeyLongerThan64K;id_000002;[tfile.fs.output.buffer.size=1258144115, file.bytes-per-checksum=975534773];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureKeyLongerThan64K;id_000004;[file.bytes-per-checksum=771149994];java.lang.NegativeArraySizeException;-1649584646;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteNonEmptyDirectory;id_000000;[file.bytes-per-checksum=195821995, io.file.buffer.size=1689466054];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteNonEmptyDirectory;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=1275570492, hadoop.security.groups.cache.secs=1272295375, hadoop.security.dns.log-slow-lookups.enabled=true, file.stream-buffer-size=18280592, hadoop.security.groups.cache.background.reload.threads=157434751, fs.local.block.size=357879154, hadoop.security.groups.cache.background.reload=true, fs.file.impl.disable.cache=true, io.file.buffer.size=986614881, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1075791129, fs.permissions.umask-mode=640, fs.creation.parallel.count=670102174];java.lang.NegativeArraySizeException;-789600401;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteNonEmptyDirectory;id_000002;[file.bytes-per-checksum=1142506250];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteNonEmptyDirectory;id_000004;[hadoop.service.shutdown.timeout=99912h, hadoop.security.groups.negative-cache.secs=1903302874, fs.contract.supports-strict-exceptions=true, hadoop.security.groups.cache.warn.after.ms=656252069];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteNonEmptyDirectory;id_000007;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteNonEmptyDirectory;id_000008;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteEmptyDirectory;id_000000;[file.bytes-per-checksum=1282713869];java.lang.NegativeArraySizeException;-1340477067;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testOverwriteEmptyDirectory;id_000004;[file.bytes-per-checksum=713067258];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsNonExistentDirectory;id_000000;[hadoop.security.token.service.use_ip=false, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=20992m, fs.permissions.umask-mode=007, file.stream-buffer-size=1675029370, fs.local.block.size=645911125, fs.creation.parallel.count=1549472033, fs.file.impl.disable.cache=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsNonExistentDirectory;id_000001;[file.bytes-per-checksum=318140200];java.lang.NegativeArraySizeException;-1431705496;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsNonExistentDirectory;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=983601176, hadoop.kerberos.min.seconds.before.relogin=1234662045, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=405547837m, fs.permissions.umask-mode=400, file.stream-buffer-size=1512641670, hadoop.security.groups.negative-cache.secs=1008248626, fs.creation.parallel.count=194465455, fs.file.impl.disable.cache=true, fs.automatic.close=true];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsNonExistentDirectory;id_000005;[hadoop.security.dns.log-slow-lookups.enabled=true, fs.permissions.umask-mode=000, file.stream-buffer-size=1512636416, fs.local.block.size=2015494144, fs.creation.parallel.count=1401205073, fs.file.impl.disable.cache=true];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testOverwriteEmptyDirectory;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1681103350, hadoop.security.auth_to_local.mechanism=MIT, hadoop.kerberos.min.seconds.before.relogin=40622685, hadoop.security.groups.cache.secs=1417931606, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.service.shutdown.timeout=96379s, hadoop.security.groups.negative-cache.secs=1424019027, fs.local.block.size=647755060, hadoop.security.groups.cache.background.reload=true, file.bytes-per-checksum=2145712919, fs.contract.supports-strict-exceptions=true, hadoop.security.groups.cache.warn.after.ms=1557429400];java.io.FileNotFoundException;/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task127/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/YpXqNVz7Mo/testOverwriteEmptyDirectory (Is a directory);[java.io.FileOutputStream.open0(Native Method), java.io.FileOutputStream.open(FileOutputStream.java:298), java.io.FileOutputStream.<init>(FileOutputStream.java:237), org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:321), org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)]
org.apache.hadoop.fs.TestTruncatedInputBug#testTruncatedInputBug;id_000000;[file.bytes-per-checksum=1393109400];java.lang.NegativeArraySizeException;-346917288;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestTruncatedInputBug#testTruncatedInputBug;id_000001;[file.bytes-per-checksum=1074575450];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestTruncatedInputBug#testTruncatedInputBug;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestTrash#testCheckpointInterval;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestTrash#testTrashRestarts;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileIntoDirFile;id_000000;[file.bytes-per-checksum=443216112];java.lang.NegativeArraySizeException;-306022288;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testFilesystemIsCaseSensitive;id_000000;[io.file.buffer.size=1403124899];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.security.TestUserGroupInformation#testImportTokensFromConfig;id_000000;[file.bytes-per-checksum=1827349251];java.lang.NegativeArraySizeException;-733725925;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.TestUserGroupInformation#testImportTokensFromConfig;id_000001;[file.bytes-per-checksum=697538919];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.TestUserGroupInformation#testImportTokensFromConfig;id_000002;[file.stream-buffer-size=1860993447, io.file.buffer.size=310223608];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.TestUserGroupInformation#testImportTokensFromConfig;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestUserGroupInformation#testImportTokensFromConfig;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.TestUserGroupInformation#testExternalTokenFiles;id_000000;[io.file.buffer.size=2114460966];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.TestUserGroupInformation#testExternalTokenFiles;id_000001;[file.bytes-per-checksum=1836159141];java.lang.NegativeArraySizeException;-654436915;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.TestUserGroupInformation#testExternalTokenFiles;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.TestUserGroupInformation#testExternalTokenFiles;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.util.TestGenericOptionsParser#testDOptionParsing;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testHarFsWithoutAuthority;id_000000;[file.bytes-per-checksum=1419327576];java.lang.NegativeArraySizeException;-110953704;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testHarFsWithoutAuthority;id_000001;[fs.har.metadatacache.entries=753508232];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testHarFsWithoutAuthority;id_000002;[fs.har.metadatacache.entries=707567033];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testHarFsWithoutAuthority;id_000003;[io.file.buffer.size=2014661335];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestHarFileSystemBasics#testHarFsWithoutAuthority;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testHarFsWithoutAuthority;id_000006;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testMakeQualifiedPath;id_000000;[io.file.buffer.size=1589836007];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testMakeQualifiedPath;id_000001;[fs.har.metadatacache.entries=815898946];java.lang.NegativeArraySizeException;-1049369926;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testMakeQualifiedPath;id_000002;[io.file.buffer.size=1622670276];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testMakeQualifiedPath;id_000003;[io.file.buffer.size=1988818761];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestHarFileSystemBasics#testMakeQualifiedPath;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testMakeQualifiedPath;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeLocalFileSystem#testLocalFsLinkSlashMergeWithOtherMountLinks;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=599338705, fs.client.resolve.remote.symlinks=false, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.overload.scheme.target.file.impl=org.apache.hadoop.fs.LocalFileSystem, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=2080434192, fs.creation.parallel.count=398968184, hadoop.security.dns.log-slow-lookups.threshold.ms=1578358205, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=200857727, fs.local.block.size=1557814520, fs.viewfs.mounttable.mt.link./lfsroot=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task77/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/Bl6EfsKq1N/wd2, fs.file.impl.disable.cache=true, fs.viewfs.mounttable.mt.linkMergeSlash=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task77/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/Bl6EfsKq1N/wd2, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=2031039797, fs.viewfs.ignore.port.in.mount.table.name=false, fs.permissions.umask-mode=600, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.viewfs.enable.inner.cache=false, fs.file.impl=org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme, hadoop.security.groups.cache.warn.after.ms=552327794];java.io.IOException;Mount table mt has already been configured with regular links. A merge slash link should not be configured.;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:602), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme.initialize(ViewFileSystemOverloadScheme.java:161), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]
org.apache.hadoop.fs.TestTrash#testTrashPermission;id_000000;[file.bytes-per-checksum=1533713150];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestTrash#testTrashPermission;id_000002;[file.bytes-per-checksum=1374144195];java.lang.NegativeArraySizeException;-517604133;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystemTestWrapper.create(FileSystemTestWrapper.java:292)]
org.apache.hadoop.fs.TestTrash#testTrashPermission;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestTrash#testTrashPermission;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestTrash#testNonDefaultFS;id_000000;[file.bytes-per-checksum=1611715901];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestTrash#testNonDefaultFS;id_000001;[file.bytes-per-checksum=827057725];java.lang.NegativeArraySizeException;-1146415067;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestTrash#testNonDefaultFS;id_000002;[file.bytes-per-checksum=1182022167];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestTrash#testNonDefaultFS;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestTrash#testNonDefaultFS;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig#testLocalFsLinkSlashMergeWithOtherMountLinks;id_000000;[file.stream-buffer-size=1332970202, io.file.buffer.size=745084005];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig#testLocalFsLinkSlashMergeWithOtherMountLinks;id_000001;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=909348702, fs.client.resolve.remote.symlinks=false, file.stream-buffer-size=105350104, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.overload.scheme.target.file.impl=org.apache.hadoop.fs.LocalFileSystem, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1554479337, fs.creation.parallel.count=263567152, hadoop.security.dns.log-slow-lookups.threshold.ms=1177845530, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=607991315, fs.local.block.size=1449553262, fs.file.impl.disable.cache=false, io.file.buffer.size=1929654948, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=111364155, fs.viewfs.ignore.port.in.mount.table.name=false, fs.permissions.umask-mode=002, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.viewfs.enable.inner.cache=false, fs.file.impl=org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme, hadoop.security.groups.cache.warn.after.ms=60901718];java.io.IOException;Mount table mt has already been configured with regular links. A merge slash link should not be configured.;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:602), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme.initialize(ViewFileSystemOverloadScheme.java:161), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]
org.apache.hadoop.util.TestGenericOptionsParser#testLibjarsOption;id_000000;[file.stream-buffer-size=282601061, io.file.buffer.size=2029532678];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.util.TestGenericOptionsParser#testLibjarsOption;id_000001;[file.bytes-per-checksum=1250918865];java.lang.NegativeArraySizeException;-1626632103;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.util.TestGenericOptionsParser#testLibjarsOption;id_000003;[file.bytes-per-checksum=1560287345];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.util.TestGenericOptionsParser#testLibjarsOption;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.util.TestGenericOptionsParser#testLibjarsOption;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testSerializationAvailability;id_000000;[file.bytes-per-checksum=1476742961];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.TestSequenceFile#testSerializationAvailability;id_000001;[file.stream-buffer-size=1363858034, io.file.buffer.size=912591541];java.lang.NegativeArraySizeException;-679446040;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFile#testSerializationAvailability;id_000003;[io.file.buffer.size=2104619388];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testSerializationAvailability;id_000004;[io.file.buffer.size=707252085, file.bytes-per-checksum=158994600];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.TestSequenceFile#testSerializationAvailability;id_000005;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.TestSequenceFile#testSequenceFileWriter;id_000000;[io.file.buffer.size=1826820259];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1307), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499)]
org.apache.hadoop.io.TestSequenceFile#testSequenceFileWriter;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170)]
org.apache.hadoop.io.TestSequenceFile#testSequenceFileWriter;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.TestSequenceFile#testCreateWriterOnExistingFile;id_000000;[file.bytes-per-checksum=2138741861];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFile#testCreateWriterOnExistingFile;id_000001;[file.bytes-per-checksum=1295964814];java.lang.NegativeArraySizeException;-1221218562;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFile#testCreateWriterOnExistingFile;id_000002;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.TestSequenceFile#testCreateWriterOnExistingFile;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.TestUserGroupInformation#testSetConfigWithRules;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenFileLazyFail;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneAndAHalfBlocks;id_000000;[file.stream-buffer-size=1726194463];java.lang.NegativeArraySizeException;-287877679;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneAndAHalfBlocks;id_000002;[file.bytes-per-checksum=158633352];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneAndAHalfBlocks;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneAndAHalfBlocks;id_000005;[file.stream-buffer-size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteOneAndAHalfBlocks;id_000006;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteEmptyFile;id_000000;[file.bytes-per-checksum=1507541383];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteEmptyFile;id_000002;[file.bytes-per-checksum=750995880];java.lang.NegativeArraySizeException;-1830971672;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteEmptyFile;id_000003;[file.bytes-per-checksum=975586820];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteEmptyFile;id_000004;[io.file.buffer.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteOneAndAHalfBlocks;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteOneAndAHalfBlocks;id_000001;[io.file.buffer.size=1805380898];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.FileSystemContractBaseTest.writeAndRead(FileSystemContractBaseTest.java:937)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteOneAndAHalfBlocks;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.security.TestUserGroupInformation#testPrivateTokenExclusion;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.security.TestUserGroupInformation#testEqualsWithRealUser;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testSequentialRead;id_000000;[fs.permissions.umask-mode=027, fs.creation.parallel.count=43326477, file.bytes-per-checksum=997926527];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.AbstractContractOpenTest.testSequentialRead(AbstractContractOpenTest.java:172)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testSequentialRead;id_000001;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=1107754724, hadoop.kerberos.min.seconds.before.relogin=1490874765, hadoop.security.groups.cache.secs=1468590421, hadoop.service.shutdown.timeout=9m, file.stream-buffer-size=1085590698, hadoop.security.groups.negative-cache.secs=1545216789, fs.local.block.size=2135076077, fs.creation.parallel.count=1501535628, fs.file.impl.disable.cache=true, fs.automatic.close=true, hadoop.security.groups.cache.warn.after.ms=1593400077];java.lang.NegativeArraySizeException;-125098015;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testSequentialRead;id_000003;[file.stream-buffer-size=1528226422, file.bytes-per-checksum=154578770];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testSequentialRead;id_000004;[file.bytes-per-checksum=1495345877];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameNonexistentFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.util.TestRunJar#testBigJar;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestChecksumFileSystem#testSetConf;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestUserGroupInformation#testEquals;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.security.TestUserGroupInformation#testTokenIdentifiers;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.util.TestGenericOptionsParser#testConfWithMultipleOpts;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriteRecordAfterMetaBlock;id_000000;[io.file.buffer.size=1693328313, tfile.fs.output.buffer.size=1278822727];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriteRecordAfterMetaBlock;id_000002;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriteRecordAfterMetaBlock;id_000004;[file.bytes-per-checksum=358953783];java.lang.NegativeArraySizeException;-1064383249;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureWriteMetaBlocksWithSameName;id_000000;[io.file.buffer.size=1115768833, tfile.fs.output.buffer.size=1026820879];java.lang.NegativeArraySizeException;-831405113;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyTooLong;id_000000;[io.file.buffer.size=1481077883, tfile.fs.output.buffer.size=1251949813];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyTooLong;id_000001;[file.bytes-per-checksum=1804515187];java.lang.NegativeArraySizeException;-939232501;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyTooLong;id_000002;[tfile.fs.output.buffer.size=1351463087, io.file.buffer.size=1169791074];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyTooLong;id_000023;[file.bytes-per-checksum=1561979392];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testPositionedBulkReadDoesntChangePosition;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureValueTooLong;id_000000;[io.file.buffer.size=1919536062, tfile.fs.output.buffer.size=1041032482];java.lang.NegativeArraySizeException;-1895372977;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureValueTooLong;id_000001;[file.bytes-per-checksum=1669579676];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureValueTooLong;id_000002;[io.file.buffer.size=1727236958, tfile.fs.output.buffer.size=666667006];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureValueTooLong;id_000004;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureValueTooLong;id_000005;[io.file.buffer.size=1782782035, file.bytes-per-checksum=85530531];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListLocatedStatusFiltering;id_000000;[file.bytes-per-checksum=214963119];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListLocatedStatusFiltering;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=1437329378, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.service.shutdown.timeout=9863m, file.stream-buffer-size=1132892211, hadoop.security.groups.cache.background.reload.threads=202104328, fs.local.block.size=1146971986, fs.file.impl.disable.cache=true, io.file.buffer.size=14076501, hadoop.security.token.service.use_ip=false, fs.permissions.umask-mode=277, hadoop.security.groups.negative-cache.secs=1953845328, fs.contract.supports-getfilestatus=true, fs.creation.parallel.count=95767387, fs.automatic.close=true, hadoop.security.groups.cache.warn.after.ms=1234811636];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListLocatedStatusFiltering;id_000002;[file.stream-buffer-size=2033067232, io.file.buffer.size=1116865800];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListLocatedStatusFiltering;id_000003;[file.bytes-per-checksum=760377830];java.lang.NegativeArraySizeException;-1746534122;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureNegativeOffset;id_000000;[file.bytes-per-checksum=914082077];java.lang.NegativeArraySizeException;-363195899;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureNegativeOffset;id_000001;[file.bytes-per-checksum=652842669];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureNegativeOffset;id_000003;[file.bytes-per-checksum=1584931829];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureNegativeLength;id_000000;[file.bytes-per-checksum=1156000739];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureNegativeLength;id_000002;[file.bytes-per-checksum=928520107];java.lang.NegativeArraySizeException;-233253629;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureNegativeLength;id_000003;[file.bytes-per-checksum=1998730667];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureOpenEmptyFile;id_000000;[io.file.buffer.size=1232839487];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureOpenEmptyFile;id_000001;[file.bytes-per-checksum=648411018];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureOpenEmptyFile;id_000002;[file.bytes-per-checksum=671239514];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureOpenEmptyFile;id_000003;[file.bytes-per-checksum=1753704118];java.lang.NegativeArraySizeException;-1396532122;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testOneBlockPlusOneEntry;id_000000;[file.bytes-per-checksum=1229203419];java.lang.NegativeArraySizeException;-1822071117;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testOneBlockPlusOneEntry;id_000001;[file.bytes-per-checksum=2110974416];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testOneBlockPlusOneEntry;id_000005;[hadoop.security.dns.log-slow-lookups.threshold.ms=2142784539, hadoop.security.groups.cache.secs=1193797609, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=09935866h, file.stream-buffer-size=659044475, fs.local.block.size=823295559, fs.file.impl.disable.cache=true, io.file.buffer.size=111320314, tfile.fs.input.buffer.size=639022739, fs.permissions.umask-mode=660, hadoop.security.groups.negative-cache.secs=1693759568, fs.creation.parallel.count=1240459998, file.bytes-per-checksum=106020972, hadoop.security.groups.cache.warn.after.ms=752230711];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testNoDataEntry;id_000000;[file.bytes-per-checksum=1903054201];java.lang.NegativeArraySizeException;-52381375;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testNoDataEntry;id_000001;[file.stream-buffer-size=880866534, io.file.buffer.size=1719634236];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testNoDataEntry;id_000002;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testNoDataEntry;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=1428348764, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1770838204, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.service.shutdown.timeout=82m, file.stream-buffer-size=880813798, fs.local.block.size=2077199938, fs.file.impl.disable.cache=true, io.file.buffer.size=819302526, hadoop.kerberos.min.seconds.before.relogin=1721932102, fs.permissions.umask-mode=770, fs.creation.parallel.count=752536194, fs.automatic.close=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testNoDataEntry;id_000004;[file.bytes-per-checksum=1560281088];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCompressionNotWorking;id_000000;[tfile.io.chunk.size=633326541, io.file.buffer.size=1422327627];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCompressionNotWorking;id_000001;[file.bytes-per-checksum=1732892746];java.lang.NegativeArraySizeException;-1583834470;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCompressionNotWorking;id_000002;[file.bytes-per-checksum=488272886, tfile.io.chunk.size=1885882048];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureCompressionNotWorking;id_000003;[tfile.fs.output.buffer.size=1645905367, tfile.io.chunk.size=816846059];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.security.alias.TestCredShell#testPromptForCredential;id_000000;[file.bytes-per-checksum=204975326];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.alias.TestCredShell#testPromptForCredential;id_000002;[file.bytes-per-checksum=973774072];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testFailureSeek;id_000000;[io.file.buffer.size=1735345918, tfile.fs.output.buffer.size=1946276689];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testFailureSeek;id_000001;[file.bytes-per-checksum=232781648];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testFailureSeek;id_000002;[file.bytes-per-checksum=1847299160];java.lang.NegativeArraySizeException;-554176744;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testFailureSeek;id_000004;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testScanRange;id_000000;[tfile.fs.output.buffer.size=2093398107];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:431), org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642), org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testScanRange;id_000001;[file.bytes-per-checksum=1309668706];java.lang.NegativeArraySizeException;-1097883534;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testScanRange;id_000003;[file.bytes-per-checksum=1183589648];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.compress.TestCompressionStreamReuse#testGzipCompressStreamReuseWithParam;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:60), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:160), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:171)]
org.apache.hadoop.io.compress.TestCompressionStreamReuse#testGzipCompressStreamReuseWithParam;id_000001;[io.file.buffer.size=2065601565];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:160), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:171)]
org.apache.hadoop.security.TestLdapGroupsMapping#testConfGetPasswordUsingAlias;id_000000;[io.file.buffer.size=2146974483];java.lang.NegativeArraySizeException;-1727254934;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.TestLdapGroupsMapping#testConfGetPasswordUsingAlias;id_000002;[file.bytes-per-checksum=1577633974];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.TestLdapGroupsMapping#testConfGetPasswordUsingAlias;id_000003;[file.bytes-per-checksum=1067883437];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.TestLdapGroupsMapping#testConfGetPasswordUsingAlias;id_000004;[file.bytes-per-checksum=1029921574];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryUnknownLength;id_000000;[tfile.io.chunk.size=1002696503, tfile.fs.output.buffer.size=1330982718];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.OutOfMemoryError(Java heap space),  java.lang.OutOfMemoryError(Java heap space);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryUnknownLength;id_000003;[file.bytes-per-checksum=885136730];java.lang.NegativeArraySizeException;-623704022;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryUnknownLength;id_000004;[tfile.fs.output.buffer.size=0];java.lang.ArrayIndexOutOfBoundsException;Index 0 out of bounds for length 0;[org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream.write(SimpleBufferedOutputStream.java:50), java.io.DataOutputStream.writeByte(DataOutputStream.java:153), org.apache.hadoop.io.file.tfile.Utils.writeVLong(Utils.java:103), org.apache.hadoop.io.file.tfile.Utils.writeVInt(Utils.java:56), org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister.close(TFile.java:450)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryMixedLengths1;id_000000;[tfile.io.chunk.size=1922436154, tfile.fs.output.buffer.size=1299737497];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryMixedLengths1;id_000001;[file.bytes-per-checksum=617773613, tfile.io.chunk.size=1935695145];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryMixedLengths1;id_000002;[file.bytes-per-checksum=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryMixedLengths1;id_000003;[io.file.buffer.size=2084450607];java.lang.NegativeArraySizeException;-789530506;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testLocate;id_000000;[io.file.buffer.size=1144982939, tfile.fs.input.buffer.size=1173102998];java.lang.NegativeArraySizeException;-1438241187;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testLocate;id_000001;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testLocate;id_000004;[tfile.fs.output.buffer.size=1792979153, file.bytes-per-checksum=145964875];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testTwoDataEntries;id_000000;[io.file.buffer.size=1783315753, tfile.fs.output.buffer.size=1233756426];java.lang.NegativeArraySizeException;-969292754;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testTwoDataEntries;id_000001;[io.file.buffer.size=1963369531, tfile.fs.output.buffer.size=179432884];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testTwoDataEntries;id_000002;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testTwoDataEntries;id_000003;[io.file.buffer.size=326316837, tfile.fs.output.buffer.size=1928240680];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testTwoDataEntries;id_000004;[file.bytes-per-checksum=1944448029];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testOneBlock;id_000000;[io.file.buffer.size=1870461028, tfile.fs.output.buffer.size=1150738702];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testOneBlock;id_000002;[hadoop.kerberos.min.seconds.before.relogin=1219112664, fs.permissions.umask-mode=027, file.stream-buffer-size=1464956432, hadoop.security.groups.negative-cache.secs=1687876891, fs.local.block.size=1552640852, fs.creation.parallel.count=1834691577, fs.file.impl.disable.cache=true, io.file.buffer.size=255985397, hadoop.security.groups.cache.warn.after.ms=601709359];java.lang.NegativeArraySizeException;-1681013542;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testOneBlock;id_000004;[file.bytes-per-checksum=1668253305];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testLocate;id_000000;[tfile.fs.input.buffer.size=1663038453, hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=754674545, hadoop.security.dns.log-slow-lookups.enabled=false, tfile.fs.output.buffer.size=2052873616];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testLocate;id_000002;[tfile.fs.output.buffer.size=536357400, file.bytes-per-checksum=592726914];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testLocate;id_000004;[file.bytes-per-checksum=891705407];java.lang.NegativeArraySizeException;-564585929;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureReadValueManyTimes;id_000000;[file.bytes-per-checksum=1553913795];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureReadValueManyTimes;id_000001;[file.bytes-per-checksum=1956189029];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureReadValueManyTimes;id_000002;[file.stream-buffer-size=1979260281, io.file.buffer.size=544366710];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureReadValueManyTimes;id_000003;[file.bytes-per-checksum=946593849];java.lang.NegativeArraySizeException;-70589951;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testTwoDataEntries;id_000000;[tfile.fs.output.buffer.size=2100484794];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testTwoDataEntries;id_000001;[io.file.buffer.size=2073760194];java.lang.NegativeArraySizeException;-1639191176;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testTwoDataEntries;id_000002;[file.bytes-per-checksum=708579564];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testTwoDataEntries;id_000003;[file.bytes-per-checksum=1918807242];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureReadValueManyTimes;id_000000;[file.bytes-per-checksum=965412070];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:431), org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642), org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureReadValueManyTimes;id_000001;[tfile.fs.output.buffer.size=1728204441, file.bytes-per-checksum=132976672];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureReadValueManyTimes;id_000002;[io.file.buffer.size=2111859909];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryUnknownLength;id_000000;[io.file.buffer.size=946365170, tfile.fs.output.buffer.size=1498087352];java.lang.NegativeArraySizeException;-632136302;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryUnknownLength;id_000001;[file.bytes-per-checksum=1444844307];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryUnknownLength;id_000002;[file.bytes-per-checksum=1995402477];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testOneEntryUnknownLength;id_000003;[file.bytes-per-checksum=1974768403];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.TestSequenceFileSync#testLowSyncpoint;id_000000;[file.bytes-per-checksum=886613670];java.lang.NegativeArraySizeException;-610411562;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFileSync#testLowSyncpoint;id_000002;[file.bytes-per-checksum=2004806357];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.TestSequenceFileSync#testLowSyncpoint;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=730615285, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.kerberos.min.seconds.before.relogin=1829374356, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.dns.log-slow-lookups.enabled=false, fs.permissions.umask-mode=770, file.stream-buffer-size=757799712, hadoop.security.groups.negative-cache.secs=358544109, fs.local.block.size=1217874010, fs.creation.parallel.count=488988144, fs.file.impl.disable.cache=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFileSync#testLowSyncpoint;id_000004;[io.file.buffer.size=2140507363];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.TestSequenceFileSync#testLowSyncpoint;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCompressionNotWorking2;id_000000;[file.bytes-per-checksum=357722653];java.lang.NegativeArraySizeException;-1075463419;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCompressionNotWorking2;id_000001;[io.file.buffer.size=465769487, file.bytes-per-checksum=609184285];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCompressionNotWorking2;id_000002;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCompressionNotWorking2;id_000004;[hadoop.security.dns.log-slow-lookups.threshold.ms=44587240, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.kerberos.min.seconds.before.relogin=56934771, fs.permissions.umask-mode=660, file.stream-buffer-size=687905846, fs.local.block.size=293589714, tfile.fs.output.buffer.size=656676735, fs.creation.parallel.count=755900812, fs.file.impl.disable.cache=true, io.file.buffer.size=465813710];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testNoEntry;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=503362865, hadoop.security.groups.cache.secs=1063743841, file.stream-buffer-size=865315364, hadoop.security.groups.cache.background.reload.threads=1935198748, fs.local.block.size=1132899763, hadoop.security.groups.cache.background.reload=true, fs.file.impl.disable.cache=true, io.file.buffer.size=289943107, fs.permissions.umask-mode=755, hadoop.security.groups.negative-cache.secs=1487354442, tfile.fs.output.buffer.size=3052853, fs.creation.parallel.count=1798161182, hadoop.security.groups.cache.warn.after.ms=1626236762];java.lang.NegativeArraySizeException;-1421189171;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testNoEntry;id_000002;[file.bytes-per-checksum=1135807141];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testNoEntry;id_000021;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIntoDestinationSubDirectoryWithDelSrc;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testDestinationDirectoryToSelf;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal#testSilentExistingWrite;id_000000;[file.bytes-per-checksum=875140262];java.lang.NegativeArraySizeException;-713672234;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal#testWrite;id_000000;[file.bytes-per-checksum=120879042, io.file.buffer.size=1396367025];java.lang.AssertionError;Sink did not produce the expected output. Actual output was: ;[org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.metrics2.sink.RollingFileSystemSinkTestBase.assertMetricsContents(RollingFileSystemSinkTestBase.java:345), org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testWrite(TestRollingFileSystemSinkWithLocal.java:43), org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal.testWrite$$CONFUZZ(TestRollingFileSystemSinkWithLocal.java)]
org.apache.hadoop.metrics2.sink.TestRollingFileSystemSinkWithLocal#testSilentWrite;id_000000;[file.bytes-per-checksum=1791455587];java.io.FileNotFoundException;File file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task138/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/RollingFileSystemSinkTest/testSilentWrite$$CONFUZZ/202301250600/testsrc-732031c6e19d482e86c4766985f227e600000N.log does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)]
org.apache.hadoop.io.TestSequenceFile#testRecursiveSeqFileCreate;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testGetActualUGI;id_000000;[hadoop.security.kms.client.encrypted.key.cache.size=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testGetActualUGI;id_000001;[hadoop.security.kms.client.encrypted.key.cache.expiry=0];java.lang.IllegalArgumentException;"expiry" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:230), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testGetActualUGI;id_000002;[hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=0];java.lang.IllegalArgumentException;"numFillerThreads" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:231), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testGetActualUGI;id_000003;[hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.0f];java.lang.IllegalArgumentException;"lowWatermark" must be > 0 and <= 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:225), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testTokenSelectionWithKMSUriInConf;id_000000;[hadoop.security.kms.client.encrypted.key.cache.expiry=0];java.lang.IllegalArgumentException;"expiry" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:230), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testTokenSelectionWithKMSUriInConf;id_000001;[hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.0f];java.lang.IllegalArgumentException;"lowWatermark" must be > 0 and <= 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:225), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testTokenSelectionWithKMSUriInConf;id_000003;[hadoop.security.kms.client.encrypted.key.cache.size=0];java.lang.IllegalArgumentException;"numValues" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:224), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testTokenSelectionWithKMSUriInConf;id_000004;[hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=0];java.lang.IllegalArgumentException;"numFillerThreads" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:231), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory.createProviders(KMSClientProvider.java:319)]
org.apache.hadoop.io.TestMapFile#testMainMethodMapFile;id_000000;[io.file.buffer.size=1934575431];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.DefaultCodec.createInputStream(DefaultCodec.java:92), org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:2055), org.apache.hadoop.io.SequenceFile$Reader.initialize(SequenceFile.java:1940)]
org.apache.hadoop.service.launcher.TestServiceLauncher#testRunService;id_000000;[delay.time=543748889];org.junit.runners.model.TestTimedOutException;test timed out after 15000 milliseconds;[java.lang.Object.wait(Native Method), org.apache.hadoop.service.AbstractService.waitForServiceToStop(AbstractService.java:279), org.apache.hadoop.service.launcher.ServiceLauncher.coreServiceLaunch(ServiceLauncher.java:638), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:495), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:469)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testOpenFileRead;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToDestinationWithParentFile;id_000000;[hadoop.security.token.service.use_ip=false, hadoop.security.dns.log-slow-lookups.threshold.ms=731756280, fs.permissions.umask-mode=077, file.stream-buffer-size=1340388019, hadoop.security.groups.negative-cache.secs=1352403872, hadoop.security.groups.cache.background.reload.threads=1443048737, fs.file.impl.disable.cache=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToDestinationWithParentFile;id_000002;[file.bytes-per-checksum=1998025030];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToDestinationWithParentFile;id_000003;[file.bytes-per-checksum=1832531432];java.lang.NegativeArraySizeException;-687086296;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToDestinationWithParentFile;id_000004;[hadoop.kerberos.min.seconds.before.relogin=1673896708, hadoop.security.groups.cache.secs=45748, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.service.shutdown.timeout=481s, fs.permissions.umask-mode=644, file.stream-buffer-size=503495561, hadoop.security.groups.negative-cache.secs=5718268, fs.local.block.size=1531952728, fs.file.impl.disable.cache=true, hadoop.security.groups.cache.warn.after.ms=510853120];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.ipc.TestRPC#testClientRpcTimeout;id_000000;[ipc.server.read.threadpool.size=752736873];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371)]
org.apache.hadoop.ipc.TestRPC#testClientBackOffByResponseTime;id_000000;[ipc.0.faircallqueue.decay-scheduler.period-ms=1211386988];java.lang.IllegalArgumentException;responseTimeThresholds must match with the number of priority levels;[org.apache.hadoop.ipc.DecayRpcScheduler.parseBackOffResponseTimeThreshold(DecayRpcScheduler.java:397), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:235), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62), jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)]
org.apache.hadoop.ipc.TestRPC#testClientBackOffByResponseTime;id_000001;[ipc.0.faircallqueue.decay-scheduler.period-ms=1173561596];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.DecayRpcScheduler.getDefaultThresholds(DecayRpcScheduler.java:377), org.apache.hadoop.ipc.DecayRpcScheduler.parseThresholds(DecayRpcScheduler.java:346), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:231), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.contract.sftp.TestSFTPContractSeek#testRandomSeeks;id_000000;[fs.contract.supports-seek=true, hadoop.security.groups.cache.background.reload=false, fs.sftp.host.port=44439, fs.creation.parallel.count=776284090, hadoop.security.groups.cache.warn.after.ms=153202206];org.junit.runners.model.TestTimedOutException;test timed out after 180000 milliseconds;[com.jcraft.jsch.ChannelSftp$2.read(ChannelSftp.java:1356), org.apache.hadoop.fs.sftp.SFTPInputStream.read(SFTPInputStream.java:115), java.io.InputStream.read(InputStream.java:280), java.io.DataInputStream.readFully(DataInputStream.java:200), java.io.DataInputStream.readFully(DataInputStream.java:170)]
org.apache.hadoop.fs.contract.sftp.TestSFTPContractSeek#testRandomSeeks;id_000001;[fs.contract.test.random-seek-count=1829306332];org.junit.runners.model.TestTimedOutException;test timed out after 180000 milliseconds;[java.lang.Object.wait(Native Method), java.io.PipedInputStream.read(PipedInputStream.java:326), java.io.PipedInputStream.read(PipedInputStream.java:377), com.jcraft.jsch.ChannelSftp.fill(ChannelSftp.java:2909), com.jcraft.jsch.ChannelSftp.header(ChannelSftp.java:2935)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testNodeUpAferAWhile;id_000000;[zk-dt-secret-manager.token.seqnum.batch.size=0];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[jdk.internal.misc.Unsafe.park(Native Method), java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234), java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1079), java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1369), java.util.concurrent.CountDownLatch.await(CountDownLatch.java:278)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testNodeUpAferAWhile;id_000001;[zk-dt-secret-manager.token.seqnum.batch.size=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testNodesLoadedAfterRestart;id_000000;[zk-dt-secret-manager.zkSessionTimeout=0];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[jdk.internal.misc.Unsafe.park(Native Method), java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234), java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1079), java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1369), java.util.concurrent.CountDownLatch.await(CountDownLatch.java:278)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetupWithoutTrustPassword;id_000000;[hadoop.http.selector.count=542777558];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetupWithoutTrustPassword;id_000002;[hadoop.http.selector.count=447867179];java.lang.IllegalStateException;Insufficient configured threads: required=15375717 < max=10 for QueuedThreadPool[qtp1905768812]@7197b96c{STARTED,8<=8<=10,i=8,r=-1,q=0}[ReservedThreadExecutor@2db27b46{s=0/1,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetupWithoutKeyStorePassword;id_000000;[hadoop.http.selector.count=1453436301];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetupWithoutKeyStorePassword;id_000002;[hadoop.http.selector.count=187288425];java.lang.IllegalStateException;Insufficient configured threads: required=317083364 < max=10 for QueuedThreadPool[qtp1262395522]@4b3ea082{STARTED,8<=8<=10,i=8,r=-1,q=0}[ReservedThreadExecutor@48531d5{s=0/1,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetup;id_000000;[hadoop.http.selector.count=2072399873];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetup;id_000002;[hadoop.http.selector.count=123634422];java.lang.IllegalStateException;Insufficient configured threads: required=298056652 < max=10 for QueuedThreadPool[qtp1975249801]@75bbeb89{STARTED,8<=8<=10,i=8,r=-1,q=0}[ReservedThreadExecutor@7f68e671{s=0/1,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutPassword;id_000000;[hadoop.http.selector.count=2111510563];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutPassword;id_000002;[hadoop.http.selector.count=190482228];java.lang.IllegalStateException;Insufficient configured threads: required=297818893 < max=10 for QueuedThreadPool[qtp1913195136]@72090a80{STARTED,8<=8<=10,i=8,r=-1,q=0}[ReservedThreadExecutor@635791b7{s=0/1,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutKeyStorePassword;id_000000;[hadoop.http.selector.count=1556932044];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutKeyStorePassword;id_000001;[hadoop.http.selector.count=2147424776];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutKeyStorePassword;id_000002;[hadoop.http.selector.count=804590639];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutKeyStorePassword;id_000003;[hadoop.http.selector.count=410647774];java.lang.IllegalStateException;Insufficient configured threads: required=26060373 < max=10 for QueuedThreadPool[qtp1927671832]@72e5f018{STARTED,8<=8<=10,i=8,r=-1,q=0}[ReservedThreadExecutor@26e74d50{s=0/1,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutTrustStorePassword;id_000000;[hadoop.http.selector.count=1080195701];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutTrustStorePassword;id_000001;[hadoop.http.selector.count=1175506156];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutTrustStorePassword;id_000002;[hadoop.http.selector.count=109210714];java.lang.IllegalStateException;Insufficient configured threads: required=139988813 < max=10 for QueuedThreadPool[qtp1238784191]@49d658bf{STARTED,8<=8<=10,i=8,r=-1,q=0}[ReservedThreadExecutor@2a71d430{s=0/1,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testKeyStoreSetupWithoutTrustStorePassword;id_000003;[hadoop.http.selector.count=1661913733];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testListLocatedFileStatus;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testListLocatedFileStatus;id_000002;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemDelegation#testAclMethods;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.ha.TestZKFailoverController#testFormatZK;id_000000;[hadoop.security.credential.clear-text-fallback=false];org.apache.zookeeper.KeeperException$NoAuthException;KeeperErrorCode = NoAuth for /hadoop-ha/dummy-cluster;[org.apache.zookeeper.KeeperException.create(KeeperException.java:120), org.apache.zookeeper.KeeperException.create(KeeperException.java:54), org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1041), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1038)]
org.apache.hadoop.ha.TestZKFailoverController#testFormatZK;id_000002;[fs.creation.parallel.count=0];java.lang.IllegalArgumentException;Invalid ZK session timeout 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:192), org.apache.hadoop.ha.ZKFailoverController.initZK(ZKFailoverController.java:367), org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:200), org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:63), org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:181)]
org.apache.hadoop.security.ssl.TestSSLFactory#testServerCredProviderPasswords;id_000000;[file.stream-buffer-size=1510104431, file.bytes-per-checksum=561835111];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.ssl.TestSSLFactory#testServerCredProviderPasswords;id_000001;[file.bytes-per-checksum=847499044];java.lang.NegativeArraySizeException;-962443196;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.ssl.TestSSLFactory#testServerCredProviderPasswords;id_000002;[file.bytes-per-checksum=1540490682];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.ssl.TestSSLFactory#testServerCredProviderPasswords;id_000003;[file.bytes-per-checksum=709132647];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteOneBlock;id_000000;[io.file.buffer.size=1246911102];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.FileSystemContractBaseTest.writeAndRead(FileSystemContractBaseTest.java:937)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteOneBlock;id_000001;[io.file.buffer.size=2137313127];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteOneBlock;id_000002;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1082702446, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.service.shutdown.timeout=4191s, fs.permissions.umask-mode=600, hadoop.security.groups.negative-cache.secs=62965, fs.local.block.size=1444925696, fs.creation.parallel.count=466456089, fs.file.impl.disable.cache=true, io.file.buffer.size=262987776];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteOneBlock;id_000004;[io.file.buffer.size=2094889955];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileExceptionallyTranslating;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenamePopulatesFileAncestors;id_000000;[file.bytes-per-checksum=1315371105];java.lang.NegativeArraySizeException;-1046561943;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenamePopulatesFileAncestors;id_000001;[io.file.buffer.size=2074848292];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenamePopulatesFileAncestors;id_000003;[file.stream-buffer-size=1200029228, io.file.buffer.size=1028367396];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir#testCreateDirWithExistingDir;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyFile;id_000000;[io.file.buffer.size=1435912529];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyFile;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.ipc.TestIPC#testSocketLeak;id_000000;[ipc.server.read.threadpool.size=706291433];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.Server.<init>(Server.java:3046), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:226), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:219)]
org.apache.hadoop.ipc.TestIPC#testHttpGetResponse;id_000000;[ipc.server.read.threadpool.size=658250025];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.Server.<init>(Server.java:3046), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:226), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:219)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeLocalFileSystem#testLocalTargetLinkWriteSimple;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme.initialize(ViewFileSystemOverloadScheme.java:161), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeLocalFileSystem#testLocalFsCreateAndDelete;id_000000;[file.stream-buffer-size=1183865819, io.file.buffer.size=1995261747];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeLocalFileSystem#testLocalFsCreateAndDelete;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig#testLocalTargetLinkWriteSimple;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig#testLocalTargetLinkWriteSimple;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme.initialize(ViewFileSystemOverloadScheme.java:161), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]
org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeCentralMountTableConfig#testLocalTargetLinkWriteSimple;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.ipc.TestRPC#testConfRpc;id_000000;[ipc.server.read.threadpool.size=1457493775];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371)]
org.apache.hadoop.ipc.TestServer#testPurgeIntervalNanosConf;id_000000;[ipc.server.read.threadpool.size=1841562506];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.Server.<init>(Server.java:3046), org.apache.hadoop.ipc.TestServer$2.<init>(TestServer.java:204), org.apache.hadoop.ipc.TestServer.testPurgeIntervalNanosConf(TestServer.java:203)]
org.apache.hadoop.fs.viewfs.TestViewfsFileStatus#testListStatusACL;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)]
org.apache.hadoop.fs.viewfs.TestViewfsFileStatus#testListStatusACL;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateFileUnderFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateFileUnderFile;id_000001;[file.bytes-per-checksum=1606185329];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateFileUnderFile;id_000003;[file.bytes-per-checksum=255498116];java.lang.NegativeArraySizeException;-1995484252;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateFileUnderFile;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithMultiplePathMatchesAndNonTrivialFilter;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateFileOverExistingFileNoOverwrite;id_000000;[io.file.buffer.size=2101224717];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testZeroByteFilesAreFiles;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testZeroByteFilesAreFiles;id_000001;[io.file.buffer.size=2075810291];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testZeroByteFilesAreFiles;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.shell.TestTextCommand#testEmptyTextFil;id_000000;[io.file.buffer.size=2144688389];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.shell.TestTextCommand#testEmptyTextFil;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.shell.TestTextCommand#testEmptyTextFil;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsNonEmptyDirectory;id_000000;[file.stream-buffer-size=1317862106, file.bytes-per-checksum=164038513];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsNonEmptyDirectory;id_000002;[hadoop.security.dns.log-slow-lookups.threshold.ms=355429264, hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=633560613, hadoop.service.shutdown.timeout=896h, file.stream-buffer-size=1812895044, hadoop.security.groups.cache.background.reload.threads=244040392, fs.file.impl.disable.cache=true, hadoop.kerberos.min.seconds.before.relogin=1447745820, fs.permissions.umask-mode=007, hadoop.security.groups.negative-cache.secs=1021744521, fs.creation.parallel.count=1511001608, fs.automatic.close=true, hadoop.security.groups.cache.warn.after.ms=452125305];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsNonEmptyDirectory;id_000003;[file.bytes-per-checksum=805712668];java.lang.NegativeArraySizeException;-1338520580;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestLocalFileSystemPermission#testLocalFSsetPermission;id_000000;[file.bytes-per-checksum=1969064763];java.lang.IllegalArgumentException;null path;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:776), org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:105), org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:774), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)]
org.apache.hadoop.fs.TestLocalFileSystemPermission#testLocalFSsetPermission;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.util.TestGenericOptionsParser#testEmptyFilenames;id_000000;[file.bytes-per-checksum=2097374156];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.util.TestGenericOptionsParser#testEmptyFilenames;id_000001;[file.bytes-per-checksum=606361563, io.file.buffer.size=268052765];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.util.TestGenericOptionsParser#testEmptyFilenames;id_000002;[file.bytes-per-checksum=600603967, io.file.buffer.size=1205774328];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.util.TestGenericOptionsParser#testEmptyFilenames;id_000003;[file.bytes-per-checksum=1770803140];java.lang.NegativeArraySizeException;-1242640924;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.util.TestGenericOptionsParser#testEmptyFilenames;id_000005;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.util.TestGenericOptionsParser#testEmptyFilenames;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameWithNonEmptySubDir;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.service.shutdown.timeout=3m, fs.permissions.umask-mode=777, hadoop.security.groups.cache.background.reload.threads=250983106, fs.contract.rename-remove-dest-if-empty-dir=false, file.bytes-per-checksum=1515345550, io.file.buffer.size=675562154, fs.automatic.close=false];java.io.FileNotFoundException;File file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task17/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/1SDTJqI9Pn/testRenameWithNonEmptySubDir/dest/src1/source.txt does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.contract.ContractTestUtils.verifyPathExists(ContractTestUtils.java:961), org.apache.hadoop.fs.contract.ContractTestUtils.assertPathExists(ContractTestUtils.java:945)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameWithNonEmptySubDir;id_000002;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameWithNonEmptySubDir;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameWithNonEmptySubDir;id_000007;[io.file.buffer.size=2098831583];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriteMetaBlocksWithSameName;id_000000;[file.bytes-per-checksum=202318751];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriteMetaBlocksWithSameName;id_000001;[tfile.fs.output.buffer.size=2137626364];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriteMetaBlocksWithSameName;id_000002;[file.bytes-per-checksum=439423358];java.lang.NegativeArraySizeException;-340157074;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriteMetaBlocksWithSameName;id_000003;[hadoop.security.token.service.use_ip=false, fs.permissions.umask-mode=664, file.stream-buffer-size=1277001390, fs.local.block.size=278347861, fs.file.impl.disable.cache=true, io.file.buffer.size=91788825];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOpenRandomFile;id_000000;[tfile.fs.output.buffer.size=2091574553];java.lang.NegativeArraySizeException;-543561362;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOpenRandomFile;id_000001;[file.stream-buffer-size=1116339068];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareMetaBlock(BCFile.java:357), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareMetaBlock(BCFile.java:383), org.apache.hadoop.io.file.tfile.TFile$Writer.close(TFile.java:319)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureNegativeOffset;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1859812328, hadoop.security.groups.cache.secs=185118109, hadoop.security.groups.negative-cache.secs=352308783, hadoop.security.groups.cache.background.reload.threads=929378791, tfile.fs.output.buffer.size=1759159628];java.lang.NegativeArraySizeException;-1678903441;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyLongerThan64K;id_000000;[file.bytes-per-checksum=982738092, tfile.fs.output.buffer.size=1104505687];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.OutOfMemoryError(Java heap space),  java.lang.OutOfMemoryError(Java heap space);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyLongerThan64K;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=2035069297, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=014h, file.stream-buffer-size=314535156, hadoop.security.groups.cache.background.reload.threads=817540738, fs.local.block.size=513457398, hadoop.security.groups.cache.background.reload=true, fs.file.impl.disable.cache=true, io.file.buffer.size=1618668231, hadoop.security.token.service.use_ip=true, fs.permissions.umask-mode=027, hadoop.security.groups.negative-cache.secs=45131771, hadoop.security.groups.cache.warn.after.ms=1310388638];java.lang.NegativeArraySizeException;-580901417;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyLongerThan64K;id_000003;[file.bytes-per-checksum=1109821985];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureBadCompressionCodec;id_000000;[file.bytes-per-checksum=1089336804];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureBadCompressionCodec;id_000001;[file.bytes-per-checksum=891051078];java.lang.NegativeArraySizeException;-570474890;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureOpenRandomFile;id_000000;[file.bytes-per-checksum=834540580];java.lang.NegativeArraySizeException;-1079069372;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureOpenRandomFile;id_000001;[file.stream-buffer-size=1515368128];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureNegativeOffset;id_000000;[io.file.buffer.size=1297465135, tfile.fs.output.buffer.size=1784307432];java.lang.NegativeArraySizeException;-298791421;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureNegativeOffset;id_000001;[file.bytes-per-checksum=1043554459, io.file.buffer.size=1432023319];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureNegativeOffset;id_000002;[file.bytes-per-checksum=1585443328];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureNegativeOffset;id_000004;[hadoop.service.shutdown.timeout=89940s, fs.permissions.umask-mode=600, file.stream-buffer-size=1285114034, hadoop.security.groups.cache.background.reload.threads=1781810550, fs.local.block.size=1919631523, fs.creation.parallel.count=181759319, fs.file.impl.disable.cache=true, io.file.buffer.size=315323242];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureKeyLongerThan64K;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=470277565, hadoop.service.shutdown.timeout=23457478h, hadoop.security.groups.cache.background.reload.threads=1589875186, tfile.fs.output.buffer.size=2059676391, hadoop.security.groups.cache.background.reload=false, fs.automatic.close=true, hadoop.security.groups.cache.warn.after.ms=541930903];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureKeyLongerThan64K;id_000001;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1449620532, hadoop.service.shutdown.timeout=8903m, file.stream-buffer-size=684885774, hadoop.security.groups.cache.background.reload.threads=1589875105, fs.local.block.size=1628621219, fs.file.impl.disable.cache=true, io.file.buffer.size=620145583, hadoop.kerberos.min.seconds.before.relogin=124255943, hadoop.kerberos.keytab.login.autorenewal.enabled=false, fs.permissions.umask-mode=077, hadoop.security.groups.negative-cache.secs=1033343077, fs.creation.parallel.count=229762115, hadoop.security.groups.cache.warn.after.ms=155982231];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:431), org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642), org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureKeyLongerThan64K;id_000002;[file.bytes-per-checksum=1845416740];java.lang.NegativeArraySizeException;-571118524;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureKeyLongerThan64K;id_000003;[file.bytes-per-checksum=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureKeyLongerThan64K;id_000013;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureKeyLongerThan64K;id_000014;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureCompressionNotWorking;id_000000;[tfile.fs.output.buffer.size=1195747715, io.file.buffer.size=1500109968];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureCompressionNotWorking;id_000001;[file.bytes-per-checksum=282598624];java.lang.NegativeArraySizeException;-1751579680;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureCompressionNotWorking;id_000003;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureCompressionNotWorking;id_000005;[file.stream-buffer-size=564545818, io.file.buffer.size=1500109968];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureValueTooLong;id_000000;[io.file.buffer.size=2070961352];java.lang.NegativeArraySizeException;-1548150583;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureValueTooLong;id_000001;[file.bytes-per-checksum=1578867007];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusIteratorFile;id_000000;[io.file.buffer.size=2120061499];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusIteratorFile;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusIteratorFile;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureAddKeyWithoutValue;id_000000;[file.bytes-per-checksum=1964704635];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureAddKeyWithoutValue;id_000001;[file.bytes-per-checksum=727158556];java.lang.NegativeArraySizeException;-2045507588;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureAddKeyWithoutValue;id_000003;[io.file.buffer.size=696796544, tfile.fs.output.buffer.size=1183217888, file.bytes-per-checksum=38415339];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureAddKeyWithoutValue;id_000005;[file.bytes-per-checksum=1174923044];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekFile;id_000000;[file.bytes-per-checksum=779734257];java.lang.NegativeArraySizeException;-1572326279;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekFile;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekFile;id_000003;[file.bytes-per-checksum=1070692296];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestPathData#testToFile;id_000000;[file.bytes-per-checksum=965107818, io.file.buffer.size=1475257256];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestPathData#testToFile;id_000001;[file.bytes-per-checksum=1211228338];java.lang.NegativeArraySizeException;-1983846846;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testToFile;id_000002;[file.bytes-per-checksum=704015623];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testToFile;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testToFile;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameFileToNonExistentDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testWriteReadAndDeleteTwoBlocks;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusFilterWithEmptyPathResults;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameFileToItself;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testCreateFlagAppendCreateOverwrite;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testWorkingDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testDeleteRecursively;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGetFileContext1;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testOpenFileApplyRead;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testMkdirRecursiveWithNonExistingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testCreateNonRecursiveWithExistingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testCwdContents;id_000000;[fs.file.impl.disable.cache=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testCwdContents;id_000001;[file.bytes-per-checksum=322354468];java.lang.NegativeArraySizeException;-1393777084;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testCwdContents;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testCwdContents;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testWithStringAndConfForBuggyPath;id_000000;[fs.file.impl.disable.cache=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestPathData#testWithStringAndConfForBuggyPath;id_000001;[file.bytes-per-checksum=451367479];java.lang.NegativeArraySizeException;-232659985;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testWithStringAndConfForBuggyPath;id_000002;[file.bytes-per-checksum=677526069];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testWithStringAndConfForBuggyPath;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testWithStringAndConfForBuggyPath;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlobBack;id_000000;[file.bytes-per-checksum=363682705];java.lang.NegativeArraySizeException;-1021822951;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlobBack;id_000001;[fs.file.impl.disable.cache=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlobBack;id_000002;[file.bytes-per-checksum=1572024443];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlobBack;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testRelativeGlobBack;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusFilterWithNoMatchingPathsAndNonTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testCreateFlagCreateAppendExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testCreateFlagOverwriteExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testCreateFlagCreateExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameDirectoryAsFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testBuilderCreateAppendNonExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testWriteReadAndDeleteOneBlock;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testListCorruptFileBlocks;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureGetNonExistentMetaBlock;id_000000;[file.bytes-per-checksum=1354810260];java.lang.NegativeArraySizeException;-691609548;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureGetNonExistentMetaBlock;id_000001;[file.bytes-per-checksum=608160608];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureBadCompressionCodec;id_000000;[file.bytes-per-checksum=2048308740];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureBadCompressionCodec;id_000001;[file.bytes-per-checksum=1381180495];java.lang.NegativeArraySizeException;-454277433;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureBadCompressionCodec;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusEmptyDirectory;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testSeekZeroByteFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testBuilderAppendToExistingFile;id_000000;[io.file.buffer.size=1981273766];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.ContractTestUtils.readDataset(ContractTestUtils.java:214)]
org.apache.hadoop.io.compress.TestCodec#testCodecPoolCompressorReinit;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:42), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.TestCodec.gzipReinitTest(TestCodec.java:411), org.apache.hadoop.io.compress.TestCodec.testCodecPoolCompressorReinit(TestCodec.java:492), org.apache.hadoop.io.compress.TestCodec.testCodecPoolCompressorReinit$$CONFUZZ(TestCodec.java)]
org.apache.hadoop.io.compress.TestCodec#testCodecPoolCompressorReinit;id_000003;[io.file.buffer.size=2118591746];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.TestCodec.gzipReinitTest(TestCodec.java:411), org.apache.hadoop.io.compress.TestCodec.testCodecPoolCompressorReinit(TestCodec.java:492), org.apache.hadoop.io.compress.TestCodec.testCodecPoolCompressorReinit$$CONFUZZ(TestCodec.java)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFilteredFile;id_000000;[io.file.buffer.size=1795039376, file.bytes-per-checksum=979848757];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFilteredFile;id_000001;[file.stream-buffer-size=740817570, io.file.buffer.size=1795047707];java.lang.NegativeArraySizeException;-1590869353;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFilteredFile;id_000002;[hadoop.security.token.service.use_ip=true, hadoop.service.shutdown.timeout=5471612h, fs.permissions.umask-mode=027, file.stream-buffer-size=267881731, hadoop.security.groups.cache.background.reload.threads=2016726533, fs.local.block.size=518154053, fs.creation.parallel.count=1365017193, fs.file.impl.disable.cache=true, file.bytes-per-checksum=483547391, io.file.buffer.size=1329598660, hadoop.security.groups.cache.warn.after.ms=252860968];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFilteredFile;id_000003;[file.bytes-per-checksum=2045209916];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testNegativeSeek;id_000000;[file.bytes-per-checksum=1426992339];java.lang.NegativeArraySizeException;-41970837;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testNegativeSeek;id_000001;[file.bytes-per-checksum=1438175176];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.AbstractContractSeekTest.testNegativeSeek(AbstractContractSeekTest.java:181)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testNegativeSeek;id_000002;[file.bytes-per-checksum=2010678988];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testNegativeSeek;id_000003;[file.bytes-per-checksum=2029853368];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalCreateMissingDir;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1904305865, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/nDEQPY3UKi/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/nDEQPY3UKi, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/nDEQPY3UKi/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/nDEQPY3UKi/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/nDEQPY3UKi/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1064442014, fs.creation.parallel.count=1088733159, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=932380399, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/nDEQPY3UKi/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1070586666, fs.local.block.size=1720766057, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=784875574, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/nDEQPY3UKi/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=007, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/nDEQPY3UKi/user, hadoop.security.groups.cache.warn.after.ms=898311747];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path viewfs://mycluster/missingDir/foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:352), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testgetFSonDanglingLink;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=569908781, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/qdt2ZJlyFa/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/qdt2ZJlyFa, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/qdt2ZJlyFa/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/qdt2ZJlyFa/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/qdt2ZJlyFa/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=849472467, fs.creation.parallel.count=1200485632, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1283863006, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/qdt2ZJlyFa/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=2067813555, fs.local.block.size=215871804, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=777845944, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/qdt2ZJlyFa/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=002, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/qdt2ZJlyFa/user, hadoop.security.groups.cache.warn.after.ms=636190756];java.io.FileNotFoundException;File /mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vuzlMZVpT1/missingTarget does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128), org.apache.hadoop.fs.FilterFs.getFileStatus(FilterFs.java:124)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalRenameToSlash;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1290515710, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vV7JeDIqKi/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vV7JeDIqKi, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vV7JeDIqKi/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vV7JeDIqKi/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vV7JeDIqKi/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=1835401099, fs.creation.parallel.count=1282039203, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=2070535181, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vV7JeDIqKi/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1528083970, fs.local.block.size=800817211, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=844168381, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vV7JeDIqKi/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=755, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vV7JeDIqKi/user, hadoop.security.groups.cache.warn.after.ms=785782821];org.apache.hadoop.security.AccessControlException;Cannot Rename within internal dirs of mount table: dest=/ is readOnly;[org.apache.hadoop.fs.viewfs.ViewFs.renameInternal(ViewFs.java:601), org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720), org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalRenameToSlash(ViewFsBaseTest.java:786), org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs.testInternalRenameToSlash$$CONFUZZ(TestViewFsWithAuthorityLocalFs.java)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testResolvePathDanglingLink;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1657736462, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/XNHzB2ghsx/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/XNHzB2ghsx, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/XNHzB2ghsx/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/XNHzB2ghsx/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/XNHzB2ghsx/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1617297140, fs.creation.parallel.count=666608640, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=701091224, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/XNHzB2ghsx/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=869516465, fs.local.block.size=1952868459, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=2118986690, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/XNHzB2ghsx/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=750, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/XNHzB2ghsx/user, hadoop.security.groups.cache.warn.after.ms=780544626];java.io.FileNotFoundException;File /mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/ztOASZoKcL/missingTarget does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128), org.apache.hadoop.fs.FilterFs.getFileStatus(FilterFs.java:124)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalCreateMissingDir;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1043682805, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/WUmFC2ihq7/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/WUmFC2ihq7, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/WUmFC2ihq7/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/WUmFC2ihq7/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/WUmFC2ihq7/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=1999708493, fs.creation.parallel.count=471619158, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1774581291, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/WUmFC2ihq7/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1539804551, fs.local.block.size=1735584008, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1856163442, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/WUmFC2ihq7/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=002, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/WUmFC2ihq7/user, hadoop.security.groups.cache.warn.after.ms=435177255];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path viewfs:/missingDir/foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:352), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalDeleteExisting2;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=2003333207, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wQVRYWEGRk/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wQVRYWEGRk, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wQVRYWEGRk/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wQVRYWEGRk/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wQVRYWEGRk/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=1868733397, fs.creation.parallel.count=636466448, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1852378500, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wQVRYWEGRk/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1275165912, fs.local.block.size=1958018923, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=806283558, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wQVRYWEGRk/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=277, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wQVRYWEGRk/user, hadoop.security.groups.cache.warn.after.ms=1931920271];org.apache.hadoop.security.AccessControlException;Cannot delete internal mount table directory: /internalDir/linkToDir2;[org.apache.hadoop.fs.viewfs.ViewFs.delete(ViewFs.java:372), org.apache.hadoop.fs.FileContext$5.next(FileContext.java:845), org.apache.hadoop.fs.FileContext$5.next(FileContext.java:841), org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90), org.apache.hadoop.fs.FileContext.delete(FileContext.java:847)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalCreate2;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1652270417, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/eymLd8pKiN/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/eymLd8pKiN, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/eymLd8pKiN/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/eymLd8pKiN/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/eymLd8pKiN/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=721360064, fs.creation.parallel.count=1960300941, fs.viewfs.rename.strategy=SAME_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1661839096, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/eymLd8pKiN/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1132799480, fs.local.block.size=115819906, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1886754657, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/eymLd8pKiN/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=400, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task13/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/eymLd8pKiN/user, hadoop.security.groups.cache.warn.after.ms=1810398821];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path /foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.createInternal(ViewFs.java:1028), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:358), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalCreateMissingDir2;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1411574810, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/MLru9NqNK4/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/MLru9NqNK4, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/MLru9NqNK4/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/MLru9NqNK4/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/MLru9NqNK4/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=1462630481, fs.creation.parallel.count=1706635833, fs.viewfs.rename.strategy=SAME_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=386658849, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/MLru9NqNK4/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1212211888, fs.local.block.size=1626731266, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=171386713, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/MLru9NqNK4/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=660, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/MLru9NqNK4/user, hadoop.security.groups.cache.warn.after.ms=9610087];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path viewfs://mycluster/missingDir/miss2/foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:352), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testResolvePathMissingThroughMountPoints2;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1590951694, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bm7T1ggNOC/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bm7T1ggNOC, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bm7T1ggNOC/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bm7T1ggNOC/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bm7T1ggNOC/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=1214615468, fs.creation.parallel.count=2081451545, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1249109454, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bm7T1ggNOC/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=924074962, fs.local.block.size=382188118, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1375835529, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bm7T1ggNOC/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=666, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bm7T1ggNOC/user, hadoop.security.groups.cache.warn.after.ms=168173233];java.io.FileNotFoundException;File /mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/HYy7icim8H/user/dirX/nonExisting does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128), org.apache.hadoop.fs.FilterFs.getFileStatus(FilterFs.java:124)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalCreateMissingDir3;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1869628777, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/imJE2gFIbg/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/imJE2gFIbg, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/imJE2gFIbg/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/imJE2gFIbg/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/imJE2gFIbg/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=858431819, fs.creation.parallel.count=1212356466, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1164119993, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/imJE2gFIbg/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1407306802, fs.local.block.size=364705884, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=949442640, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/imJE2gFIbg/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=277, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/imJE2gFIbg/user, hadoop.security.groups.cache.warn.after.ms=1644383743];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path viewfs://mycluster/internalDir/miss2/foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:352), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testResolvePathMissingThroughMountPoints;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=269035650, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/xtr26MXYN4/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/xtr26MXYN4, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/xtr26MXYN4/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/xtr26MXYN4/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/xtr26MXYN4/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=2066663251, fs.creation.parallel.count=354399479, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=633407681, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/xtr26MXYN4/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=193050479, fs.local.block.size=1374796824, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=2011823342, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/xtr26MXYN4/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=755, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/xtr26MXYN4/user, hadoop.security.groups.cache.warn.after.ms=1295799881];java.io.FileNotFoundException;File /mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/vXETcdlSe4/user/nonExisting does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128), org.apache.hadoop.fs.FilterFs.getFileStatus(FilterFs.java:124)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testResolvePathMissingThroughMountPoints2;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=2108804370, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sFTwM8WReI/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sFTwM8WReI, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sFTwM8WReI/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sFTwM8WReI/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sFTwM8WReI/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1312596449, fs.creation.parallel.count=2070260326, fs.viewfs.rename.strategy=SAME_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1754867536, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sFTwM8WReI/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1599171914, fs.local.block.size=577880010, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1897148217, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sFTwM8WReI/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=664, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sFTwM8WReI/user, hadoop.security.groups.cache.warn.after.ms=185796296];java.io.FileNotFoundException;File /mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/0LQ8AeK5XO/user/dirX/nonExisting does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128), org.apache.hadoop.fs.FilterFs.getFileStatus(FilterFs.java:124)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalRenameToSlash;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1844021690, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sGC7YutOZo/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sGC7YutOZo, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sGC7YutOZo/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sGC7YutOZo/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sGC7YutOZo/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=713922912, fs.creation.parallel.count=1148550505, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1547396774, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sGC7YutOZo/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=2080068379, fs.local.block.size=1326076081, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1927772272, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sGC7YutOZo/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=027, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/sGC7YutOZo/user, hadoop.security.groups.cache.warn.after.ms=2102418210];org.apache.hadoop.security.AccessControlException;Cannot Rename within internal dirs of mount table: dest=/ is readOnly;[org.apache.hadoop.fs.viewfs.ViewFs.renameInternal(ViewFs.java:601), org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720), org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalRenameToSlash(ViewFsBaseTest.java:786), org.apache.hadoop.fs.viewfs.TestViewFsLocalFs.testInternalRenameToSlash$$CONFUZZ(TestViewFsLocalFs.java)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalMkdirExisting1;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1038541603, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/zobbuDyCWI/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/zobbuDyCWI, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/zobbuDyCWI/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/zobbuDyCWI/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/zobbuDyCWI/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=890623423, fs.creation.parallel.count=146035430, fs.viewfs.rename.strategy=SAME_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=886536427, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/zobbuDyCWI/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1453836465, fs.local.block.size=1333562116, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=70099250, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/zobbuDyCWI/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=640, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task30/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/zobbuDyCWI/user, hadoop.security.groups.cache.warn.after.ms=385590228];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /internalDir.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteEmptyDirNonRecursive;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusWithMultipleWildCardMatches;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteNonEmptyDirNonRecursive;id_000000;[fs.defaultFS=file:///, hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=76816876, fs.client.resolve.remote.symlinks=true, hadoop.service.shutdown.timeout=880m, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=703473569, fs.creation.parallel.count=840497468, file.bytes-per-checksum=1905713208, fs.automatic.close=true, hadoop.security.dns.log-slow-lookups.threshold.ms=226075628, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=792008670, fs.local.block.size=999690715, fs.file.impl.disable.cache=true, io.file.buffer.size=2023362686, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=333425948, fs.permissions.umask-mode=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.security.groups.cache.warn.after.ms=1764308730];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteNonEmptyDirNonRecursive;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteNonEmptyDirNonRecursive;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.security.token.TestDtUtilShell#testGetWithAliasFlag;id_000000;[file.bytes-per-checksum=2032151906];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.token.TestDtUtilShell#testGetWithAliasFlag;id_000001;[file.bytes-per-checksum=710986492];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.token.TestDtUtilShell#testGetWithAliasFlag;id_000002;[file.bytes-per-checksum=633562925];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.security.token.TestDtUtilShell#testFormatJavaFlag;id_000000;[io.file.buffer.size=1764879863, file.bytes-per-checksum=36708469];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.token.TestDtUtilShell#testFormatJavaFlag;id_000002;[file.bytes-per-checksum=1508563228];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testDeleteSnapshot;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testGetAllStoragePolicy;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testSetStoragePolicy;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testRenameSnapshot;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testUnsetStoragePolicy;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testAclMethodsPathTranslation;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.ha.TestZKFailoverController#testFormatOneClusterLeavesOtherClustersAlone;id_000000;[hadoop.security.credential.clear-text-fallback=false];org.apache.zookeeper.KeeperException$NoAuthException;KeeperErrorCode = NoAuth for /shuai/dummy-cluster;[org.apache.zookeeper.KeeperException.create(KeeperException.java:120), org.apache.zookeeper.KeeperException.create(KeeperException.java:54), org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1041), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1038)]
org.apache.hadoop.ha.TestZKFailoverController#testFormatOneClusterLeavesOtherClustersAlone;id_000002;[fs.creation.parallel.count=0];java.lang.IllegalArgumentException;Invalid ZK session timeout 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:192), org.apache.hadoop.ha.ZKFailoverController.initZK(ZKFailoverController.java:367), org.apache.hadoop.ha.ZKFailoverController.doRun(ZKFailoverController.java:200), org.apache.hadoop.ha.ZKFailoverController.access$000(ZKFailoverController.java:63), org.apache.hadoop.ha.ZKFailoverController$1.run(ZKFailoverController.java:181)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalMkdirNew;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=2105908609, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task113/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/3G0LLgBAvk/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task113/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/3G0LLgBAvk, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task113/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/3G0LLgBAvk/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task113/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/3G0LLgBAvk/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task113/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/3G0LLgBAvk/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1688590125, fs.creation.parallel.count=1877564304, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1075722048, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task113/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/3G0LLgBAvk/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1182875440, fs.local.block.size=444273402, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=418158030, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task113/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/3G0LLgBAvk/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=770, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task113/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/3G0LLgBAvk/user, hadoop.security.groups.cache.warn.after.ms=1365812879];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /dirNew.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusWithMultipleMatchesOfSingleChar;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameFileToExistingParent;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameDirectoryToItself;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameFileAsExistingDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testRenameDirectoryAsNonExistentDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcCreateMkdirLocalFs#testCreateRecursiveWithNonExistingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.RuntimeException(java.io.IOException: ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testTwoDataEntries;id_000000;[io.file.buffer.size=1066575029, tfile.fs.output.buffer.size=1541907398];java.lang.NegativeArraySizeException;-602232254;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testTwoDataEntries;id_000002;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testTwoDataEntries;id_000005;[file.bytes-per-checksum=1550112485];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testTwoDataEntries;id_000023;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testOneBlock;id_000000;[file.bytes-per-checksum=429956399];java.lang.NegativeArraySizeException;-425359705;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testOneBlock;id_000004;[file.bytes-per-checksum=1989492948];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testLocate;id_000000;[io.file.buffer.size=1886656396, tfile.fs.output.buffer.size=1551322484];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testLocate;id_000001;[io.file.buffer.size=1050626022, tfile.fs.output.buffer.size=1376046532];java.lang.NegativeArraySizeException;-945913018;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testLocate;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=1509404709, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=999641774, hadoop.service.shutdown.timeout=55833m, file.stream-buffer-size=79668398, hadoop.security.groups.cache.background.reload.threads=1960761959, fs.local.block.size=96139604, fs.file.impl.disable.cache=true, io.file.buffer.size=995005011, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1226110502, fs.permissions.umask-mode=500, hadoop.security.groups.negative-cache.secs=2139374203, fs.creation.parallel.count=384305486, file.bytes-per-checksum=61997702, fs.automatic.close=true, hadoop.security.groups.cache.warn.after.ms=2139757249];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:431), org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642), org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testOneBlock;id_000000;[file.bytes-per-checksum=387730691];java.lang.NegativeArraySizeException;-805391077;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testOneBlock;id_000001;[file.bytes-per-checksum=1638350206];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testOneBlock;id_000003;[file.bytes-per-checksum=1636094817];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testNoEntry;id_000000;[io.file.buffer.size=1772705760, tfile.fs.output.buffer.size=1355654354];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testNoEntry;id_000001;[file.bytes-per-checksum=272522226];java.lang.NegativeArraySizeException;-1842267262;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testScan;id_000000;[io.file.buffer.size=2138922329];java.lang.NegativeArraySizeException;-497799498;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileUnsortedByteArrays#testScan;id_000001;[io.file.buffer.size=1750721038, tfile.fs.output.buffer.size=1755125862];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testTwoEntriesKnownLength;id_000000;[io.file.buffer.size=1088554137, tfile.fs.output.buffer.size=1169033555];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testTwoEntriesKnownLength;id_000001;[file.bytes-per-checksum=265810070];java.lang.NegativeArraySizeException;-1902676666;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testTwoEntriesKnownLength;id_000002;[file.bytes-per-checksum=1018371243];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testTwoEntriesKnownLength;id_000003;[io.file.buffer.size=212077880, tfile.fs.output.buffer.size=1930404020];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.OutOfMemoryError(Java heap space),  java.lang.OutOfMemoryError(Java heap space);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testTwoEntriesKnownLength;id_000004;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testTwoEntriesKnownLength;id_000006;[file.bytes-per-checksum=1152606064];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryKnownLength;id_000000;[file.bytes-per-checksum=2082851562];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryKnownLength;id_000001;[file.bytes-per-checksum=1444516872];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryKnownLength;id_000002;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryKnownLength;id_000005;[file.stream-buffer-size=1376452852, io.file.buffer.size=1616770307];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testOneEntryKnownLength;id_000006;[file.bytes-per-checksum=405129962];java.lang.NegativeArraySizeException;-648797638;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCompressionNotWorking;id_000000;[io.file.buffer.size=288673483, tfile.fs.output.buffer.size=1904356195];java.lang.NegativeArraySizeException;-1959338914;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCompressionNotWorking;id_000001;[file.bytes-per-checksum=710440616];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureCompressionNotWorking;id_000003;[fs.client.resolve.remote.symlinks=true, fs.permissions.umask-mode=750, file.stream-buffer-size=187312983, hadoop.security.groups.cache.background.reload.threads=789155192, fs.creation.parallel.count=592186586, file.bytes-per-checksum=551110451, io.file.buffer.size=635069204, hadoop.security.groups.cache.warn.after.ms=73574469];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.OutOfMemoryError(Java heap space),  java.lang.OutOfMemoryError(Java heap space);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.compress.TestCodec#testGzipCompatibility;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:60), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.TestCodec.testGzipCompatibility(TestCodec.java:693), org.apache.hadoop.io.compress.TestCodec.testGzipCompatibility$$CONFUZZ(TestCodec.java)]
org.apache.hadoop.io.compress.TestCodec#testGzipCompatibility;id_000002;[io.file.buffer.size=2058871348];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.TestCodec.testGzipCompatibility(TestCodec.java:693), org.apache.hadoop.io.compress.TestCodec.testGzipCompatibility$$CONFUZZ(TestCodec.java)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testNoDataEntry;id_000000;[file.bytes-per-checksum=1926790420];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testNoDataEntry;id_000001;[file.bytes-per-checksum=896993283];java.lang.NegativeArraySizeException;-516995045;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testNoDataEntry;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testNoDataEntry;id_000005;[file.bytes-per-checksum=1522578309];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateNewFile;id_000000;[file.stream-buffer-size=1830099900, io.file.buffer.size=557250448];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateNewFile;id_000001;[io.file.buffer.size=1282976987, file.bytes-per-checksum=561992461];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateNewFile;id_000002;[file.bytes-per-checksum=1384076117];java.lang.NegativeArraySizeException;-428216835;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.createNonRecursive(ChecksumFileSystem.java:565), org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder.build(FileSystem.java:4523)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenWhenBothExist;id_000000;[hadoop.ssl.require.client.cert=true];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:391), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenWhenBothExist(TestKMSClientProvider.java:106)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenWhenBothExist;id_000002;[hadoop.security.kms.client.encrypted.key.cache.size=0];java.lang.IllegalArgumentException;"numValues" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:224), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenWhenBothExist(TestKMSClientProvider.java:106)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenWhenBothExist;id_000003;[hadoop.security.kms.client.encrypted.key.cache.expiry=0];java.lang.IllegalArgumentException;"expiry" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:230), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenWhenBothExist(TestKMSClientProvider.java:106)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenWhenBothExist;id_000004;[hadoop.security.kms.client.encrypted.key.cache.low-watermark=0.0f];java.lang.IllegalArgumentException;"lowWatermark" must be > 0 and <= 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:225), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenWhenBothExist(TestKMSClientProvider.java:106)]
org.apache.hadoop.crypto.key.kms.TestKMSClientProvider#testSelectTokenWhenBothExist;id_000005;[hadoop.security.kms.client.encrypted.key.cache.num.refill.threads=0];java.lang.IllegalArgumentException;"numFillerThreads" must be > 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:231), org.apache.hadoop.crypto.key.kms.ValueQueue.<init>(ValueQueue.java:263), org.apache.hadoop.crypto.key.kms.KMSClientProvider.<init>(KMSClientProvider.java:417), org.apache.hadoop.crypto.key.kms.TestKMSClientProvider.testSelectTokenWhenBothExist(TestKMSClientProvider.java:106)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureWriterNotClosed;id_000000;[file.bytes-per-checksum=184805555];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureWriterNotClosed;id_000001;[file.bytes-per-checksum=721405486];java.lang.NegativeArraySizeException;-2097285218;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureWriterNotClosed;id_000002;[io.file.buffer.size=1198651030];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureWriterNotClosed;id_000003;[file.bytes-per-checksum=604061939];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOverwrite;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOverwrite;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOverwrite;id_000005;[io.file.buffer.size=2119064934];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameNonexistentFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyFileOverwrite;id_000000;[io.file.buffer.size=1326103201];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyFileOverwrite;id_000001;[io.file.buffer.size=1022207820];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:419), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:391)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyFileOverwrite;id_000002;[io.file.buffer.size=2013275033];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyFileOverwrite;id_000003;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyFileOverwrite;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testOverwrite;id_000000;[file.stream-buffer-size=1433953568, file.bytes-per-checksum=111535493];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testOverwrite;id_000001;[file.bytes-per-checksum=2142914956];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testOverwrite;id_000002;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=495944786, hadoop.service.shutdown.timeout=0948m, fs.permissions.umask-mode=277, file.stream-buffer-size=1433116470, fs.local.block.size=278661393, fs.creation.parallel.count=1542985496, fs.file.impl.disable.cache=true];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testOverwrite;id_000003;[file.bytes-per-checksum=1815778023];java.lang.NegativeArraySizeException;-837866977;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testOverwrite;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testDeleteRecursively;id_000000;[file.bytes-per-checksum=1880433371];java.lang.NegativeArraySizeException;-255968845;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testDeleteRecursively;id_000001;[file.bytes-per-checksum=1660304437];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testDeleteRecursively;id_000002;[file.stream-buffer-size=2026423326, file.bytes-per-checksum=548799293];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testDeleteRecursively;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=1294587559, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.kerberos.min.seconds.before.relogin=1540281151, hadoop.security.groups.cache.secs=1654794219, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.service.shutdown.timeout=966700m, fs.permissions.umask-mode=660, file.stream-buffer-size=889141352, hadoop.security.groups.negative-cache.secs=2014837894, fs.local.block.size=546223150];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testDeleteRecursively;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testDeleteOnExitPathHandling;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.ExceptionInInitializerError(null),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testMkdirUnderFileSubdir;id_000000;[io.file.buffer.size=2110771965];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFileSystemPermission#testSetUmaskInRealTime;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithNoMatchingPathsAndNonTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testDeleteEmptyDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithMultiplePathMatchesAndNonTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusWithMultipleWildCardMatches;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryToNonExistentParent;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testWDAbsolute;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusWithNoMatchesInPath;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithMultiplePathWildcardsAndNonTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithMultipleWildCardMatchesAndTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testListStatus;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithSomePathMatchesAndTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.TestArrayFile#testArrayFile;id_000000;[io.file.buffer.size=1985380491];java.lang.NegativeArraySizeException;-267359079;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestArrayFile#testArrayFile;id_000001;[io.file.buffer.size=1551079238];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.TestArrayFile#testArrayFile;id_000003;[io.file.buffer.size=1424127076];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.crypto.key.TestKeyProviderFactory#testJksProvider;id_000000;[hadoop.security.key.default.bitlength=278541450];java.io.IOException;Wrong key length. Required 278541450, but got 128;[org.apache.hadoop.crypto.key.JavaKeyStoreProvider.createKey(JavaKeyStoreProvider.java:452), org.apache.hadoop.crypto.key.TestKeyProviderFactory.checkSpecificProvider(TestKeyProviderFactory.java:120), org.apache.hadoop.crypto.key.TestKeyProviderFactory.testJksProvider(TestKeyProviderFactory.java:224), org.apache.hadoop.crypto.key.TestKeyProviderFactory.testJksProvider$$CONFUZZ(TestKeyProviderFactory.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.fs.sftp.TestSFTPFileSystem#testRenameNonExistFile;id_000000;[fs.defaultFS=file:///, hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1334449763, fs.client.resolve.remote.symlinks=true, hadoop.service.shutdown.timeout=862134h, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.sftp.password.localhost.user=password, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.sftp.user.localhost=user, fs.sftp.impl.disable.cache=true, fs.sftp.host=localhost, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=950713192, fs.creation.parallel.count=1871485353, file.bytes-per-checksum=1672921315, fs.automatic.close=true, hadoop.security.dns.log-slow-lookups.threshold.ms=716434128, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1982967128, fs.local.block.size=1082735518, fs.sftp.impl=org.apache.hadoop.fs.sftp.SFTPFileSystem, fs.sftp.connection.max=5, fs.sftp.host.port=46263, fs.file.impl.disable.cache=false, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1979116481, fs.permissions.umask-mode=277, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.security.groups.cache.warn.after.ms=614639038];java.io.IOException;Source path file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task56/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/testsftp/testrenamenonexistfile$$confuzz1 does not exist;[org.apache.hadoop.fs.sftp.SFTPFileSystem.rename(SFTPFileSystem.java:476), org.apache.hadoop.fs.sftp.SFTPFileSystem.rename(SFTPFileSystem.java:607), org.apache.hadoop.fs.sftp.TestSFTPFileSystem.testRenameNonExistFile(TestSFTPFileSystem.java:324), org.apache.hadoop.fs.sftp.TestSFTPFileSystem.testRenameNonExistFile$$CONFUZZ(TestSFTPFileSystem.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testAwaitFutureTimeoutFailToFNFE;id_000000;[hadoop.service.shutdown.timeout=5969769m, fs.permissions.umask-mode=027, hadoop.security.groups.cache.background.reload.threads=13326416, fs.test.something=false, fs.creation.parallel.count=1116784281, fs.file.impl.disable.cache=true];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testMkdirDelete;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testGetContentSummary;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOutputStreamClosedTwice;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;Invalid value of fs.creation.parallel.count: 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:415), org.apache.hadoop.fs.FileSystem$Cache.<init>(FileSystem.java:3514), org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:206), org.apache.hadoop.fs.TestRawLocalFileSystemContract.setUp(TestRawLocalFileSystemContract.java:66), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOutputStreamClosedTwice;id_000001;[io.file.buffer.size=0];java.lang.NoClassDefFoundError;Could not initialize class org.apache.hadoop.fs.FileSystem;[org.apache.hadoop.fs.TestRawLocalFileSystemContract.setUp(TestRawLocalFileSystemContract.java:66), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.lang.reflect.Method.invoke(Method.java:566)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testDestinationFileIsToParentDirectory;id_000000;[io.file.buffer.size=1277959479];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testDestinationFileIsToParentDirectory;id_000002;[io.file.buffer.size=1277959479];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsDirectoryAndDestinationIsFile;id_000000;[io.file.buffer.size=1966099294];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsDirectoryAndDestinationIsFile;id_000002;[io.file.buffer.size=1102185019];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsDirectoryAndDestinationIsFile;id_000003;[io.file.buffer.size=1748054792];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSrcIsEmptyDirWithCopySuccessful;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureNegativeOffset;id_000000;[io.file.buffer.size=1546702979, file.bytes-per-checksum=1007412863];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureNegativeOffset;id_000001;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureNegativeOffset;id_000005;[file.bytes-per-checksum=733012965];java.lang.NegativeArraySizeException;-1992817907;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureNegativeLength;id_000000;[file.bytes-per-checksum=1255970973];java.lang.NegativeArraySizeException;-1581163131;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureNegativeLength;id_000003;[file.bytes-per-checksum=1084008745];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureNegativeLength;id_000007;[file.stream-buffer-size=1631007601, io.file.buffer.size=1132479339];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyTooShort;id_000000;[tfile.fs.output.buffer.size=1838424920, file.bytes-per-checksum=557554460];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyTooShort;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=2012456976, hadoop.security.groups.cache.secs=99842502, file.stream-buffer-size=941526291, hadoop.security.groups.cache.background.reload.threads=1082059393, fs.local.block.size=1014376040, fs.file.impl.disable.cache=true, io.file.buffer.size=464372059, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=2066738845, hadoop.kerberos.keytab.login.autorenewal.enabled=true, fs.permissions.umask-mode=666, hadoop.security.groups.negative-cache.secs=1033086661, fs.creation.parallel.count=1721393003, hadoop.security.groups.cache.warn.after.ms=1036073289];java.lang.NegativeArraySizeException;-861107196;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyTooShort;id_000003;[file.bytes-per-checksum=1156155208];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureKeyTooShort;id_000022;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureValueTooShort;id_000000;[tfile.fs.output.buffer.size=2077730340];java.lang.NegativeArraySizeException;-1529052605;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureValueTooShort;id_000004;[file.bytes-per-checksum=1130769131];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureValueTooShort;id_000020;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testSeekFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testReadFullyPastEOF;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullySmallFile;id_000000;[file.bytes-per-checksum=628695680];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullySmallFile;id_000001;[file.bytes-per-checksum=1781571644];java.lang.NegativeArraySizeException;-1145724388;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullySmallFile;id_000004;[file.bytes-per-checksum=672451038];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyPastEOFZeroByteFile;id_000000;[file.bytes-per-checksum=2016239449];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyPastEOFZeroByteFile;id_000001;[file.bytes-per-checksum=1714652071];java.lang.NegativeArraySizeException;-1748000545;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadFullyPastEOFZeroByteFile;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=1649166437, hadoop.security.groups.cache.secs=536449476, hadoop.service.shutdown.timeout=43861m, file.stream-buffer-size=1375236888, hadoop.security.groups.cache.background.reload.threads=2142062137, fs.local.block.size=1779495624, hadoop.security.groups.cache.background.reload=false, fs.file.impl.disable.cache=true, fs.contract.supports-positioned-readable=false, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=350234300, fs.contract.supports-seek=true, fs.permissions.umask-mode=640, hadoop.security.groups.negative-cache.secs=1984497849, fs.creation.parallel.count=928261068, hadoop.security.groups.cache.warn.after.ms=1224511346];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriteMetaBlocksWithSameName;id_000000;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriteMetaBlocksWithSameName;id_000001;[file.bytes-per-checksum=661059289];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriteMetaBlocksWithSameName;id_000003;[file.bytes-per-checksum=1828784423];java.lang.NegativeArraySizeException;-720809377;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.TestGroupFallback#testNetgroupWithFallback;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestGroupFallback.testNetgroupWithFallback(TestGroupFallback.java:97)]
org.apache.hadoop.security.TestGroupFallback#testNetgroupShell;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestGroupFallback.testNetgroupShell(TestGroupFallback.java:59)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testAwaitFutureFailToFNFE;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToExistingParent;id_000000;[file.stream-buffer-size=1527252940, file.bytes-per-checksum=1039229930];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToExistingParent;id_000001;[file.bytes-per-checksum=1285056841];java.lang.NegativeArraySizeException;-1319390319;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToExistingParent;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestChecksumFileSystem#testTruncatedChecksum;id_000000;[file.bytes-per-checksum=1933016882];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestChecksumFileSystem#testTruncatedChecksum;id_000001;[io.file.buffer.size=1963618109];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestChecksumFileSystem#testTruncatedChecksum;id_000002;[file.bytes-per-checksum=1667182400];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestChecksumFileSystem#testTruncatedChecksum;id_000003;[file.bytes-per-checksum=766559714];java.lang.NegativeArraySizeException;-1690897166;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testTruncatedChecksum;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestChecksumFileSystem#testTruncatedChecksum;id_000006;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testChainedFailureAwaitFuture;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testAwaitFutureFailToFNFE;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.util.TestGenericOptionsParser#testTokenCacheOption;id_000000;[file.bytes-per-checksum=1802342011];java.lang.NegativeArraySizeException;-958791085;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.util.TestGenericOptionsParser#testTokenCacheOption;id_000001;[file.bytes-per-checksum=2118158202];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.util.TestGenericOptionsParser#testTokenCacheOption;id_000003;[file.bytes-per-checksum=64235237, io.file.buffer.size=1651370781];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.util.TestGenericOptionsParser#testTokenCacheOption;id_000004;[io.file.buffer.size=1315826461, file.bytes-per-checksum=126928417];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.util.TestGenericOptionsParser#testTokenCacheOption;id_000005;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.util.TestGenericOptionsParser#testTokenCacheOption;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameDirIntoExistingDir;id_000000;[io.file.buffer.size=779423557];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameDirIntoExistingDir;id_000001;[file.bytes-per-checksum=1044485649];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameDirIntoExistingDir;id_000002;[file.bytes-per-checksum=1112994286];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameDirIntoExistingDir;id_000003;[io.file.buffer.size=1692953495];java.lang.NegativeArraySizeException;-1348495171;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.TestGroupsCaching#testGroupLookupForStaticUsers;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestGroupsCaching.testGroupLookupForStaticUsers(TestGroupsCaching.java:309)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileOverExistingFile;id_000000;[file.stream-buffer-size=1003932833, io.file.buffer.size=1134585242];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileOverExistingFile;id_000002;[file.bytes-per-checksum=1342519932];java.lang.NegativeArraySizeException;-802222500;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileNonexistentDir;id_000000;[io.file.buffer.size=808985057];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileNonexistentDir;id_000001;[file.bytes-per-checksum=256464285];java.lang.NegativeArraySizeException;-1986788731;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileNonexistentDir;id_000004;[file.bytes-per-checksum=1989786161];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.security.token.TestDtUtilShell#testEdit;id_000000;[file.bytes-per-checksum=1593018655];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.token.TestDtUtilShell#testAppend;id_000000;[file.bytes-per-checksum=1995348278];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.token.TestDtUtilShell#testAppend;id_000001;[hadoop.security.token.service.use_ip=false, fs.permissions.umask-mode=002, fs.local.block.size=533436658, fs.file.impl.disable.cache=true, io.file.buffer.size=1176235774];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.token.TestDtUtilShell#testAppend;id_000002;[file.bytes-per-checksum=1667240093];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateFileOverExistingFileNoOverwrite;id_000000;[io.file.buffer.size=2142651346];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateFileOverExistingFileNoOverwrite;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=150913130, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.kerberos.min.seconds.before.relogin=981208054, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.service.shutdown.timeout=544494m, fs.permissions.umask-mode=666, file.stream-buffer-size=581615871, fs.local.block.size=1614250019, fs.creation.parallel.count=1448548068, fs.file.impl.disable.cache=true, io.file.buffer.size=1035703535];java.lang.NegativeArraySizeException;-1334450766;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateFileOverExistingFileNoOverwrite;id_000004;[io.file.buffer.size=1052683452, file.bytes-per-checksum=159288622];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenReadDir;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateNewFile;id_000000;[io.file.buffer.size=2097061532];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.ContractTestUtils.readDataset(ContractTestUtils.java:214)]
org.apache.hadoop.fs.TestLocalFileSystem#testStripFragmentFromPath;id_000000;[fs.file.impl.disable.cache=true, file.bytes-per-checksum=1106979658];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestLocalFileSystem#testStripFragmentFromPath;id_000001;[fs.permissions.umask-mode=277, hadoop.security.token.service.use_ip=true, hadoop.service.shutdown.timeout=582s, file.stream-buffer-size=561163788, fs.creation.parallel.count=1508731899, fs.file.impl.disable.cache=true];java.lang.NegativeArraySizeException;-1299636651;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestLocalFileSystem#testStripFragmentFromPath;id_000003;[fs.file.impl.disable.cache=true, file.bytes-per-checksum=661192049];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testDeleteRecursively;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testDeleteRecursively;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testDeleteRecursively;id_000003;[io.file.buffer.size=2144459207];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testWriteByteArrays;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1786042961, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=85247263, hadoop.service.shutdown.timeout=12053363h, file.stream-buffer-size=186001927, hadoop.security.groups.cache.background.reload.threads=1146407950, fs.local.block.size=1789728901, fs.file.impl.disable.cache=true, io.file.buffer.size=1409879374, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=414643028, fs.permissions.umask-mode=644, hadoop.security.groups.negative-cache.secs=1218158651, fs.creation.parallel.count=1480674092, fs.automatic.close=false, hadoop.security.groups.cache.warn.after.ms=1053400830];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testWriteByteArrays;id_000002;[file.bytes-per-checksum=828925506];java.lang.NegativeArraySizeException;-1129605038;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileReadZeroByte;id_000000;[file.bytes-per-checksum=1641481492];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileReadZeroByte;id_000001;[file.bytes-per-checksum=327735900];java.lang.NegativeArraySizeException;-1345344196;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.createNonRecursive(ChecksumFileSystem.java:565), org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder.build(FileSystem.java:4523)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileReadZeroByte;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringNoCharSetFileSystem;id_000000;[file.bytes-per-checksum=1277647810];java.lang.NegativeArraySizeException;-1386071598;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.createNonRecursive(ChecksumFileSystem.java:565), org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder.build(FileSystem.java:4523)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringNoCharSetFileSystem;id_000001;[file.stream-buffer-size=1349817386, io.file.buffer.size=1624866244];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringNoCharSetFileSystem;id_000002;[file.bytes-per-checksum=1470433815, io.file.buffer.size=929660356];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringNoCharSetFileSystem;id_000004;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileUtil#testWriteStringNoCharSetFileSystem;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestStat#testStat;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGetWrappedInputStream;id_000000;[file.stream-buffer-size=1912369104, io.file.buffer.size=1496639963];java.lang.NegativeArraySizeException;-2050975588;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGetWrappedInputStream;id_000002;[file.bytes-per-checksum=235957761];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGetWrappedInputStream;id_000003;[io.file.buffer.size=1496639963, file.bytes-per-checksum=1015221180];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGetWrappedInputStream;id_000004;[file.bytes-per-checksum=659882349];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGetWrappedInputStream;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGetWrappedInputStream;id_000006;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteHalfABlock;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteHalfABlock;id_000002;[io.file.buffer.size=1527192769];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.FileSystemContractBaseTest.writeAndRead(FileSystemContractBaseTest.java:937)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testWriteReadAndDeleteHalfABlock;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractDelete#testDeleteSingleFile;id_000000;[io.file.buffer.size=2091722339];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileAsExistingFile;id_000000;[file.bytes-per-checksum=853168218];java.lang.NegativeArraySizeException;-911420630;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileAsExistingFile;id_000001;[file.stream-buffer-size=1108463408, file.bytes-per-checksum=581118016];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testList;id_000000;[file.bytes-per-checksum=1721444471];java.lang.NegativeArraySizeException;-1686868945;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:197)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testList;id_000001;[file.bytes-per-checksum=1989920550];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testList;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongByte;id_000000;[io.file.buffer.size=1949207638];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongByte;id_000001;[file.bytes-per-checksum=543096758];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongByte;id_000002;[file.bytes-per-checksum=1397345919];java.lang.NegativeArraySizeException;-308788617;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongByte;id_000003;[file.bytes-per-checksum=646286333];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongByte;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongByte;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testOutputStreamStatisticKeys;id_000000;[file.bytes-per-checksum=1830891052];java.lang.NegativeArraySizeException;-701849716;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testOutputStreamStatisticKeys;id_000001;[file.bytes-per-checksum=164553065, io.file.buffer.size=777943451];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractStreamIOStatistics#testOutputStreamStatisticKeys;id_000002;[file.stream-buffer-size=1533248832, io.file.buffer.size=764099130];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.service.launcher.TestServiceConf#testRunService;id_000000;[delay.time=232633237];org.junit.runners.model.TestTimedOutException;test timed out after 15000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.service.launcher.testservices.LaunchableRunningService.execute(LaunchableRunningService.java:97), org.apache.hadoop.service.launcher.ServiceLauncher.coreServiceLaunch(ServiceLauncher.java:627), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:495), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:469)]
org.apache.hadoop.service.launcher.TestServiceLauncher#testNullBindService;id_000000;[delay.time=412945365];org.junit.runners.model.TestTimedOutException;test timed out after 15000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.service.launcher.testservices.LaunchableRunningService.execute(LaunchableRunningService.java:97), org.apache.hadoop.service.launcher.ServiceLauncher.coreServiceLaunch(ServiceLauncher.java:627), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:495), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:469)]
org.apache.hadoop.service.launcher.TestServiceLauncher#testRunLaunchableService;id_000000;[fail.runnable=true];org.junit.runners.model.TestTimedOutException;test timed out after 15000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.service.launcher.testservices.LaunchableRunningService.execute(LaunchableRunningService.java:97), org.apache.hadoop.service.launcher.ServiceLauncher.coreServiceLaunch(ServiceLauncher.java:627), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:495), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:469)]
org.apache.hadoop.service.launcher.TestServiceLauncher#testArgBinding;id_000000;[fail.runnable=false];org.junit.runners.model.TestTimedOutException;test timed out after 15000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.service.launcher.testservices.LaunchableRunningService.execute(LaunchableRunningService.java:97), org.apache.hadoop.service.launcher.ServiceLauncher.coreServiceLaunch(ServiceLauncher.java:627), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:495), org.apache.hadoop.service.launcher.ServiceLauncher.launchService(ServiceLauncher.java:469)]
org.apache.hadoop.fs.TestLocalFSFileContextMainOperations#testCreateFlagOverwriteNonExistingFile;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=898436620, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1890099161, hadoop.security.authentication=simple, fs.client.resolve.remote.symlinks=false, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1131386237, fs.local.block.size=945911304, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=635882062, hadoop.kerberos.keytab.login.autorenewal.enabled=false, fs.permissions.umask-mode=000, hadoop.security.groups.negative-cache.secs=111885733, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.creation.parallel.count=1564042172, hadoop.security.groups.cache.warn.after.ms=761917749];java.io.FileNotFoundException;Non existing file: file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task137/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/Qr2IAzOWiF/test/testCreateFlagOverwriteNonExistingFile. Create option is not specified in [OVERWRITE];[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:184), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353), org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)]
org.apache.hadoop.fs.TestLocalFSFileContextMainOperations#testCreateFlagCreateExistingFile;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1112958543, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=513300614, hadoop.security.authentication=simple, fs.client.resolve.remote.symlinks=true, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1331469904, fs.local.block.size=1404160921, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=2060782355, hadoop.kerberos.keytab.login.autorenewal.enabled=false, fs.permissions.umask-mode=640, hadoop.security.groups.negative-cache.secs=1329374731, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.creation.parallel.count=1618075817, hadoop.security.groups.cache.warn.after.ms=2069149995];org.apache.hadoop.fs.FileAlreadyExistsException;File already exists: file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task137/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/QptuNUEuNt/test/testCreateFlagCreateExistingFile. Append or overwrite option must be specified in [CREATE];[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:180), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353), org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToItself;id_000000;[file.bytes-per-checksum=1710671595];java.lang.NegativeArraySizeException;-1783824829;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToItself;id_000001;[file.bytes-per-checksum=1068695286];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameFileToItself;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testMkdirsFailsForSubdirectoryOfExistingFile;id_000000;[file.bytes-per-checksum=1522136756];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testMkdirsFailsForSubdirectoryOfExistingFile;id_000001;[hadoop.security.token.service.use_ip=true, fs.permissions.umask-mode=750, file.stream-buffer-size=1122923867, fs.creation.parallel.count=8719415, fs.file.impl.disable.cache=true];java.lang.NegativeArraySizeException;-177549974;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testMkdirsFailsForSubdirectoryOfExistingFile;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileIntoDir;id_000000;[file.bytes-per-checksum=1692853663];java.lang.NegativeArraySizeException;-1944186217;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileIntoDir;id_000001;[file.bytes-per-checksum=2102730414];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileIntoDir;id_000003;[file.stream-buffer-size=158827391, io.file.buffer.size=902575981, file.bytes-per-checksum=111570950];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileIntoDir;id_000004;[file.bytes-per-checksum=1486131194];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestChecksumFileSystem#testRenameFileIntoDir;id_000005;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testInputStreamClosedTwice;id_000000;[io.file.buffer.size=2087401648];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOverWriteAndRead;id_000000;[io.file.buffer.size=1161306915];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.FileSystemContractBaseTest.writeAndRead(FileSystemContractBaseTest.java:937)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOverWriteAndRead;id_000001;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOverWriteAndRead;id_000002;[io.file.buffer.size=2076872605];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOverWriteAndRead;id_000003;[io.file.buffer.size=2116212771];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testOverWriteAndRead;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir#testMkdirSlashHandling;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.token.TestDtUtilShell#testImport;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1656048800, hadoop.security.groups.cache.secs=57606899, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=02s, hadoop.security.groups.cache.background.reload.threads=877123916, fs.local.block.size=33302426, fs.file.impl.disable.cache=true, io.file.buffer.size=1993736216, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=373095183, fs.permissions.umask-mode=750, hadoop.security.groups.negative-cache.secs=602115428, fs.creation.parallel.count=1445972500, fs.automatic.close=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriterNotClosed;id_000000;[io.file.buffer.size=2128386714];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriterNotClosed;id_000001;[file.bytes-per-checksum=309205087];java.lang.NegativeArraySizeException;-1512121513;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriterNotClosed;id_000002;[file.stream-buffer-size=1802875979];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriterNotClosed;id_000003;[file.stream-buffer-size=1532221205];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriterNotClosed;id_000037;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractDelete#testDeleteEmptyDirRecursive;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testDeleteEmptyDirectory;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testMkdirUnderFile;id_000000;[io.file.buffer.size=2089944475];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testMkdirUnderFile;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testMkdirUnderFile;id_000003;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testFsIsEncrypted;id_000000;[file.bytes-per-checksum=1964215598];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testFsIsEncrypted;id_000002;[file.bytes-per-checksum=1337669167];java.lang.NegativeArraySizeException;-845879385;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenReadDir;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testAwaitFutureTimeoutFailToFNFE;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyFileNoOverwrite;id_000000;[io.file.buffer.size=1190630950];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyFileNoOverwrite;id_000001;[io.file.buffer.size=724865490];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:419), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:391)]
org.apache.hadoop.fs.viewfs.TestViewFsConfig#testInvalidConfig;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.TestViewFsConfig$1.<init>(TestViewFsConfig.java:43), org.apache.hadoop.fs.viewfs.TestViewFsConfig.testInvalidConfig(TestViewFsConfig.java:43), org.apache.hadoop.fs.viewfs.TestViewFsConfig.testInvalidConfig$$CONFUZZ(TestViewFsConfig.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.fs.TestFileSystemInitialization#testInitializationWithRegisteredStreamFactory;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=881946933, hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=240618140, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1518981254, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1312373037, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=475068734, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.creation.parallel.count=882298465, hadoop.security.groups.cache.warn.after.ms=1145341303];java.lang.Error;factory already defined;[java.net.URL.setURLStreamHandlerFactory(URL.java:1232), org.apache.hadoop.fs.TestFileSystemInitialization.testInitializationWithRegisteredStreamFactory(TestFileSystemInitialization.java:48), org.apache.hadoop.fs.TestFileSystemInitialization.testInitializationWithRegisteredStreamFactory$$CONFUZZ(TestFileSystemInitialization.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]
org.apache.hadoop.security.authorize.TestProxyUsers#testNullIpAddress;id_000000;[hadoop.proxyuser.proxier.hosts=10.222.0.0/16,10.113.221.221, hadoop.security.dns.log-slow-lookups.threshold.ms=427687463, hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=153887731, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=945307076, hadoop.security.groups.cache.background.reload=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.proxyuser.proxier.groups=*, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1858202557, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=87938716, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.security.groups.cache.warn.after.ms=2091955067];java.lang.IllegalArgumentException;address is null.;[org.apache.hadoop.util.MachineList.includes(MachineList.java:172), org.apache.hadoop.security.authorize.DefaultImpersonationProvider.authorize(DefaultImpersonationProvider.java:131), org.apache.hadoop.security.authorize.ProxyUsers.authorize(ProxyUsers.java:113), org.apache.hadoop.security.authorize.TestProxyUsers.testNullIpAddress(TestProxyUsers.java:380), org.apache.hadoop.security.authorize.TestProxyUsers.testNullIpAddress$$CONFUZZ(TestProxyUsers.java)]
org.apache.hadoop.security.alias.TestCredentialProviderFactory#testUserProvider;id_000001;[hadoop.security.credential.clear-text-fallback=false];java.io.IOException;Credential pass already exists in user:///;[org.apache.hadoop.security.alias.UserProvider.createCredentialEntry(UserProvider.java:69), org.apache.hadoop.security.alias.TestCredentialProviderFactory.checkSpecificProvider(TestCredentialProviderFactory.java:136), org.apache.hadoop.security.alias.TestCredentialProviderFactory.testUserProvider(TestCredentialProviderFactory.java:196), org.apache.hadoop.security.alias.TestCredentialProviderFactory.testUserProvider$$CONFUZZ(TestCredentialProviderFactory.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testThreeBlocks;id_000000;[tfile.fs.output.buffer.size=476998864, file.bytes-per-checksum=189161995];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testThreeBlocks;id_000001;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testThreeBlocks;id_000003;[file.bytes-per-checksum=872204927];java.lang.NegativeArraySizeException;-740090249;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongRandom;id_000000;[file.bytes-per-checksum=495604447, io.file.buffer.size=2032302346];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongRandom;id_000001;[file.bytes-per-checksum=972407498, hadoop.security.token.service.use_ip=true];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongRandom;id_000002;[file.bytes-per-checksum=1340757965];java.lang.NegativeArraySizeException;-818080203;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLongRandom;id_000003;[file.bytes-per-checksum=689569248];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.compress.TestCodec#testSequenceFileBZip2Codec;id_000000;[file.bytes-per-checksum=1471281118];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.compress.TestCodec#testGzipCodec;id_000000;[io.file.buffer.size=2100388498];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:160), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:171)]
org.apache.hadoop.io.compress.TestCodec#testGzipCodec;id_000001;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:60), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:160), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:171)]
org.apache.hadoop.io.compress.TestCodec#testGzipCodec;id_000004;[io.file.buffer.size=2087678761];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestLocalFSFileContextMainOperations#testEmptyCreateFlag;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=588824037, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=866227728, hadoop.security.authentication=simple, fs.client.resolve.remote.symlinks=false, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1367105527, fs.local.block.size=358710569, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=95673695, hadoop.kerberos.keytab.login.autorenewal.enabled=false, fs.permissions.umask-mode=022, hadoop.security.groups.negative-cache.secs=1791911230, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.creation.parallel.count=582782926, hadoop.security.groups.cache.warn.after.ms=1238562574];org.apache.hadoop.HadoopIllegalArgumentException;[] does not specify any options;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:151), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)]
org.apache.hadoop.fs.TestLocalFSFileContextMainOperations#testCreateFlagAppendNonExistingFile;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=235422041, hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1512265052, hadoop.security.authentication=simple, fs.client.resolve.remote.symlinks=false, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=537221358, fs.local.block.size=1048083214, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1176106135, hadoop.kerberos.keytab.login.autorenewal.enabled=false, fs.permissions.umask-mode=640, hadoop.security.groups.negative-cache.secs=499540392, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.creation.parallel.count=808252682, hadoop.security.groups.cache.warn.after.ms=1946255478];java.io.FileNotFoundException;Non existing file: file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task147/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/A83cIdiofO/test/testCreateFlagAppendNonExistingFile. Create option is not specified in [APPEND];[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:184), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353), org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)]
org.apache.hadoop.security.token.TestDtUtilShell#testGet;id_000000;[file.bytes-per-checksum=226761467];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.token.TestDtUtilShell#testGet;id_000001;[file.stream-buffer-size=1641850625, io.file.buffer.size=1854483096];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.token.TestDtUtilShell#testGet;id_000002;[file.bytes-per-checksum=232419405];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureNegativeLength;id_000000;[io.file.buffer.size=2048010771, tfile.fs.output.buffer.size=853852653];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureNegativeLength;id_000001;[file.bytes-per-checksum=2084779036];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureNegativeLength;id_000002;[file.stream-buffer-size=1560009570, tfile.fs.output.buffer.size=628062696];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureNegativeLength;id_000003;[file.bytes-per-checksum=824023340];java.lang.NegativeArraySizeException;-1173724532;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureNegativeLength;id_000006;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureBadCompressionCodec;id_000000;[tfile.fs.output.buffer.size=722423512, file.bytes-per-checksum=985924689];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureBadCompressionCodec;id_000001;[file.bytes-per-checksum=1264637091];java.lang.NegativeArraySizeException;-1503168069;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureBadCompressionCodec;id_000003;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureBadCompressionCodec;id_000018;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureBadCompressionCodec;id_000019;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureOneEntryKnownLength;id_000000;[file.bytes-per-checksum=830943358];java.lang.NegativeArraySizeException;-1111444370;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureOneEntryKnownLength;id_000001;[tfile.fs.output.buffer.size=1926193701, file.bytes-per-checksum=980504387];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureOneEntryKnownLength;id_000003;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=563803899, hadoop.security.dns.log-slow-lookups.enabled=false, fs.permissions.umask-mode=027, file.stream-buffer-size=1245801669, fs.local.block.size=504353356, fs.creation.parallel.count=753426049, fs.file.impl.disable.cache=true, io.file.buffer.size=674552958];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureOneEntryKnownLength;id_000004;[tfile.fs.output.buffer.size=1820618671, io.file.buffer.size=674552958];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.OutOfMemoryError(Java heap space),  java.lang.OutOfMemoryError(Java heap space);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.compress.TestCodec#testBuiltInGzipConcat;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:60), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.TestCodec.GzipConcatTest(TestCodec.java:730), org.apache.hadoop.io.compress.TestCodec.testBuiltInGzipConcat(TestCodec.java:743)]
org.apache.hadoop.io.compress.TestCodec#testBuiltInGzipConcat;id_000002;[io.file.buffer.size=2131592442];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.GzipCodec.createInputStream(GzipCodec.java:182), org.apache.hadoop.io.compress.TestCodec.GzipConcatTest(TestCodec.java:730), org.apache.hadoop.io.compress.TestCodec.testBuiltInGzipConcat(TestCodec.java:743)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadSmallFile;id_000000;[file.bytes-per-checksum=2028137128];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadSmallFile;id_000001;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=2083340120, hadoop.kerberos.min.seconds.before.relogin=2103858982, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=0h, fs.permissions.umask-mode=664, file.stream-buffer-size=238488216, fs.local.block.size=356219471, fs.creation.parallel.count=400501386, fs.file.impl.disable.cache=true, fs.automatic.close=false];java.lang.NegativeArraySizeException;-933216946;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadSmallFile;id_000004;[file.bytes-per-checksum=1969198754];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenFileNullStatus;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;Invalid value of fs.creation.parallel.count: 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:415), org.apache.hadoop.fs.FileSystem$Cache.<init>(FileSystem.java:3514), org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:206), org.apache.hadoop.fs.contract.rawlocal.RawlocalFSContract.getLocalFS(RawlocalFSContract.java:46), org.apache.hadoop.fs.contract.localfs.LocalFSContract.init(LocalFSContract.java:63)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusWithMultipleMatchesOfSingleChar;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testListStatus;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testCreateDelete;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testCreateDelete;id_000001;[file.bytes-per-checksum=1567248941];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testCreateDelete;id_000002;[file.bytes-per-checksum=953712246];java.lang.NegativeArraySizeException;-6524378;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:197)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testWorkingDirectory;id_000000;[io.file.buffer.size=2129588494];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testWorkingDirectory;id_000001;[file.bytes-per-checksum=1884053372];java.lang.NegativeArraySizeException;-223388836;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:197)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testWorkingDirectory;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testWorkingDirectory;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestFileSystemCaching#testCacheSingleSemaphoredConstruction;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalRename2;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1395279950, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/BBEQFkmtbX/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/BBEQFkmtbX, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/BBEQFkmtbX/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/BBEQFkmtbX/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/BBEQFkmtbX/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=270955969, fs.creation.parallel.count=1599917802, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=200318668, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/BBEQFkmtbX/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1282582254, fs.local.block.size=1521547200, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1996647650, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/BBEQFkmtbX/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=027, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/BBEQFkmtbX/user, hadoop.security.groups.cache.warn.after.ms=617072644];org.apache.hadoop.security.AccessControlException;Cannot Rename within internal dirs of mount table: src=/internalDir/linkToDir2 is readOnly;[org.apache.hadoop.fs.viewfs.ViewFs.renameInternal(ViewFs.java:578), org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720), org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalRename2(ViewFsBaseTest.java:777), org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs.testInternalRename2$$CONFUZZ(TestViewFsWithAuthorityLocalFs.java)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalMkdirExisting2;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1761286631, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/YrYCxpzZMc/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/YrYCxpzZMc, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/YrYCxpzZMc/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/YrYCxpzZMc/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/YrYCxpzZMc/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=2123935140, fs.creation.parallel.count=936078148, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=314422210, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/YrYCxpzZMc/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=839927193, fs.local.block.size=956209992, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=338231388, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/YrYCxpzZMc/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=600, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/YrYCxpzZMc/user, hadoop.security.groups.cache.warn.after.ms=671386309];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /linkToDir2.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalMkdirExisting1;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=2013271265, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/hXp65mJcfy/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/hXp65mJcfy, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/hXp65mJcfy/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/hXp65mJcfy/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/hXp65mJcfy/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=309127598, fs.creation.parallel.count=617758062, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1207179988, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/hXp65mJcfy/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=461545569, fs.local.block.size=795325967, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1845514268, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/hXp65mJcfy/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=777, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/hXp65mJcfy/user, hadoop.security.groups.cache.warn.after.ms=2097212785];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /internalDir.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testListStatusFilterWithSomeMatches;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testMkdirs;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryToItself;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestFSMainOperationsLocalFileSystem#testGlobStatusSomeMatchesInDirectories;id_000000;[fs.viewfs.mounttable.default.name.key=default];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalRename2;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=940576420, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/OccbBewwwY/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/OccbBewwwY, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/OccbBewwwY/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/OccbBewwwY/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/OccbBewwwY/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1313371947, fs.creation.parallel.count=178246576, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=962111146, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/OccbBewwwY/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=947065498, fs.local.block.size=1900587417, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=928096905, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/OccbBewwwY/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=750, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task89/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/OccbBewwwY/user, hadoop.security.groups.cache.warn.after.ms=1764914000];org.apache.hadoop.security.AccessControlException;Cannot Rename within internal dirs of mount table: src=/internalDir/linkToDir2 is readOnly;[org.apache.hadoop.fs.viewfs.ViewFs.renameInternal(ViewFs.java:578), org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720), org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalRename2(ViewFsBaseTest.java:777), org.apache.hadoop.fs.viewfs.TestViewFsLocalFs.testInternalRename2$$CONFUZZ(TestViewFsLocalFs.java)]
org.apache.hadoop.fs.shell.TestPathData#testQualifiedUriContents;id_000000;[io.file.buffer.size=2131840544];java.lang.NegativeArraySizeException;-663783231;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.shell.TestPathData#testQualifiedUriContents;id_000001;[file.bytes-per-checksum=1579786828];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.shell.TestPathData#testQualifiedUriContents;id_000002;[file.bytes-per-checksum=983451829, io.file.buffer.size=1247905887];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.shell.TestPathData#testQualifiedUriContents;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.shell.TestPathData#testQualifiedUriContents;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testListStatusFilterWithSomeMatches;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testWriteInNonExistentDirectory;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testMkdirs;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testCreateFlagAppendOverwrite;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testFileContextStatistics;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusWithNoMatchesInPath;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testOpenFileApplyAsyncRead;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testMkdirsFailsForSubdirectoryOfExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testGlobStatusFilterWithMultipleWildCardMatchesAndTrivialFilter;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testNullCreateFlag;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.viewfs.TestFcMainOperationsLocalFs#testCreateFlagCreateAppendNonExistingFile;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testGlobStatusFilterWithMultipleWildCardMatchesAndTrivialFilter;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testOverwriteExistingFile;id_000000;[io.file.buffer.size=2131909967];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.security.token.TestDtUtilShell#testImportWithAliasFlag;id_000000;[file.bytes-per-checksum=2035246219];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.token.TestDtUtilShell#testImportWithAliasFlag;id_000001;[file.bytes-per-checksum=2049231962];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.token.TestDtUtilShell#testGetWithServiceFlag;id_000000;[file.bytes-per-checksum=706734138];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.token.TestDtUtilShell#testGetWithServiceFlag;id_000001;[hadoop.security.token.service.use_ip=false, fs.permissions.umask-mode=666, fs.local.block.size=370400343, fs.creation.parallel.count=1873221128, fs.file.impl.disable.cache=true, io.file.buffer.size=1984587623];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.token.TestDtUtilShell#testGetWithServiceFlag;id_000002;[io.file.buffer.size=1984587623, file.bytes-per-checksum=130732182];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testMkdirUnderFileSubdir;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=669436933, hadoop.kerberos.min.seconds.before.relogin=860882068, hadoop.security.groups.cache.secs=734747425, hadoop.service.shutdown.timeout=86439058m, fs.permissions.umask-mode=027, file.stream-buffer-size=974183453, hadoop.security.groups.negative-cache.secs=717993925, hadoop.security.groups.cache.background.reload.threads=1813081502, fs.creation.parallel.count=499766678, fs.file.impl.disable.cache=true, io.file.buffer.size=357869402, hadoop.security.groups.cache.warn.after.ms=1387908646];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testMkdirUnderFileSubdir;id_000001;[io.file.buffer.size=2141935814];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testMkdirUnderFileSubdir;id_000002;[file.bytes-per-checksum=1784464907];java.lang.NegativeArraySizeException;-1119685021;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testMkdirUnderFileSubdir;id_000003;[hadoop.security.dns.log-slow-lookups.enabled=true, file.stream-buffer-size=85657317, fs.local.block.size=286238680, fs.creation.parallel.count=1605914774, fs.file.impl.disable.cache=true, io.file.buffer.size=1116007659];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateMakesParentDirs;id_000000;[file.bytes-per-checksum=236631374];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateMakesParentDirs;id_000001;[file.bytes-per-checksum=1644823900];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateMakesParentDirs;id_000002;[io.file.buffer.size=1672073469, file.bytes-per-checksum=1454893622];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateMakesParentDirs;id_000003;[hadoop.security.dns.log-slow-lookups.threshold.ms=225214919, hadoop.kerberos.min.seconds.before.relogin=1664182966, hadoop.service.shutdown.timeout=15507h, fs.permissions.umask-mode=660, file.stream-buffer-size=150220832, hadoop.security.groups.negative-cache.secs=110323641, hadoop.security.groups.cache.background.reload.threads=29911531, fs.local.block.size=882567183, fs.creation.parallel.count=400681743, fs.file.impl.disable.cache=true, io.file.buffer.size=1223217179, hadoop.security.groups.cache.warn.after.ms=1061701660];java.lang.NegativeArraySizeException;-1703038049;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateMakesParentDirs;id_000004;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=11907606, hadoop.security.dns.log-slow-lookups.enabled=true, fs.permissions.umask-mode=777, file.stream-buffer-size=0, fs.local.block.size=162791424, fs.creation.parallel.count=2000925455, fs.file.impl.disable.cache=true, io.file.buffer.size=15257627];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testCreateMakesParentDirs;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testFileStatusBlocksizeEmptyFile;id_000000;[file.bytes-per-checksum=1982286313];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testFileStatusBlocksizeEmptyFile;id_000001;[file.bytes-per-checksum=2062497912];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testFileStatusBlocksizeEmptyFile;id_000002;[file.bytes-per-checksum=2062488513];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testFileStatusBlocksizeEmptyFile;id_000003;[file.bytes-per-checksum=251541381];java.lang.NegativeArraySizeException;-2031094867;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFsShellReturnCode#testChgrpGroupValidity;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenFileApplyAsyncRead;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateUnderFileSubdir;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateUnderFileSubdir;id_000002;[io.file.buffer.size=2130843839];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreateUnderFileSubdir;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testMkdirs;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;Invalid value of fs.creation.parallel.count: 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:415), org.apache.hadoop.fs.FileSystem$Cache.<init>(FileSystem.java:3514), org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:206), org.apache.hadoop.fs.TestRawLocalFileSystemContract.setUp(TestRawLocalFileSystemContract.java:66), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileFailExceptionally;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testNoMkdirOverFile;id_000000;[file.bytes-per-checksum=370713354];java.lang.NegativeArraySizeException;-958547110;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testNoMkdirOverFile;id_000001;[file.stream-buffer-size=1763182811, io.file.buffer.size=237266351];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testNoMkdirOverFile;id_000003;[file.stream-buffer-size=1594301425, io.file.buffer.size=1295025479];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.ftp.TestFTPFileSystem#testFTPDataConnectionMode;id_000000;[fs.ftp.data.connection.mode=PASSIVE_REMOTE_DATA_CONNECTION_MODE];java.io.IOException;Connection is not open;[org.apache.commons.net.ftp.FTP.sendCommand(FTP.java:514), org.apache.commons.net.ftp.FTP.sendCommand(FTP.java:648), org.apache.commons.net.ftp.FTP.sendCommand(FTP.java:622), org.apache.commons.net.ftp.FTP.pasv(FTP.java:1045), org.apache.commons.net.ftp.FTPClient.enterRemotePassiveMode(FTPClient.java:1342)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFiltering;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFiltering;id_000001;[io.file.buffer.size=2127699223];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFiltering;id_000003;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFilteredFile;id_000000;[io.file.buffer.size=2087356128];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFilteredFile;id_000001;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFilteredFile;id_000004;[io.file.buffer.size=2092020232];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFilteredFile;id_000013;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testSeekBigFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFile;id_000000;[file.bytes-per-checksum=1761440555];java.lang.NegativeArraySizeException;-1326904189;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFile;id_000002;[file.bytes-per-checksum=1528230898];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListStatusFile;id_000004;[file.stream-buffer-size=1927479385, io.file.buffer.size=184549439];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.TestSequenceFile#testCloseForErroneousSequenceFile;id_000000;[file.stream-buffer-size=1391368451, io.file.buffer.size=980372585];java.lang.NegativeArraySizeException;-1461081495;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFile#testCloseForErroneousSequenceFile;id_000002;[file.bytes-per-checksum=1053286423];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testCloseForErroneousSequenceFile;id_000004;[file.stream-buffer-size=1592170755, file.bytes-per-checksum=492294303];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.io.SequenceFile$Reader.openFile(SequenceFile.java:1962), org.apache.hadoop.io.TestSequenceFile$1.openFile(TestSequenceFile.java:597)]
org.apache.hadoop.io.TestSequenceFile#testCloseForErroneousSequenceFile;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testCloseForErroneousSequenceFile;id_000007;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureFileWriteNotAt0Position;id_000000;[io.file.buffer.size=2100678950];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureFileWriteNotAt0Position;id_000001;[file.stream-buffer-size=1997955540, io.file.buffer.size=1004053170];java.lang.NegativeArraySizeException;-1490975554;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1143815390;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir2;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir2;id_000016;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts3;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-496369510;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts3;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts3;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts3;id_000008;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1174823899;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir;id_000007;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMountPoints;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMountPoints;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-279639277;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMountPoints;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMountPoints;id_000020;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCheckOwnerWithFileStatus;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCheckOwnerWithFileStatus;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-520751803;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testCheckOwnerWithFileStatus;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir3;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-928961734;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir3;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreateMissingDir3;id_000017;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalGetAllStoragePolicies;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1493660694;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalGetAllStoragePolicies;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalGetAllStoragePolicies;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1473670624;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testRenameAcrossMounts2;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testViewFileSystemUtil;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-94736092;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testViewFileSystemUtil;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testViewFileSystemUtil;id_000012;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testUsed;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testUsed;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-438462463;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testUsed;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testViewFileSystemUtil;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-668697250;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testViewFileSystemUtil;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testViewFileSystemUtil;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testViewFileSystemUtil;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathThroughMountPoints;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-414005359;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathThroughMountPoints;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathThroughMountPoints;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathThroughMountPoints;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalDeleteExisting2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-416890961;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalDeleteExisting2;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalDeleteExisting2;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalDeleteExisting2;id_000005;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalRenameToSlash;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalRenameToSlash;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-669677727;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalRenameToSlash;id_000019;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMissingThroughMountPoints;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMissingThroughMountPoints;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1953464151;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMissingThroughMountPoints;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testResolvePathMissingThroughMountPoints;id_000024;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetBlockLocations;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetBlockLocations;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1727610591;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetBlockLocations;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetBlockLocations;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testGetBlockLocations;id_000014;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreate1;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-2001348502;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreate1;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreate1;id_000012;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemWithAuthorityLocalFileSystem#testInternalCreate1;id_000017;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts3;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts3;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-889391731;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts3;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts3;id_000013;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testCheckOwnerWithFileStatus;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testCheckOwnerWithFileStatus;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1676865580;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testCheckOwnerWithFileStatus;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testCheckOwnerWithFileStatus;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts4;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1032248338;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts4;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts4;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts4;id_000004;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathMissingThroughMountPoints2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathMissingThroughMountPoints2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testResolvePathMissingThroughMountPoints2;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1856240143;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreate1;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1090582537;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreate1;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreate1;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreate1;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testInternalCreate1;id_000008;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts2;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-1983762152;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts2;id_000002;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testRenameAcrossMounts2;id_000009;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalMkdirNew2;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1041192000, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wRSt5LZakP/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wRSt5LZakP, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wRSt5LZakP/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wRSt5LZakP/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wRSt5LZakP/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=980065713, fs.creation.parallel.count=950357545, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=645469845, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wRSt5LZakP/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=2119397241, fs.local.block.size=1279016507, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1223351622, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wRSt5LZakP/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=644, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/wRSt5LZakP/user, hadoop.security.groups.cache.warn.after.ms=126039537];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /dirNew.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalCreate2;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=2031580747, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/lRXri5QPJI/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/lRXri5QPJI, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/lRXri5QPJI/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/lRXri5QPJI/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/lRXri5QPJI/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=710900107, fs.creation.parallel.count=606031278, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=238988932, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/lRXri5QPJI/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=203388929, fs.local.block.size=193585946, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=491463021, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/lRXri5QPJI/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=750, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/lRXri5QPJI/user, hadoop.security.groups.cache.warn.after.ms=220156203];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path /foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.createInternal(ViewFs.java:1028), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:358), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalMkdirNew2;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=676742000, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/PYhNnO99zP/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/PYhNnO99zP, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/PYhNnO99zP/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/PYhNnO99zP/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/PYhNnO99zP/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=2019586422, fs.creation.parallel.count=813921720, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1385501170, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/PYhNnO99zP/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=795796903, fs.local.block.size=227262632, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1357500729, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/PYhNnO99zP/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=640, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/PYhNnO99zP/user, hadoop.security.groups.cache.warn.after.ms=227687878];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /dirNew.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testgetFSonDanglingLink;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1243780097, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/KYZnJYDkGt/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/KYZnJYDkGt, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/KYZnJYDkGt/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/KYZnJYDkGt/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/KYZnJYDkGt/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=1135345418, fs.creation.parallel.count=1242156551, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1749753721, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/KYZnJYDkGt/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1261892757, fs.local.block.size=140049035, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=468137720, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/KYZnJYDkGt/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/KYZnJYDkGt/user, hadoop.security.groups.cache.warn.after.ms=29531325];java.io.FileNotFoundException;File /mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/QDvkE694qE/missingTarget does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128), org.apache.hadoop.fs.FilterFs.getFileStatus(FilterFs.java:124)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalMkdirSlash;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=985912297, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uoOXacF3MM/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uoOXacF3MM, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uoOXacF3MM/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uoOXacF3MM/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uoOXacF3MM/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1213031371, fs.creation.parallel.count=1323336364, fs.viewfs.rename.strategy=SAME_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=689425861, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uoOXacF3MM/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=89056870, fs.local.block.size=938053750, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=53645329, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uoOXacF3MM/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=755, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uoOXacF3MM/user, hadoop.security.groups.cache.warn.after.ms=886967120];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalMkdirExisting2;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1084800531, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uPAOYJDqPH/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uPAOYJDqPH, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uPAOYJDqPH/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uPAOYJDqPH/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uPAOYJDqPH/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=299713947, fs.creation.parallel.count=1965434312, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1360616633, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uPAOYJDqPH/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=30390859, fs.local.block.size=545805219, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1980378466, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uPAOYJDqPH/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=770, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/uPAOYJDqPH/user, hadoop.security.groups.cache.warn.after.ms=1061606476];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /linkToDir2.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testResolvePathMissingThroughMountPoints;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1501292781, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/1baSzOXjhD/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/1baSzOXjhD, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/1baSzOXjhD/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/1baSzOXjhD/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/1baSzOXjhD/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1621145279, fs.creation.parallel.count=2042649437, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=665744837, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/1baSzOXjhD/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=514321939, fs.local.block.size=1289002453, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=980884198, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/1baSzOXjhD/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=500, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/1baSzOXjhD/user, hadoop.security.groups.cache.warn.after.ms=1086494603];java.io.FileNotFoundException;File /mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/LjoV2dL4qr/user/nonExisting does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128), org.apache.hadoop.fs.FilterFs.getFileStatus(FilterFs.java:124)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalCreate1;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1457815212, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/pisUMGQzJk/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/pisUMGQzJk, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/pisUMGQzJk/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/pisUMGQzJk/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/pisUMGQzJk/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=681482236, fs.creation.parallel.count=414060362, fs.viewfs.rename.strategy=SAME_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=600936858, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/pisUMGQzJk/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=980243581, fs.local.block.size=1913969430, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1826240777, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/pisUMGQzJk/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=002, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task96/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/pisUMGQzJk/user, hadoop.security.groups.cache.warn.after.ms=617590048];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path /foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.createInternal(ViewFs.java:1028), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:358), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]
org.apache.hadoop.fs.TestTrash#testTrashEmptier;id_000000;[file.bytes-per-checksum=1310122751];java.lang.NegativeArraySizeException;-1093797129;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestTrash#testTrashEmptier;id_000001;[io.file.buffer.size=2082160751];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestTrash#testTrashEmptier;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestTrash#testTrashEmptier;id_000006;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.ha.TestZKFailoverController#testNoZK;id_000000;[ha.zookeeper.session-timeout.ms=1332599424];org.junit.runners.model.TestTimedOutException;test timed out after 180000 milliseconds;[jdk.internal.misc.Unsafe.park(Native Method), java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234), java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1079), java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1369), java.util.concurrent.CountDownLatch.await(CountDownLatch.java:278)]
org.apache.hadoop.io.TestSequenceFile#testZlibSequenceFile;id_000000;[file.bytes-per-checksum=57051657];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.TestSequenceFile#testZlibSequenceFile;id_000001;[io.file.buffer.size=401675367];java.lang.NegativeArraySizeException;-542825508;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFile#testZlibSequenceFile;id_000002;[io.file.buffer.size=1868724412];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testZlibSequenceFile;id_000003;[io.file.buffer.size=2136731644];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testClientRetriesWithSSLHandshakeExceptionFailsAtEveryAttempt;id_000000;[hadoop.security.kms.client.failover.sleep.max.millis=1235949039, hadoop.security.kms.client.failover.sleep.base.millis=378545475];org.junit.runners.model.TestTimedOutException;test timed out after 30000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:220), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.createKey(LoadBalancingKMSClientProvider.java:481), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.lambda$testClientRetriesWithSSLHandshakeExceptionFailsAtEveryAttempt$0(TestLoadBalancingKMSClientProvider.java:761), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider$$Lambda$104/0x000000084020e440.call(Unknown Source)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testClientRetriesSucceedsSecondTime;id_000000;[hadoop.security.kms.client.failover.sleep.max.millis=1111635037, hadoop.security.kms.client.failover.sleep.base.millis=2071679023];org.junit.runners.model.TestTimedOutException;test timed out after 30000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:220), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.createKey(LoadBalancingKMSClientProvider.java:481), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testClientRetriesSucceedsSecondTime(TestLoadBalancingKMSClientProvider.java:590), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testClientRetriesSucceedsSecondTime$$CONFUZZ(TestLoadBalancingKMSClientProvider.java)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testClientRetriesWithSSLHandshakeExceptionSucceedsSecondTime;id_000000;[hadoop.security.kms.client.failover.sleep.max.millis=1063182215, hadoop.security.kms.client.failover.sleep.base.millis=1356993142];org.junit.runners.model.TestTimedOutException;test timed out after 30000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:220), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.createKey(LoadBalancingKMSClientProvider.java:481), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testClientRetriesWithSSLHandshakeExceptionSucceedsSecondTime(TestLoadBalancingKMSClientProvider.java:726), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testClientRetriesWithSSLHandshakeExceptionSucceedsSecondTime$$CONFUZZ(TestLoadBalancingKMSClientProvider.java)]
org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider#testClientRetriesSpecifiedNumberOfTimes;id_000000;[hadoop.security.kms.client.failover.sleep.max.millis=785116708];org.junit.runners.model.TestTimedOutException;test timed out after 30000 milliseconds;[java.lang.Thread.sleep(Native Method), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.doOp(LoadBalancingKMSClientProvider.java:220), org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider.createKey(LoadBalancingKMSClientProvider.java:481), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testClientRetriesSpecifiedNumberOfTimes(TestLoadBalancingKMSClientProvider.java:622), org.apache.hadoop.crypto.key.kms.TestLoadBalancingKMSClientProvider.testClientRetriesSpecifiedNumberOfTimes$$CONFUZZ(TestLoadBalancingKMSClientProvider.java)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testNPEatInitialization;id_000000;[decay-scheduler.metrics.top.user.count=986942074];java.lang.OutOfMemoryError;Java heap space;[java.util.PriorityQueue.<init>(PriorityQueue.java:172), java.util.PriorityQueue.<init>(PriorityQueue.java:139), org.apache.hadoop.metrics2.util.Metrics2Util$TopN.<init>(Metrics2Util.java:80), org.apache.hadoop.ipc.DecayRpcScheduler.getTopCallers(DecayRpcScheduler.java:1002), org.apache.hadoop.ipc.DecayRpcScheduler.addTopNCallerSummary(DecayRpcScheduler.java:982)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testNPEatInitialization;id_000001;[decay-scheduler.metrics.top.user.count=1419140791];org.apache.hadoop.metrics2.MetricsException;Metrics source DecayRpcSchedulerDetailedMetrics.ipc.14 already exists!;[org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152), org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125), org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229), org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics.create(DecayRpcSchedulerDetailedMetrics.java:63), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:250)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testParsePeriod;id_000000;[ipc.1.decay-scheduler.period-ms=0];java.lang.IllegalArgumentException;Decay Factor must be between 0 and 1;[org.apache.hadoop.ipc.DecayRpcScheduler.parseDecayFactor(DecayRpcScheduler.java:311), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:227), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParsePeriod(TestDecayRpcScheduler.java:73), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParsePeriod$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testParsePeriod;id_000001;[ipc.1.decay-scheduler.period-ms=0];java.lang.IllegalArgumentException;Period millis must be >= 0;[org.apache.hadoop.ipc.DecayRpcScheduler.parseDecayPeriodMillis(DecayRpcScheduler.java:332), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:228), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParsePeriod(TestDecayRpcScheduler.java:73), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParsePeriod$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.ipc.TestDecayRpcScheduler#testParsePeriod;id_000002;[ipc.1.faircallqueue.decay-scheduler.period-ms=84];java.lang.IllegalArgumentException;the number of top users for scheduler metrics must be at least 1;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:246), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParsePeriod(TestDecayRpcScheduler.java:73), org.apache.hadoop.ipc.TestDecayRpcScheduler.testParsePeriod$$CONFUZZ(TestDecayRpcScheduler.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.ipc.TestCallQueueManager#testFcqBackwardCompatibility;id_000000;[.scheduler.priority.levels=1500054628];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.DecayRpcScheduler.getDefaultThresholds(DecayRpcScheduler.java:377), org.apache.hadoop.ipc.DecayRpcScheduler.parseThresholds(DecayRpcScheduler.java:346), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:231), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.ipc.TestCallQueueManager#testSchedulerWithoutFCQ;id_000000;[.faircallqueue.priority-levels=1552811285];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.DecayRpcScheduler.getDefaultThresholds(DecayRpcScheduler.java:377), org.apache.hadoop.ipc.DecayRpcScheduler.parseThresholds(DecayRpcScheduler.java:346), org.apache.hadoop.ipc.DecayRpcScheduler.<init>(DecayRpcScheduler.java:231), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testCopyToLocalWithUseRawLocalFileSystemOption;id_000000;[io.file.buffer.size=1793254177];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testCopyToLocalWithUseRawLocalFileSystemOption;id_000002;[io.file.buffer.size=2084766030];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testCopyToLocalWithUseRawLocalFileSystemOption;id_000003;[fs.creation.parallel.count=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testCopyToLocalWithUseRawLocalFileSystemOption;id_000004;[io.file.buffer.size=668598272];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestTrash#testPluggableTrash;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileSystemCaching#testUserFS;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileSystemCaching#testFsUniqueness;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFsShellList#testListWithUGI;id_000000;[fs.defaultFS=file:///, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=85485771, fs.client.resolve.remote.symlinks=false, hadoop.service.shutdown.timeout=52884s, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=1652140780, fs.creation.parallel.count=845766053, file.bytes-per-checksum=1225522717, fs.automatic.close=false, hadoop.security.dns.log-slow-lookups.threshold.ms=1865315588, hadoop.security.authentication=DUMMYAUTH, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1866877635, fs.local.block.size=1208524579];java.lang.IllegalArgumentException;Invalid attribute value for hadoop.security.authentication of DUMMYAUTH;[org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:720), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312), org.apache.hadoop.security.UserGroupInformation.setConfiguration(UserGroupInformation.java:366), org.apache.hadoop.fs.FsShell.init(FsShell.java:102), org.apache.hadoop.fs.FsShell.run(FsShell.java:303)]
org.apache.hadoop.fs.TestFileUtil#testCopy5;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFileUtil#testCopy5;id_000001;[io.file.buffer.size=1081515039];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:114), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:502), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:482)]
org.apache.hadoop.fs.TestFileUtil#testCopy5;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testCreatedFileIsEventuallyVisible;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testMkdirsFailsForSubdirectoryOfExistingFile;id_000000;[io.file.buffer.size=2137562486];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testMkdirsFailsForSubdirectoryOfExistingFile;id_000001;[io.file.buffer.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestRawLocalFileSystemContract#testMkdirsFailsForSubdirectoryOfExistingFile;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileTwice;id_000000;[file.bytes-per-checksum=1276441134];java.lang.NegativeArraySizeException;-1396931682;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileTwice;id_000001;[file.stream-buffer-size=1163801818];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testMkdirOverParentFile;id_000000;[file.stream-buffer-size=1281269691, io.file.buffer.size=1757459741];java.lang.NegativeArraySizeException;-1315047941;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testMkdirOverParentFile;id_000002;[file.bytes-per-checksum=1093533411];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractMkdir#testMkdirOverParentFile;id_000004;[file.stream-buffer-size=2012632819, io.file.buffer.size=514635264];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir#testMkDirRmRfDir;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.ftp.TestFTPFileSystem#testCreateWithWritePermissions;id_000000;[fs.ftp.data.connection.mode=PASSIVE_REMOTE_DATA_CONNECTION_MODE];java.io.IOException;Unable to create file: test1.txt, Aborting;[org.apache.hadoop.fs.ftp.FTPFileSystem.create(FTPFileSystem.java:356), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)]
org.apache.hadoop.security.alias.TestCredShell#testCredentialSuccessfulLifecycle;id_000000;[io.file.buffer.size=2089278316];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.alias.TestCredShell#testCredentialSuccessfulLifecycle;id_000002;[io.file.buffer.size=2089278237];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithAnUnsupportedVersion;id_000000;[io.file.buffer.size=1539973090];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithAnUnsupportedVersion;id_000001;[io.file.buffer.size=1250138801];java.lang.NegativeArraySizeException;-1397107322;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithAnUnsupportedVersion;id_000002;[fs.har.metadatacache.entries=1384616248];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithAnUnsupportedVersion;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestHarFileSystemBasics#testNegativeInitWithAnUnsupportedVersion;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.ipc.TestServer#testLogExceptions;id_000000;[ipc.server.read.threadpool.size=649967866];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.Server.<init>(Server.java:3046), org.apache.hadoop.ipc.TestServer$1.<init>(TestServer.java:149), org.apache.hadoop.ipc.TestServer.testLogExceptions(TestServer.java:149)]
org.apache.hadoop.io.TestSequenceFile#testSequenceFileMetadata;id_000000;[io.file.buffer.size=1863967192];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testSequenceFileMetadata;id_000001;[io.file.buffer.size=1762487049];java.lang.NegativeArraySizeException;-862604990;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFile#testSequenceFileMetadata;id_000002;[file.bytes-per-checksum=1481631912];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.io.SequenceFile$Reader.openFile(SequenceFile.java:1962), org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1882)]
org.apache.hadoop.io.TestSequenceFile#testSequenceFileMetadata;id_000003;[file.bytes-per-checksum=2128906688];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.TestSequenceFile#testSequenceFileMetadata;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFile#testSequenceFileMetadata;id_000006;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.http.TestHttpServerWebapps#testValidServerResource;id_000000;[hadoop.http.selector.count=1133943515];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServerWebapps#testValidServerResource;id_000001;[hadoop.http.selector.count=944670185];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestHttpServerWebapps#testValidServerResource;id_000002;[hadoop.http.acceptor.count=834159168];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFileSystemCaching#testCacheDisabled;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFileSystemCaching#testCloseAllForUGI;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.http.TestHttpServerLifecycle#testCreatedServerIsNotAlive;id_000000;[hadoop.http.selector.count=1168758760];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStopUnstartedServer;id_000000;[hadoop.http.selector.count=926696018];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStopUnstartedServer;id_000001;[hadoop.http.selector.count=1740449821];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStopUnstartedServer;id_000002;[hadoop.http.selector.count=709020928];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.http.TestHttpServerLifecycle#testStopUnstartedServer;id_000003;[hadoop.http.max.threads=0];java.lang.IllegalStateException;Insufficient configured threads: required=0 < max=0 for QueuedThreadPool[qtp1584938872]@5e783f78{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.QueuedThreadPool.setMaxThreads(QueuedThreadPool.java:364), org.apache.hadoop.http.HttpServer2.initializeWebServer(HttpServer2.java:703), org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:687), org.apache.hadoop.http.HttpServer2.<init>(HttpServer2.java:129)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStartedServerIsAlive;id_000000;[hadoop.http.selector.count=1404881691];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStartedServerIsAlive;id_000001;[hadoop.http.selector.count=1240046432];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStartedServerIsAlive;id_000002;[hadoop.http.selector.count=546264293];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.http.TestHttpServerLifecycle#testStoppingTwiceServerIsAllowed;id_000000;[hadoop.http.selector.count=1887881400];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStoppedServerIsNotAlive;id_000000;[hadoop.http.selector.count=1860284689];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServerLifecycle#testStoppedServerIsNotAlive;id_000002;[hadoop.http.selector.count=130128309];java.lang.IllegalStateException;Insufficient configured threads: required=264754447 < max=106702977 for QueuedThreadPool[qtp1652216902]@627ad446{STARTED,8<=8<=106702977,i=8,r=-1,q=0}[ReservedThreadExecutor@158f0af4{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriterNotClosed;id_000000;[io.file.buffer.size=1438036058];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriterNotClosed;id_000001;[file.stream-buffer-size=517276418, io.file.buffer.size=606348028];java.lang.NegativeArraySizeException;-349187363;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriterNotClosed;id_000003;[io.file.buffer.size=2078727585];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureWriterNotClosed;id_000004;[file.bytes-per-checksum=1631939334];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenFileFailExceptionally;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testFileStatusBlocksizeNonEmptyFile;id_000000;[io.file.buffer.size=2108509742];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractCreate#testFileStatusBlocksizeNonEmptyFile;id_000002;[fs.local.block.size=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileLazyFail;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.util.TestGenericOptionsParser#testNullArgs;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSourceIsFileAndDelSrcTrue;id_000000;[io.file.buffer.size=1431603721];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsEmptyDirectory;id_000000;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=236277478, hadoop.service.shutdown.timeout=090132s, fs.permissions.umask-mode=770, file.stream-buffer-size=287462795, hadoop.security.groups.negative-cache.secs=1578301832, fs.local.block.size=1739753902, fs.file.impl.disable.cache=true];java.lang.NegativeArraySizeException;-1299437526;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsEmptyDirectory;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=623902810, hadoop.security.groups.cache.secs=778901585, hadoop.service.shutdown.timeout=844673s, hadoop.security.groups.cache.background.reload.threads=503624808, fs.local.block.size=286167882, hadoop.kerberos.min.seconds.before.relogin=1618260215, hadoop.kerberos.keytab.login.autorenewal.enabled=true, fs.permissions.umask-mode=500, hadoop.security.groups.negative-cache.secs=276228255, fs.creation.parallel.count=1136787575, file.bytes-per-checksum=1921121687, hadoop.security.groups.cache.warn.after.ms=1469179609];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsEmptyDirectory;id_000004;[file.stream-buffer-size=2083746372];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsEmptyDirectory;id_000005;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsFile;id_000000;[file.stream-buffer-size=2093188875];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsFile;id_000002;[file.bytes-per-checksum=1778689668];java.lang.NegativeArraySizeException;-1171662172;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testRenameDirectoryAsFile;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestPath#testGlobEscapeStatus;id_000000;[io.file.buffer.size=2079500179];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestPath#testGlobEscapeStatus;id_000001;[file.bytes-per-checksum=395669373];java.lang.NegativeArraySizeException;-733942939;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestPath#testGlobEscapeStatus;id_000002;[hadoop.security.dns.log-slow-lookups.threshold.ms=471341871, hadoop.kerberos.min.seconds.before.relogin=892852272, hadoop.security.groups.cache.secs=1029870404, hadoop.service.shutdown.timeout=626142s, fs.permissions.umask-mode=750, file.stream-buffer-size=1182658740, hadoop.security.groups.cache.background.reload.threads=2086784738, fs.local.block.size=1070604918, fs.creation.parallel.count=1913072792, fs.automatic.close=true, hadoop.security.groups.cache.warn.after.ms=1947406464];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestPath#testGlobEscapeStatus;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestPath#testGlobEscapeStatus;id_000005;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameWithNonEmptySubDir;id_000000;[file.bytes-per-checksum=1109082555];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameWithNonEmptySubDir;id_000002;[file.stream-buffer-size=1794078468, io.file.buffer.size=392111780];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameWithNonEmptySubDir;id_000004;[file.bytes-per-checksum=1866305856];java.lang.NegativeArraySizeException;-383116480;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameWithNonEmptySubDir;id_000006;[hadoop.security.dns.log-slow-lookups.threshold.ms=326470146, hadoop.security.groups.cache.secs=924016507, hadoop.service.shutdown.timeout=16h, file.stream-buffer-size=1305052399, hadoop.security.groups.cache.background.reload.threads=1428731671, fs.local.block.size=0, fs.file.impl.disable.cache=true, io.file.buffer.size=14937, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1748757311, fs.permissions.umask-mode=775, hadoop.security.groups.negative-cache.secs=754057035, fs.creation.parallel.count=1248124778, hadoop.security.groups.cache.warn.after.ms=446084606];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.http.TestHttpServer#testHttpResonseContainsXFrameOptions;id_000000;[hadoop.http.selector.count=560117619];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServer#testHttpResonseDoesNotContainXFrameOptions;id_000000;[hadoop.http.selector.count=2053762407];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServer#testHttpResonseContainsAllowFrom;id_000000;[hadoop.http.selector.count=265568186];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServer#testHttpResponseDefaultHeaders;id_000000;[hadoop.http.selector.count=256662724];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServer#testBacklogSize;id_000000;[hadoop.http.selector.count=2049770134];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestHttpServer#testIdleTimeout;id_000000;[hadoop.http.selector.count=616080658];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServerLogs#testLogsDisabled;id_000000;[hadoop.http.selector.count=848337738];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestGlobalFilter#testServletFilter;id_000000;[hadoop.http.selector.count=178407142];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.ipc.TestIPC#testIpcConnectTimeout;id_000000;[ipc.server.read.threadpool.size=1784894382];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.Server.<init>(Server.java:3046), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:226), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:219)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetupWithNoKeyStorePassword;id_000000;[hadoop.http.acceptor.count=1896539180];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetupWithoutKeyStoreKeyPassword;id_000000;[hadoop.http.selector.count=1047377564];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetupWithoutKeyStoreKeyPassword;id_000001;[hadoop.http.selector.count=1629338238];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetupWithoutKeyStoreKeyPassword;id_000002;[hadoop.http.selector.count=262181518];java.lang.IllegalStateException;Insufficient configured threads: required=114371767 < max=10 for QueuedThreadPool[qtp1453847935]@56a7f57f{STARTED,8<=8<=10,i=8,r=-1,q=0}[ReservedThreadExecutor@7942987{s=0/1,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]
org.apache.hadoop.http.TestSSLHttpServerConfigs#testServerSetupWithWrongKeyStorePassword;id_000000;[hadoop.http.selector.count=1387973886];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.ipc.TestRPC#testClientBackOff;id_000000;[ipc.server.read.threadpool.size=1320712011];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testTargetFileSystemLazyInitializationForChecksumMethods;id_000000;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testTargetFileSystemLazyInitializationForChecksumMethods;id_000001;[fs.viewfs.mounttable.default.name.key=default];java.lang.NegativeArraySizeException;-346538002;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testTargetFileSystemLazyInitializationForChecksumMethods;id_000003;[fs.viewfs.mounttable.default.name.key=default];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFileSystemLocalFileSystem#testTargetFileSystemLazyInitializationForChecksumMethods;id_000014;[fs.viewfs.mounttable.default.name.key=default];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]
org.apache.hadoop.io.compress.TestCodec#testDeflateCodec;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:42), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.io.compress.TestCodec.codecTest(TestCodec.java:201)]
org.apache.hadoop.io.compress.TestCodec#testDeflateCodec;id_000001;[io.file.buffer.size=2102179254];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.io.compress.TestCodec.codecTest(TestCodec.java:201)]
org.apache.hadoop.io.compress.TestCodec#testDeflateCodec;id_000004;[io.file.buffer.size=1963012634];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.compress.TestCodec#testDefaultCodec;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Illegal bufferSize;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:42), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.io.compress.TestCodec.codecTest(TestCodec.java:201)]
org.apache.hadoop.io.compress.TestCodec#testDefaultCodec;id_000001;[io.file.buffer.size=2097397925];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.io.compress.TestCodec.codecTest(TestCodec.java:201)]
org.apache.hadoop.io.compress.TestCodec#testDefaultCodec;id_000002;[io.file.buffer.size=2026122587];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:64), org.apache.hadoop.io.compress.DecompressorStream.<init>(DecompressorStream.java:71), org.apache.hadoop.io.compress.DefaultCodec.createInputStream(DefaultCodec.java:92), org.apache.hadoop.io.compress.CompressionCodec$Util.createInputStreamWithCodecPool(CompressionCodec.java:160), org.apache.hadoop.io.compress.DefaultCodec.createInputStream(DefaultCodec.java:84)]
org.apache.hadoop.io.compress.TestCodec#testDefaultCodec;id_000004;[io.file.buffer.size=2087557470];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testTwoBlocks;id_000000;[io.file.buffer.size=1561075310, tfile.fs.output.buffer.size=1565472386];java.lang.NegativeArraySizeException;-334984035;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testTwoBlocks;id_000002;[file.bytes-per-checksum=1145027251];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testTwoBlocks;id_000005;[file.bytes-per-checksum=1010778946];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:431), org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642), org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testOneDataEntry;id_000000;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testOneDataEntry;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=1401060283, hadoop.security.auth_to_local.mechanism=MIT, hadoop.kerberos.min.seconds.before.relogin=980485440, hadoop.service.shutdown.timeout=10883037h, file.stream-buffer-size=681606950, hadoop.security.groups.negative-cache.secs=80168065, fs.local.block.size=1994587702, fs.creation.parallel.count=2026206438, fs.file.impl.disable.cache=true, io.file.buffer.size=1096463271, fs.automatic.close=false, hadoop.security.groups.cache.warn.after.ms=1752816962];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testOneDataEntry;id_000002;[file.bytes-per-checksum=966279620];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:431), org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642), org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testOneDataEntry;id_000003;[file.bytes-per-checksum=1683371154];java.lang.NegativeArraySizeException;-2029528798;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testOneDataEntry;id_000004;[hadoop.service.shutdown.timeout=8m, file.stream-buffer-size=681607974, fs.local.block.size=1141982666, fs.creation.parallel.count=648358626, fs.file.impl.disable.cache=true, io.file.buffer.size=1096481303, fs.automatic.close=false];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testOneBlockPlusOneEntry;id_000000;[file.bytes-per-checksum=851560986];java.lang.NegativeArraySizeException;-925885718;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testOneBlockPlusOneEntry;id_000001;[io.file.buffer.size=532443589, tfile.fs.output.buffer.size=1744046314];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testOneBlockPlusOneEntry;id_000003;[io.file.buffer.size=2101016065];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testOneBlockPlusOneEntry;id_000004;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.security.TestLdapGroupsMapping#testConfGetPassword;id_000000;[file.bytes-per-checksum=783640206];java.lang.NegativeArraySizeException;-1537172738;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.TestLdapGroupsMapping#testConfGetPassword;id_000001;[io.file.buffer.size=1093648640, file.bytes-per-checksum=598034422];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.TestLdapGroupsMapping#testConfGetPassword;id_000002;[hadoop.security.dns.log-slow-lookups.threshold.ms=1832234864, fs.permissions.umask-mode=770, file.stream-buffer-size=547831428, hadoop.security.groups.negative-cache.secs=923849289, fs.local.block.size=1800878591, fs.file.impl.disable.cache=true, io.file.buffer.size=886413200, hadoop.security.groups.cache.warn.after.ms=1272666542];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.TestLdapGroupsMapping#testConfGetPassword;id_000003;[file.bytes-per-checksum=1506975823];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.TestSequenceFileSync#testDefaultSyncInterval;id_000000;[file.bytes-per-checksum=925371850];java.lang.NegativeArraySizeException;-261587942;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.TestSequenceFileSync#testDefaultSyncInterval;id_000001;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFileSync#testDefaultSyncInterval;id_000002;[io.file.buffer.size=2134876796];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.TestSequenceFileSync#testDefaultSyncInterval;id_000004;[file.bytes-per-checksum=1090330688];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.security.TestUserGroupInformation#testLogin;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]
org.apache.hadoop.io.file.tfile.TestTFile#testMetaBlocks;id_000000;[file.bytes-per-checksum=983352221];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFile#testMetaBlocks;id_000001;[file.bytes-per-checksum=1395649734];java.lang.NegativeArraySizeException;-324054282;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFile#testUnsortedTFileFeatures;id_000000;[file.bytes-per-checksum=2079358964];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFile#testUnsortedTFileFeatures;id_000002;[hadoop.kerberos.min.seconds.before.relogin=1700381795, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.service.shutdown.timeout=40s, io.file.buffer.size=4096, tfile.io.chunk.size=1651617104];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFile#testUnsortedTFileFeatures;id_000003;[file.stream-buffer-size=2061529112];java.lang.NegativeArraySizeException;-812941204;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneBlock;id_000000;[io.file.buffer.size=2091500172];java.lang.NegativeArraySizeException;-1580924465;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneBlock;id_000001;[io.file.buffer.size=582854494, tfile.fs.output.buffer.size=512021528, file.bytes-per-checksum=101601477];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneBlock;id_000002;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneBlock;id_000004;[io.file.buffer.size=2126069245];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneBlock;id_000005;[file.bytes-per-checksum=1499498894];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testTwoBlocks;id_000000;[io.file.buffer.size=1796769357, tfile.fs.output.buffer.size=1011387082];java.lang.NegativeArraySizeException;-400448186;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testTwoBlocks;id_000001;[io.file.buffer.size=2141797550];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testTwoBlocks;id_000004;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureCompressionNotWorking;id_000000;[file.bytes-per-checksum=1065226901];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureCompressionNotWorking;id_000001;[tfile.fs.output.buffer.size=169167363, file.bytes-per-checksum=1037714420];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureCompressionNotWorking;id_000002;[file.bytes-per-checksum=844850376];java.lang.NegativeArraySizeException;-986281208;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneDataEntry;id_000000;[tfile.fs.input.buffer.size=1531077906, hadoop.service.shutdown.timeout=892h, tfile.fs.output.buffer.size=1786604239, io.file.buffer.size=4096, hadoop.security.groups.cache.warn.after.ms=1822544556];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneDataEntry;id_000002;[file.bytes-per-checksum=382709689];java.lang.NegativeArraySizeException;-850580095;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testOneDataEntry;id_000003;[file.stream-buffer-size=1065361275, io.file.buffer.size=1933117101];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testOneBlockPlusOneEntry;id_000000;[file.bytes-per-checksum=1252754801];java.lang.NegativeArraySizeException;-1610108679;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testOneBlockPlusOneEntry;id_000002;[tfile.fs.input.buffer.size=795644149, file.bytes-per-checksum=547167863];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureGetNonExistentMetaBlock;id_000000;[io.file.buffer.size=1544298436];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureGetNonExistentMetaBlock;id_000001;[file.bytes-per-checksum=290081068];java.lang.NegativeArraySizeException;-1684237684;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureGetNonExistentMetaBlock;id_000003;[file.bytes-per-checksum=603321711];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testTwoBlocks;id_000000;[hadoop.security.token.service.use_ip=true, hadoop.security.dns.log-slow-lookups.threshold.ms=1102253854, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.kerberos.min.seconds.before.relogin=2023397712, fs.permissions.umask-mode=500, file.stream-buffer-size=803407730, hadoop.security.groups.cache.background.reload.threads=2133259202, fs.local.block.size=1399597340, fs.file.impl.disable.cache=true, io.file.buffer.size=492142050];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testTwoBlocks;id_000002;[file.bytes-per-checksum=1312313922];java.lang.NegativeArraySizeException;-1074076590;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testTwoBlocks;id_000003;[tfile.fs.output.buffer.size=0];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testTwoDataEntries;id_000000;[file.bytes-per-checksum=1617113522];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testTwoDataEntries;id_000001;[file.bytes-per-checksum=972873418];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testTwoDataEntries;id_000002;[file.bytes-per-checksum=254396408];java.lang.NegativeArraySizeException;-2005399624;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testTwoDataEntries;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureCompressionNotWorking;id_000000;[file.stream-buffer-size=406336173, io.file.buffer.size=1537170019, tfile.fs.output.buffer.size=215664646];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureCompressionNotWorking;id_000001;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureCompressionNotWorking;id_000002;[file.bytes-per-checksum=1215938260];java.lang.NegativeArraySizeException;-1941457548;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileJClassComparatorByteArrays#testFailureCompressionNotWorking;id_000004;[file.bytes-per-checksum=1650861367];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureNegativeOffset;id_000000;[tfile.fs.output.buffer.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(duration must be positive: 0 MILLISECONDS),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureNegativeOffset;id_000001;[file.stream-buffer-size=1529787733, tfile.fs.output.buffer.size=729176318];java.lang.NegativeArraySizeException;-1381468863;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureNegativeOffset;id_000002;[tfile.fs.output.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureNegativeOffset;id_000006;[tfile.fs.output.buffer.size=1699563153, file.bytes-per-checksum=497482620];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testTwoEntriesKnownLength;id_000000;[file.bytes-per-checksum=232840736];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testTwoEntriesKnownLength;id_000001;[file.bytes-per-checksum=317766910];java.lang.NegativeArraySizeException;-1435065106;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testTwoEntriesKnownLength;id_000002;[file.stream-buffer-size=1397107163, io.file.buffer.size=1096421803];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testTwoEntriesKnownLength;id_000007;[file.bytes-per-checksum=1459631177];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong7Bytes;id_000000;[file.bytes-per-checksum=444699421];java.lang.NegativeArraySizeException;-292672507;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong7Bytes;id_000001;[file.bytes-per-checksum=1081045083];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong7Bytes;id_000002;[file.bytes-per-checksum=965678784];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong7Bytes;id_000004;[io.file.buffer.size=1264800596, file.bytes-per-checksum=145263263];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong3Bytes;id_000000;[file.bytes-per-checksum=253248058];java.lang.NegativeArraySizeException;-2015734774;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong3Bytes;id_000001;[file.bytes-per-checksum=1661732782];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong3Bytes;id_000002;[file.bytes-per-checksum=1974042535];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong3Bytes;id_000003;[io.file.buffer.size=279809040, file.bytes-per-checksum=594024615];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong4Bytes;id_000000;[file.bytes-per-checksum=1707897680];java.lang.NegativeArraySizeException;-1808790064;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong4Bytes;id_000001;[file.stream-buffer-size=1612408964, file.bytes-per-checksum=68659536];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong4Bytes;id_000003;[file.stream-buffer-size=1612394421, file.bytes-per-checksum=60336464];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong5Bytes;id_000000;[fs.automatic.close=true, hadoop.security.dns.log-slow-lookups.enabled=true, io.file.buffer.size=1440691531, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.security.groups.cache.warn.after.ms=1519449972];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong5Bytes;id_000001;[file.bytes-per-checksum=1888575291];java.lang.NegativeArraySizeException;-182691565;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestVLong#testVLong5Bytes;id_000003;[file.bytes-per-checksum=213616372];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileComparator2#testSortedLongWritable;id_000000;[io.file.buffer.size=1392913750];java.lang.NegativeArraySizeException;-1990066822;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileComparator2#testSortedLongWritable;id_000001;[tfile.io.chunk.size=1192718771, tfile.fs.output.buffer.size=825169625];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileComparator2#testSortedLongWritable;id_000004;[file.bytes-per-checksum=2126669849];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testResolvePathDanglingLink;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1258540507, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/iY5tEDPGd0/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/iY5tEDPGd0, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/iY5tEDPGd0/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/iY5tEDPGd0/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/iY5tEDPGd0/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=23326284, fs.creation.parallel.count=1756784549, fs.viewfs.rename.strategy=SAME_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=335005287, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/iY5tEDPGd0/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1913758551, fs.local.block.size=421252620, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=101672664, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/iY5tEDPGd0/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=700, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/iY5tEDPGd0/user, hadoop.security.groups.cache.warn.after.ms=1471544871];java.io.FileNotFoundException;File /mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/4gnwQjJrpP/missingTarget does not exist;[org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128), org.apache.hadoop.fs.FilterFs.getFileStatus(FilterFs.java:124)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalDeleteExisting2;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1718263467, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/GoKmfmQ6aw/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/GoKmfmQ6aw, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/GoKmfmQ6aw/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/GoKmfmQ6aw/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/GoKmfmQ6aw/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=541733075, fs.creation.parallel.count=1086343117, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1704338679, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/GoKmfmQ6aw/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1741620657, fs.local.block.size=2072351230, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=700261809, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/GoKmfmQ6aw/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=770, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/GoKmfmQ6aw/user, hadoop.security.groups.cache.warn.after.ms=1825014615];org.apache.hadoop.security.AccessControlException;Cannot delete internal mount table directory: /internalDir/linkToDir2;[org.apache.hadoop.fs.viewfs.ViewFs.delete(ViewFs.java:372), org.apache.hadoop.fs.FileContext$5.next(FileContext.java:845), org.apache.hadoop.fs.FileContext$5.next(FileContext.java:841), org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90), org.apache.hadoop.fs.FileContext.delete(FileContext.java:847)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalMkdirSlash;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=468011684, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/gZwyAZpyJH/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/gZwyAZpyJH, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/gZwyAZpyJH/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/gZwyAZpyJH/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/gZwyAZpyJH/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1689031712, fs.creation.parallel.count=930914870, fs.viewfs.rename.strategy=SAME_TARGET_URI_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1882323496, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/gZwyAZpyJH/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=970101539, fs.local.block.size=1369192639, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=916495121, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/gZwyAZpyJH/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=755, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/gZwyAZpyJH/user, hadoop.security.groups.cache.warn.after.ms=1962931846];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.fs.viewfs.TestViewFsWithAuthorityLocalFs#testInternalCreate1;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1725836351, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/yZSGXrkfa3/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/yZSGXrkfa3, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/yZSGXrkfa3/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/yZSGXrkfa3/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/yZSGXrkfa3/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=551770704, fs.creation.parallel.count=1017804486, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=873245101, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/yZSGXrkfa3/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=57869496, fs.local.block.size=1080528387, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1904003408, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/yZSGXrkfa3/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=027, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/yZSGXrkfa3/user, hadoop.security.groups.cache.warn.after.ms=686724702];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path /foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.createInternal(ViewFs.java:1028), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:358), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalCreateMissingDir2;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=419769707, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bKQFU6Qdyb/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bKQFU6Qdyb, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bKQFU6Qdyb/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bKQFU6Qdyb/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bKQFU6Qdyb/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=671067779, fs.creation.parallel.count=719190995, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=1296293969, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bKQFU6Qdyb/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1193211416, fs.local.block.size=2018699206, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1086169806, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bKQFU6Qdyb/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=077, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/bKQFU6Qdyb/user, hadoop.security.groups.cache.warn.after.ms=133367104];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path viewfs:/missingDir/miss2/foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:352), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalCreateMissingDir3;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=1943767065, fs.client.resolve.remote.symlinks=false, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/8LPGhoRn6c/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/8LPGhoRn6c, fs.viewfs.mount.links.as.symlinks=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/8LPGhoRn6c/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/8LPGhoRn6c/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/8LPGhoRn6c/data, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=744921806, fs.creation.parallel.count=390050296, fs.viewfs.rename.strategy=SAME_FILESYSTEM_ACROSS_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=534145636, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/8LPGhoRn6c/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1895354389, fs.local.block.size=1697828089, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1340364175, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/8LPGhoRn6c/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=022, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/8LPGhoRn6c/user, hadoop.security.groups.cache.warn.after.ms=51494618];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation create not permitted on path viewfs:/internalDir/miss2/foo.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs.createInternal(ViewFs.java:352), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]
org.apache.hadoop.fs.viewfs.TestViewFsLocalFs#testInternalMkdirNew;id_000000;[hadoop.security.auth_to_local.mechanism=MIT, hadoop.security.groups.cache.secs=999832534, fs.client.resolve.remote.symlinks=true, fs.viewfs.mounttable.mycluster.link./internalDir/internalDir2/linkToDir3=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/LYZ80IJiO0/dir3, fs.viewfs.mounttable.mycluster.link./targetRoot=file:///mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/LYZ80IJiO0, fs.viewfs.mount.links.as.symlinks=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, fs.viewfs.mounttable.mycluster.link./internalDir/linkToDir2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/LYZ80IJiO0/dir2, hadoop.user.group.static.mapping.overrides=dr.who=;, fs.viewfs.mounttable.mycluster.link./linkToAFile=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/LYZ80IJiO0/aFile, fs.viewfs.mounttable.mycluster.link./data=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/LYZ80IJiO0/data, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=1114869503, fs.creation.parallel.count=937023094, fs.viewfs.rename.strategy=SAME_MOUNTPOINT, hadoop.security.dns.log-slow-lookups.threshold.ms=232698419, fs.viewfs.mounttable.mycluster.link./danglingLink=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/LYZ80IJiO0/missingTarget, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=228875120, fs.local.block.size=1293731270, fs.AbstractFileSystem.file.impl=org.apache.hadoop.fs.local.LocalFs, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1111964137, fs.viewfs.mounttable.mycluster.link./user2=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/LYZ80IJiO0/user, fs.viewfs.mounttable.default.name.key=mycluster, fs.permissions.umask-mode=277, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, fs.AbstractFileSystem.viewfs.impl=org.apache.hadoop.fs.viewfs.ViewFs, fs.viewfs.mounttable.mycluster.link./user=file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task103/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/LYZ80IJiO0/user, hadoop.security.groups.cache.warn.after.ms=1130096141];org.apache.hadoop.security.AccessControlException;InternalDir of ViewFileSystem is readonly, operation mkdir not permitted on path /dirNew.;[org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:177), org.apache.hadoop.fs.viewfs.ViewFs.readOnlyMountTable(ViewFs.java:183), org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs.mkdir(ViewFs.java:1287), org.apache.hadoop.fs.viewfs.ViewFs.mkdir(ViewFs.java:545), org.apache.hadoop.fs.FileContext$4.next(FileContext.java:809)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureFileWriteNotAt0Position;id_000000;[file.bytes-per-checksum=2084493040];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureFileWriteNotAt0Position;id_000001;[file.bytes-per-checksum=840035565];java.lang.NegativeArraySizeException;-1029614507;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureFileWriteNotAt0Position;id_000003;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOutOfOrderKeys;id_000000;[io.file.buffer.size=1974938081, tfile.fs.output.buffer.size=197145246];java.lang.NegativeArraySizeException;-305280233;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOutOfOrderKeys;id_000001;[file.bytes-per-checksum=220100654];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOutOfOrderKeys;id_000002;[file.bytes-per-checksum=234981330];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOutOfOrderKeys;id_000003;[hadoop.kerberos.min.seconds.before.relogin=2073160497, hadoop.security.groups.cache.secs=990407999, hadoop.service.shutdown.timeout=485122m, fs.permissions.umask-mode=666, file.stream-buffer-size=384165399, hadoop.security.groups.negative-cache.secs=908500573, fs.local.block.size=1076217108, fs.creation.parallel.count=270095679, fs.file.impl.disable.cache=true, io.file.buffer.size=1032711227];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOutOfOrderKeys;id_000028;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsByteArrays#testFailureOutOfOrderKeys;id_000029;[io.file.buffer.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListFilesFileRecursive;id_000000;[io.file.buffer.size=2078978731];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListFilesFileRecursive;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListFilesFileRecursive;id_000010;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListFilesEmptyDirectoryNonrecursive;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFile;id_000001;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListStatusFile;id_000003;[io.file.buffer.size=2119580219];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListFilesFile;id_000000;[io.file.buffer.size=2100401731];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testSeekReadClosedFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListLocatedStatusFile;id_000000;[file.bytes-per-checksum=2081009256];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListLocatedStatusFile;id_000001;[file.bytes-per-checksum=2146682773];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractGetFileStatus#testListLocatedStatusFile;id_000002;[file.bytes-per-checksum=899951590];java.lang.NegativeArraySizeException;-490370282;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekZeroByteFile;id_000000;[file.bytes-per-checksum=1388543671];java.lang.NegativeArraySizeException;-388008849;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekZeroByteFile;id_000002;[file.bytes-per-checksum=2025832549];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekZeroByteFile;id_000006;[file.bytes-per-checksum=2051010993];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekAndReadPastEndOfFile;id_000000;[file.bytes-per-checksum=2016325215];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekAndReadPastEndOfFile;id_000001;[file.bytes-per-checksum=785491185];java.lang.NegativeArraySizeException;-1520513927;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testSeekAndReadPastEndOfFile;id_000041;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testBlockReadZeroByteFile;id_000000;[file.bytes-per-checksum=309970342];java.lang.NegativeArraySizeException;-1505234218;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testBlockReadZeroByteFile;id_000001;[file.bytes-per-checksum=2138180763];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testURIEmptyPath;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testRename;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=606482917, hadoop.kerberos.min.seconds.before.relogin=1856504035, hadoop.security.groups.cache.secs=473496233, fs.permissions.umask-mode=660, file.stream-buffer-size=1647840121, hadoop.security.groups.negative-cache.secs=1359366292, hadoop.security.groups.cache.warn.after.ms=77454964];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testRename;id_000002;[file.bytes-per-checksum=239173085];java.lang.NegativeArraySizeException;-2142409531;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:197)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testRename;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameNewFileSameDir;id_000000;[file.bytes-per-checksum=1241639082];java.lang.NegativeArraySizeException;-1710150150;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameNewFileSameDir;id_000001;[file.stream-buffer-size=1568993583, io.file.buffer.size=1300456011];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractMkdir#testMkdirsPopulatingAllNonexistentAncestors;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameWithNonEmptySubDirPOSIX;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameWithNonEmptySubDirPOSIX;id_000001;[io.file.buffer.size=2077846921];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameWithNonEmptySubDirPOSIX;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyTreeDirectoryWithoutDelete;id_000000;[io.file.buffer.size=1595549001];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyTreeDirectoryWithoutDelete;id_000002;[io.file.buffer.size=1655398028];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.security.TestGroupsCaching#testNegativeCacheEntriesExpire;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.TestGroupsCaching.testNegativeCacheEntriesExpire(TestGroupsCaching.java:877), org.apache.hadoop.security.TestGroupsCaching.testNegativeCacheEntriesExpire$$CONFUZZ(TestGroupsCaching.java)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenFileTwice;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWDAbsolute;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteHalfABlock;id_000000;[file.bytes-per-checksum=570395307];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteHalfABlock;id_000001;[file.bytes-per-checksum=1421360930];java.lang.NegativeArraySizeException;-92653518;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteHalfABlock;id_000002;[file.bytes-per-checksum=683181071];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteHalfABlock;id_000003;[file.stream-buffer-size=1994482547];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteHalfABlock;id_000005;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteHalfABlock;id_000006;[file.stream-buffer-size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:153)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testWriteReadAndDeleteHalfABlock;id_000007;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:207), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.TestFileSystemCaching#testCacheDualSemaphoreConstruction;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileApplyAsyncRead;id_000000;[file.bytes-per-checksum=286370628];java.lang.NegativeArraySizeException;-1717631644;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileApplyAsyncRead;id_000001;[hadoop.security.groups.cache.secs=1942225621, hadoop.service.shutdown.timeout=30s, fs.permissions.umask-mode=666, file.stream-buffer-size=1583733707, hadoop.security.groups.negative-cache.secs=1469831121, hadoop.security.groups.cache.background.reload.threads=890281551, fs.local.block.size=1267746148, hadoop.security.groups.cache.background.reload=false, fs.file.impl.disable.cache=true];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$134/0x0000000840194440.call(Unknown Source)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileApplyAsyncRead;id_000002;[file.bytes-per-checksum=1640210251];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileApplyAsyncRead;id_000004;[file.stream-buffer-size=1587896960, file.bytes-per-checksum=529332121];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileUnderFile;id_000000;[file.bytes-per-checksum=713845804];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileUnderFile;id_000001;[file.bytes-per-checksum=1860759287];java.lang.NegativeArraySizeException;-433035601;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileUnderFile;id_000002;[file.bytes-per-checksum=2021535903];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractRename#testRenameFileUnderFile;id_000003;[file.bytes-per-checksum=1595501053];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameFileUnderFileSubdir;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameFileUnderFileSubdir;id_000002;[io.file.buffer.size=2132289768];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameFileUnderFileSubdir;id_000004;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameNewFileSameDir;id_000000;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameNewFileSameDir;id_000001;[io.file.buffer.size=2082257931];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractRename#testRenameNewFileSameDir;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testCopyEmptyFile;id_000000;[io.file.buffer.size=1098514197];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSrcIsDirWithFilesAndCopySuccessful;id_000000;[io.file.buffer.size=1966600780];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSrcIsDirWithFilesAndCopySuccessful;id_000002;[io.file.buffer.size=2145972812];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriteRecordAfterMetaBlock;id_000000;[tfile.fs.output.buffer.size=1328878235, io.file.buffer.size=1743117486];java.lang.NegativeArraySizeException;-384051748;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriteRecordAfterMetaBlock;id_000001;[tfile.fs.output.buffer.size=0];java.lang.ArrayIndexOutOfBoundsException;Index 0 out of bounds for length 0;[org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream.write(SimpleBufferedOutputStream.java:50), java.io.DataOutputStream.writeByte(DataOutputStream.java:153), org.apache.hadoop.io.file.tfile.Utils.writeVLong(Utils.java:103), org.apache.hadoop.io.file.tfile.Utils.writeVInt(Utils.java:56), org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister.close(TFile.java:450)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriteRecordAfterMetaBlock;id_000002;[file.bytes-per-checksum=713535776];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.io.BytesWritable.setCapacity(BytesWritable.java:146), org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState.<init>(BCFile.java:125), org.apache.hadoop.io.file.tfile.BCFile$Writer.prepareDataBlock(BCFile.java:431), org.apache.hadoop.io.file.tfile.TFile$Writer.initDataBlock(TFile.java:642), org.apache.hadoop.io.file.tfile.TFile$Writer.prepareAppendKey(TFile.java:533)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureWriteRecordAfterMetaBlock;id_000005;[file.bytes-per-checksum=1146592470];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOutOfOrderKeys;id_000000;[file.bytes-per-checksum=124854909, tfile.fs.output.buffer.size=1867424850];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOutOfOrderKeys;id_000001;[file.bytes-per-checksum=1563996229];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOutOfOrderKeys;id_000002;[file.stream-buffer-size=322434681, io.file.buffer.size=1095031236, file.bytes-per-checksum=80188614];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOutOfOrderKeys;id_000003;[file.bytes-per-checksum=256589063];java.lang.NegativeArraySizeException;-1985665729;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOutOfOrderKeys;id_000007;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsJClassComparatorByteArrays#testFailureOutOfOrderKeys;id_000008;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureAddKeyWithoutValue;id_000000;[io.file.buffer.size=2135856479];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureAddKeyWithoutValue;id_000001;[file.bytes-per-checksum=2099945104];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureAddKeyWithoutValue;id_000002;[file.stream-buffer-size=1875145539, io.file.buffer.size=2011656282];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileStreams#testFailureAddKeyWithoutValue;id_000003;[file.bytes-per-checksum=348976772];java.lang.NegativeArraySizeException;-1154176348;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListLocatedStatusFiltering;id_000000;[io.file.buffer.size=2094366586];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractGetFileStatus#testListLocatedStatusFiltering;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=747243477, hadoop.kerberos.min.seconds.before.relogin=2010398882, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.service.shutdown.timeout=087639m, fs.permissions.umask-mode=777, hadoop.security.groups.cache.background.reload.threads=8781867, fs.local.block.size=623439104, hadoop.security.groups.cache.background.reload=true, fs.creation.parallel.count=116064256, fs.file.impl.disable.cache=true, io.file.buffer.size=1475880729];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testSeekAndReadPastEndOfFile;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;Invalid value of fs.creation.parallel.count: 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:415), org.apache.hadoop.fs.FileSystem$Cache.<init>(FileSystem.java:3514), org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:206), org.apache.hadoop.fs.contract.rawlocal.RawlocalFSContract.getLocalFS(RawlocalFSContract.java:46), org.apache.hadoop.fs.contract.localfs.LocalFSContract.init(LocalFSContract.java:63)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractSeek#testSeekPastEndOfFileThenReseekAndRead;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureValueTooShort;id_000000;[file.bytes-per-checksum=1713658853];java.lang.NegativeArraySizeException;-1756939507;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureValueTooShort;id_000001;[io.file.buffer.size=2138114854];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureValueTooShort;id_000003;[file.bytes-per-checksum=1607920006];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureValueTooShort;id_000059;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureOneEntryKnownLength;id_000000;[file.bytes-per-checksum=1898247916];java.lang.NegativeArraySizeException;-95637940;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureOneEntryKnownLength;id_000001;[file.stream-buffer-size=650554177, fs.file.impl.disable.cache=true, io.file.buffer.size=1303924409];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureOneEntryKnownLength;id_000003;[hadoop.service.shutdown.timeout=2m, fs.permissions.umask-mode=700, file.stream-buffer-size=650554177, hadoop.security.groups.negative-cache.secs=182200760, fs.local.block.size=2037805557, fs.creation.parallel.count=1247251159, fs.file.impl.disable.cache=true, io.file.buffer.size=1303924409];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureOneEntryKnownLength;id_000005;[io.file.buffer.size=428166964, tfile.fs.output.buffer.size=585667040, file.bytes-per-checksum=101971583];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.OutOfMemoryError(Java heap space),  java.lang.OutOfMemoryError(Java heap space);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.io.file.tfile.TestTFileNoneCodecsStreams#testFailureOneEntryKnownLength;id_000009;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testRenameFileBeingAppended;id_000000;[io.file.buffer.size=1422440178];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.ContractTestUtils.readDataset(ContractTestUtils.java:214)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testRenameFileBeingAppended;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractAppend#testRenameFileBeingAppended;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadAtExactEOF;id_000000;[file.bytes-per-checksum=1375961692];java.lang.NegativeArraySizeException;-501246660;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadAtExactEOF;id_000002;[file.bytes-per-checksum=1456860116];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.fs.contract.AbstractContractSeekTest.testReadAtExactEOF(AbstractContractSeekTest.java:586)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadAtExactEOF;id_000003;[file.bytes-per-checksum=1984345388];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractSeek#testReadAtExactEOF;id_000004;[file.bytes-per-checksum=694220816];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriteRecordAfterMetaBlock;id_000000;[file.bytes-per-checksum=1784052428];java.lang.NegativeArraySizeException;-1123397332;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriteRecordAfterMetaBlock;id_000001;[file.stream-buffer-size=1053774647, io.file.buffer.size=1347775222];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriteRecordAfterMetaBlock;id_000003;[file.bytes-per-checksum=1134086077];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.io.file.tfile.TestTFileByteArrays#testFailureWriteRecordAfterMetaBlock;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestFileSystemCaching#testCacheEnabled;id_000000;[fs.cachedfile.impl.disable.cache=true];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.security.TestGroupsCaching#testExceptionsFromImplNotCachedInNegativeCache;id_000000;[hadoop.security.groups.cache.secs=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.TestGroupsCaching.testExceptionsFromImplNotCachedInNegativeCache(TestGroupsCaching.java:410)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testSyncablePassthroughIfChecksumDisabled;id_000000;[io.file.buffer.size=1038172302];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testSyncablePassthroughIfChecksumDisabled;id_000002;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:525)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testSyncable;id_000000;[io.file.buffer.size=1994549189];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testSyncable;id_000002;[file.bytes-per-checksum=647637991];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testSyncable;id_000003;[file.bytes-per-checksum=1325862138];java.lang.NegativeArraySizeException;-952142646;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testMkdirUnderFile;id_000000;[file.bytes-per-checksum=1617864576];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testMkdirUnderFile;id_000001;[file.bytes-per-checksum=1297389379];java.lang.NegativeArraySizeException;-1208397477;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractCreate#testMkdirUnderFile;id_000002;[file.bytes-per-checksum=2131924972];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.contract.rawlocal.TestRawlocalContractOpen#testOpenReadDirWithChild;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testOutputStreamClosedTwice;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1956904644, hadoop.security.auth_to_local.mechanism=MIT, hadoop.service.shutdown.timeout=8473281h, fs.permissions.umask-mode=755, file.stream-buffer-size=176379454, hadoop.security.groups.cache.background.reload.threads=166575631, fs.local.block.size=1155565142, fs.creation.parallel.count=2076822405, fs.file.impl.disable.cache=true, io.file.buffer.size=843405535, hadoop.security.groups.cache.warn.after.ms=448587474];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testOutputStreamClosedTwice;id_000002;[file.bytes-per-checksum=408062427];java.lang.NegativeArraySizeException;-622405453;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testOutputStreamClosedTwice;id_000003;[file.bytes-per-checksum=160815141, io.file.buffer.size=843405535];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.fs.TestFSMainOperationsLocalFileSystem#testOutputStreamClosedTwice;id_000004;[file.stream-buffer-size=2013484606, io.file.buffer.size=843405535];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testResolvePath;id_000000;[file.bytes-per-checksum=785946816];java.lang.NegativeArraySizeException;-1516413248;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:197)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testResolvePath;id_000002;[file.bytes-per-checksum=2112130806];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testResolvePath;id_000004;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.TestChecksumFileSystem#testMultiChunkFile;id_000000;[io.file.buffer.size=1108872987];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.TestChecksumFileSystem#testMultiChunkFile;id_000001;[file.stream-buffer-size=496905204, io.file.buffer.size=688163333];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.fs.TestChecksumFileSystem#testMultiChunkFile;id_000002;[io.file.buffer.size=1522433429];java.lang.NegativeArraySizeException;-1241040283;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestChecksumFileSystem#testMultiChunkFile;id_000003;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]
org.apache.hadoop.fs.TestChecksumFileSystem#testMultiChunkFile;id_000004;[io.file.buffer.size=0];java.lang.IllegalArgumentException;Buffer size <= 0;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:73), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileApplyRead;id_000000;[hadoop.security.token.service.use_ip=false, hadoop.security.dns.log-slow-lookups.threshold.ms=1310439167, hadoop.security.groups.cache.secs=1549958825, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.service.shutdown.timeout=2m, fs.permissions.umask-mode=755, file.stream-buffer-size=44297933, hadoop.security.groups.negative-cache.secs=1242686815, fs.local.block.size=699845272, fs.file.impl.disable.cache=true, hadoop.security.groups.cache.warn.after.ms=1736690158];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$136/0x0000000840196440.call(Unknown Source)]
org.apache.hadoop.fs.contract.localfs.TestLocalFSContractOpen#testOpenFileApplyRead;id_000002;[file.bytes-per-checksum=391103827];java.lang.NegativeArraySizeException;-775032853;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSrcIsDirWithDelSrcOptions;id_000000;[io.file.buffer.size=1254266714];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)]
org.apache.hadoop.fs.TestLocalFSCopyFromLocal#testSrcIsDirWithDelSrcOptions;id_000002;[io.file.buffer.size=1078691037];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.ipc.TestIPC#testConnectionIdleTimeouts;id_000000;[ipc.server.read.threadpool.size=909293760];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.Server.<init>(Server.java:3046), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:226), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:219)]
org.apache.hadoop.ipc.TestAsyncIPC#testUniqueSequentialCallIds;id_000000;[ipc.server.read.threadpool.size=1826223889];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.Server.<init>(Server.java:3046), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:226), org.apache.hadoop.ipc.TestIPC$TestServer.<init>(TestIPC.java:219)]
org.apache.hadoop.http.TestHttpServerWithSpnego#testAuthenticationWithProxyUser;id_000000;[hadoop.http.selector.count=775016272];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.ha.TestZKFailoverController#testCedeActive;id_000000;[hadoop.security.credential.clear-text-fallback=false];org.apache.zookeeper.KeeperException$NoAuthException;KeeperErrorCode = NoAuth for /hadoop-ha/dummy-cluster;[org.apache.zookeeper.KeeperException.create(KeeperException.java:120), org.apache.zookeeper.KeeperException.create(KeeperException.java:54), org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1041), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1038)]
org.apache.hadoop.ha.TestZKFailoverController#testBecomingActiveFails;id_000000;[hadoop.security.credential.clear-text-fallback=false];org.apache.zookeeper.KeeperException$NoAuthException;KeeperErrorCode = NoAuth for /shuai/dummy-cluster;[org.apache.zookeeper.KeeperException.create(KeeperException.java:120), org.apache.zookeeper.KeeperException.create(KeeperException.java:54), org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1041), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1038)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testStopThreads;id_000000;[zk-dt-secret-manager.zkNumRetries=0];java.lang.ArithmeticException;/ by zero;[org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager.<init>(ZKDelegationTokenSecretManager.java:210), org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager.<init>(DelegationTokenManager.java:99), org.apache.hadoop.security.token.delegation.web.DelegationTokenManager.<init>(DelegationTokenManager.java:120), org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testStopThreads(TestZKDelegationTokenSecretManager.java:352), org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager.testStopThreads$$CONFUZZ(TestZKDelegationTokenSecretManager.java)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testStopThreads;id_000002;[zk-dt-secret-manager.zkSessionTimeout=0];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[java.lang.Object.wait(Native Method), java.lang.Object.wait(Object.java:328), org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1529), org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1512), org.apache.zookeeper.ZooKeeper.exists(ZooKeeper.java:2016)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testGetStoragePolicy;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.fs.viewfs.TestChRootedFileSystem#testCreateSnapshot;id_000000;[file.bytes-per-checksum=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalStateException(bytes per checksum should be positive but was 0),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]
org.apache.hadoop.security.TestLdapGroupsMappingWithBindUserSwitch#testBindUserSwitchPasswordFromAlias;id_000000;[file.bytes-per-checksum=1381150669];java.lang.NegativeArraySizeException;-454545867;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.security.TestLdapGroupsMappingWithBindUserSwitch#testBindUserSwitchPasswordFromAlias;id_000001;[file.bytes-per-checksum=2112855780];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.security.TestLdapGroupsMappingWithBindUserSwitch#testBindUserSwitchPasswordFromAlias;id_000002;[file.bytes-per-checksum=1003353471, hadoop.security.group.mapping.ldap.read.timeout.ms=1103728059];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437)]
org.apache.hadoop.security.TestLdapGroupsMappingWithBindUserSwitch#testBindUserSwitchPasswordFromAlias;id_000003;[file.bytes-per-checksum=2109564292];java.lang.OutOfMemoryError;Java heap space;[]
org.apache.hadoop.ha.TestZKFailoverController#testDontFailoverToUnhealthyNode;id_000000;[hadoop.security.credential.clear-text-fallback=false];org.apache.zookeeper.KeeperException$NoAuthException;KeeperErrorCode = NoAuth for /uiuc/dummy-cluster;[org.apache.zookeeper.KeeperException.create(KeeperException.java:120), org.apache.zookeeper.KeeperException.create(KeeperException.java:54), org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:1538), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1041), org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1038)]
org.apache.hadoop.fs.sftp.TestSFTPFileSystem#testDeleteNonEmptyDir;id_000000;[file.bytes-per-checksum=651764029];java.io.IOException;Directory: file:/mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task146/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/testsftp is not empty.;[org.apache.hadoop.fs.sftp.SFTPFileSystem.delete(SFTPFileSystem.java:400), org.apache.hadoop.fs.sftp.SFTPFileSystem.delete(SFTPFileSystem.java:618), org.apache.hadoop.fs.sftp.TestSFTPFileSystem.testDeleteNonEmptyDir(TestSFTPFileSystem.java:273), org.apache.hadoop.fs.sftp.TestSFTPFileSystem.testDeleteNonEmptyDir$$CONFUZZ(TestSFTPFileSystem.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.fs.sftp.TestSFTPFileSystem#testDeleteNonEmptyDir;id_000001;[file.bytes-per-checksum=1730256773];java.lang.OutOfMemoryError;Java heap space;[java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433)]
org.apache.hadoop.fs.sftp.TestSFTPFileSystem#testRenamingFileOntoExistingFile;id_000000;[file.bytes-per-checksum=1877779209];java.io.IOException;Destination path /mnt/batch/tasks/workitems/FUZZ_hcommon_r125_1M24d23h38m5s/job-1/Task146/wd/confuzz/scripts/fuzz-hadoop/hadoop-common-project/hadoop-common/target/test/data/testsftp/testrenamingfileontoexistingfile$$confuzz2 already exist, cannot rename!;[org.apache.hadoop.fs.sftp.SFTPFileSystem.rename(SFTPFileSystem.java:479), org.apache.hadoop.fs.sftp.SFTPFileSystem.rename(SFTPFileSystem.java:607), org.apache.hadoop.fs.sftp.TestSFTPFileSystem.testRenamingFileOntoExistingFile(TestSFTPFileSystem.java:336), org.apache.hadoop.fs.sftp.TestSFTPFileSystem.testRenamingFileOntoExistingFile$$CONFUZZ(TestSFTPFileSystem.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]
org.apache.hadoop.fs.sftp.TestSFTPFileSystem#testRenamingFileOntoExistingFile;id_000001;[file.bytes-per-checksum=1169059356];java.lang.NegativeArraySizeException;-279856303;[org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)]
org.apache.hadoop.http.TestHttpServer#testAuthorizationOfDefaultServlets;id_000000;[hadoop.http.selector.count=1513894031];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.http.TestHttpServer#testDisabledAuthorizationOfDefaultServlets;id_000000;[hadoop.http.selector.count=2034129131];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]
org.apache.hadoop.security.token.delegation.TestZKDelegationTokenSecretManager#testMultiNodeOperations;id_000000;[zk-dt-secret-manager.zkSessionTimeout=0];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[jdk.internal.misc.Unsafe.park(Native Method), java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234), java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1079), java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1369), java.util.concurrent.CountDownLatch.await(CountDownLatch.java:278)]
