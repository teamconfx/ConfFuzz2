note	bugId	status	times	testName	failure	failureMessage	stackTrace	reproStatus	replayedFailure	replayedErrorMessage	replayedStackTrace	replayedFile	minConfig	debugFiles																																																																																																																																																																																																																	
		FP	22	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2TaskCleanupEnabled	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2TaskCleanupEnabled/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:388), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000005"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000005"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:560), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:482), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:720), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:725), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000005"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:274), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:214), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV2/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000012	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000012"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:336), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1(TestFileOutputCommitter.java:473), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV1/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000000"]
		FP	26	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2TaskCleanupEnabled	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2TaskCleanupEnabled/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000005"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000010	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000010"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:394), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000009	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000009"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:639), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:560), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:482), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:720), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:725), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:274), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:214), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV2/campaign/failures/debug_000007	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000011	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec"}	["debug_000011"]
			6	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2TaskCleanupEnabled	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2TaskCleanupEnabled/campaign/failures/debug_000008	{"file.bytes-per-checksum": "229577020", "io.file.buffer.size": "1969718351"}	["debug_000008"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithScheme	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:331), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:325), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentials(TestTokenCache.java:99), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithScheme(TestTokenCache.java:74), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithScheme/campaign/failures/debug_000004	{"file.bytes-per-checksum": "1146560936", "io.file.buffer.size": "903682836"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:285), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000000	{"io.file.buffer.size": "1326893147", "file.bytes-per-checksum": "1161426220"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000008	{"io.file.buffer.size": "1775290288", "file.bytes-per-checksum": "550203664"}	["debug_000008"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000008	{"io.file.buffer.size": "1570024515", "file.bytes-per-checksum": "1515568314"}	["debug_000008"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1(TestFileOutputCommitter.java:473), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV1/campaign/failures/debug_000005	{"io.file.buffer.size": "1356965180", "file.bytes-per-checksum": "1027764659"}	["debug_000005"]																																																																																																																																																										
		FP	1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2TaskCleanupEnabled	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:223), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:315), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2TaskCleanupEnabled/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true"}	["debug_000000"]																																																																																																																																																																																																																	
	Bug-50	Repeated	24	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2TaskCleanupEnabled	java.lang.NegativeArraySizeException	-577794076	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2TaskCleanupEnabled/campaign/failures/debug_000002	{"file.bytes-per-checksum": "1844675012"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.lang.NegativeArraySizeException	-1795044154	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000002	{"file.bytes-per-checksum": "277769238"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.NegativeArraySizeException	-1222646277	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000008	{"file.bytes-per-checksum": "1295806179"}	["debug_000008"]	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputWithLargeSplitSize	java.lang.NegativeArraySizeException	-463171129	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputWithLargeSplitSize(TestLineRecordReader.java:377), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputWithLargeSplitSize$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputWithLargeSplitSize/campaign/failures/debug_000002	{"file.bytes-per-checksum": "1857410895"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.lang.NegativeArraySizeException	-312113122	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000008	{"file.bytes-per-checksum": "1874195118"}	["debug_000008"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.NegativeArraySizeException	-493513145	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:394), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000004	{"file.bytes-per-checksum": "899602383"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.NegativeArraySizeException	-1224424623	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:388), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000001	{"file.bytes-per-checksum": "1295608585"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testUncompressedInputCustomDelimiterPosValue	java.lang.NegativeArraySizeException	-192715284	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.createInputFile(TestLineRecordReader.java:312), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputCustomDelimiterPosValue(TestLineRecordReader.java:417), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputCustomDelimiterPosValue$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testUncompressedInputCustomDelimiterPosValue/campaign/failures/debug_000001	{"file.bytes-per-checksum": "1410242956"}	["debug_000001"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithScheme	java.lang.NegativeArraySizeException	-320815862	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:331), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:325), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentials(TestTokenCache.java:99), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithScheme(TestTokenCache.java:74), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithScheme$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithScheme/campaign/failures/debug_000002	{"file.bytes-per-checksum": "918790970"}	["debug_000002"]	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputDefaultDelimiterPosValue	java.lang.NegativeArraySizeException	-1356002809	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue(TestLineRecordReader.java:607), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputDefaultDelimiterPosValue/campaign/failures/debug_000000	{"file.bytes-per-checksum": "1758207375"}	["debug_000000"]	org.apache.hadoop.mapreduce.split.TestJobSplitWriter#testMaxBlockLocationsOldSplits	java.lang.NegativeArraySizeException	-1274735028	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:728), org.apache.hadoop.mapreduce.split.JobSplitWriter.createFile(JobSplitWriter.java:102), org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:91), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsOldSplits(TestJobSplitWriter.java:74), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsOldSplits$$CONFUZZ(TestJobSplitWriter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriter/testMaxBlockLocationsOldSplits/campaign/failures/debug_000004	{"file.bytes-per-checksum": "1290018540"}	["debug_000004"]	org.apache.hadoop.mapreduce.split.TestJobSplitWriter#testMaxBlockLocationsNewSplits	java.lang.NegativeArraySizeException	-1386742863	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:728), org.apache.hadoop.mapreduce.split.JobSplitWriter.createFile(JobSplitWriter.java:102), org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:78), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsNewSplits(TestJobSplitWriter.java:50), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsNewSplits$$CONFUZZ(TestJobSplitWriter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriter/testMaxBlockLocationsNewSplits/campaign/failures/debug_000003	{"file.bytes-per-checksum": "1277573225"}	["debug_000003"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testGetTokensForNamenodes	java.lang.NegativeArraySizeException	-739245495	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:331), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:325), org.apache.hadoop.mapreduce.security.TestTokenCache.testGetTokensForNamenodes(TestTokenCache.java:205), org.apache.hadoop.mapreduce.security.TestTokenCache.testGetTokensForNamenodes$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testGetTokensForNamenodes/campaign/failures/debug_000002	{"file.bytes-per-checksum": "1349517377"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV2	java.lang.NegativeArraySizeException	-1850573695	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:482), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000007	{"file.bytes-per-checksum": "271599289"}	["debug_000007"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV1	java.lang.NegativeArraySizeException	-341189566	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000006	{"file.bytes-per-checksum": "1870964402"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testUncompressedInputContainingCRLF	java.lang.NegativeArraySizeException	-506486317	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.createInputFile(TestLineRecordReader.java:312), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputContainingCRLF(TestLineRecordReader.java:400), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputContainingCRLF$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testUncompressedInputContainingCRLF/campaign/failures/debug_000000	{"file.bytes-per-checksum": "420942331"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.NegativeArraySizeException	-1147077643	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:274), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000004	{"file.bytes-per-checksum": "349765517"}	["debug_000004"]	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputContainingCRLF	java.lang.NegativeArraySizeException	-1465609958	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputContainingCRLF(TestLineRecordReader.java:467), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputContainingCRLF$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputContainingCRLF/campaign/failures/debug_000000	{"file.bytes-per-checksum": "791591626"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testUncompressedInputDefaultDelimiterPosValue	java.lang.NegativeArraySizeException	-707224449	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.createInputFile(TestLineRecordReader.java:312), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue(TestLineRecordReader.java:559), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testUncompressedInputDefaultDelimiterPosValue/campaign/failures/debug_000001	{"file.bytes-per-checksum": "1353075271"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.NegativeArraySizeException	-1097184774	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000002	{"file.bytes-per-checksum": "1309746346"}	["debug_000002"]
		FP	21	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2TaskCleanupEnabled	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2TaskCleanupEnabled/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000005"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000009	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000009"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:394), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:388), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:639), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:482), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:720), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:725), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:274), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:341), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000008	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000008"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:336), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1(TestFileOutputCommitter.java:473), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV1/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2(TestFileOutputCommitter.java:478), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV2/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000005"]
			21	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2TaskCleanupEnabled	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2TaskCleanupEnabled/campaign/failures/debug_000007	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000007"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000013	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000013"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:388), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:388), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000007	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:639), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:639), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000005"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000008	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000008"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000008	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000008"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:560), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:560), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:720), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:720), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000009	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000009"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:214), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV2/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000013	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000013"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:341), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:341), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000007	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2(TestFileOutputCommitter.java:478), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2(TestFileOutputCommitter.java:478), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV2/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000006"]
			40	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2TaskCleanupEnabled	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:266), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2TaskCleanupEnabled/campaign/failures/debug_000004	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:98), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000006	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000006"]	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheEnabled	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.yarn.client.api.impl.SharedCacheClientImpl.getFileChecksum(SharedCacheClientImpl.java:163), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache$MyFileUploader$1.answer(TestJobResourceUploaderWithSharedCache.java:134), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache$MyFileUploader$1.answer(TestJobResourceUploaderWithSharedCache.java:127), org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39), org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96), org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29), org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:61), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49), org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptAbstract(MockMethodInterceptor.java:126), org.apache.hadoop.yarn.client.api.SharedCacheClient$MockitoMock$628655281.getFileChecksum(Unknown Source), org.apache.hadoop.mapreduce.JobResourceUploader.useSharedCache(JobResourceUploader.java:736), org.apache.hadoop.mapreduce.JobResourceUploader.uploadFiles(JobResourceUploader.java:250), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:205), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabled(TestJobResourceUploaderWithSharedCache.java:177), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabled$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.isAuthenticationMethodEnabled(UserGroupInformation.java:395), org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:389), org.apache.hadoop.hdfs.server.common.Util.<clinit>(Util.java:77), org.apache.hadoop.hdfs.MiniDFSCluster.initNameNodeConf(MiniDFSCluster.java:1268), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1089), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheEnabled/campaign/failures/debug_000001	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:232), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000005	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000005"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:232), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000007"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:438), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377), org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:154), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:98), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000007"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:98), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:232), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000006	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:440), org.apache.hadoop.mapred.TestFileOutputCommitter$CommitterFailedFirst.commitJobInternal(TestFileOutputCommitter.java:615), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377), org.apache.hadoop.mapred.TestFileOutputCommitter$CommitterWithFailedThenSucceed.commitJob(TestFileOutputCommitter.java:599), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:252), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:232), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:444), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000009	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000009"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:328), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000008	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000008"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:328), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000009	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000009"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:440), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377), org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:346), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:328), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000002	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:440), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377), org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:346), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:328), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000007	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:266), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:400), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:394), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000005	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:394), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:400), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:394), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000000	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:400), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:388), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000006	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:284), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:603), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000000"]
		FP	1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2TaskCleanupEnabled	java.lang.AssertionError	job temp dir still exists	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertFalse(Assert.java:65), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:311), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled(TestFileOutputCommitter.java:331), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2TaskCleanupEnabled$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2TaskCleanupEnabled/campaign/failures/debug_000003	{"mapreduce.fileoutputcommitter.cleanup.skipped": "true"}	["debug_000003"]																																																																																																																																																																																																																	
			8	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:92), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000005	{"hadoop.security.authentication": "kerberos"}	["debug_000005"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:92), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000006	{"hadoop.security.authentication": "kerberos"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:324), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000006	{"hadoop.security.authentication": "kerberos"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:388), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:366), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000017	{"hadoop.security.authentication": "kerberos"}	["debug_000017"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:366), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000013	{"hadoop.security.authentication": "kerberos"}	["debug_000013"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:366), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000010	{"hadoop.security.authentication": "kerberos"}	["debug_000010"]																																																																																																																																				
			3	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000008	{"file.stream-buffer-size": "832726714", "io.file.buffer.size": "1987966117"}	["debug_000008"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000007	{"file.stream-buffer-size": "722233868", "io.file.buffer.size": "1970070251"}	["debug_000007"]	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager#testDetermineCacheVisibilities	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:333), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.createTempFile(TestClientDistributedCacheManager.java:249), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.setup(TestClientDistributedCacheManager.java:78), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager/testDetermineCacheVisibilities/campaign/failures/debug_000004	{"io.file.buffer.size": "656127591", "file.stream-buffer-size": "1807693885"}	["debug_000004"]																																																																																																																																																																																											
			12	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:92), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000009	{"hadoop.security.authentication": "kerberos"}	["debug_000009"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:226), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000002	{"hadoop.security.authentication": "kerberos"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:324), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:394), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos"}	["debug_000007"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:366), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000009	{"hadoop.security.authentication": "kerberos"}	["debug_000009"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:482), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:442), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:482), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:725), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000003	{"hadoop.security.authentication": "kerberos"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:274), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:280), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:274), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000008	{"hadoop.security.authentication": "kerberos"}	["debug_000008"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000012	{"hadoop.security.authentication": "kerberos"}	["debug_000012"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000016	{"hadoop.security.authentication": "kerberos"}	["debug_000016"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:336), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1(TestFileOutputCommitter.java:473), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV1/campaign/failures/debug_000003	{"hadoop.security.authentication": "kerberos"}	["debug_000003"]																																																																																								
		FP	5	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.io.FileNotFoundException	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/output/_temporary/1/task_200707121733_0001_m_000000/part-00000 (No such file or directory)	java.base/java.io.FileInputStream.open0(Native Method), java.base/java.io.FileInputStream.open(FileInputStream.java:219), java.base/java.io.FileInputStream.<init>(FileInputStream.java:157), org.apache.hadoop.mapred.TestFileOutputCommitter.slurp(TestFileOutputCommitter.java:570), org.apache.hadoop.mapred.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:184), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:121), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.io.FileNotFoundException	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/output/part-00000 (No such file or directory)	java.base/java.io.FileInputStream.open0(Native Method), java.base/java.io.FileInputStream.open(FileInputStream.java:219), java.base/java.io.FileInputStream.<init>(FileInputStream.java:157), org.apache.hadoop.mapred.TestFileOutputCommitter.slurp(TestFileOutputCommitter.java:570), org.apache.hadoop.mapred.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:184), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:349), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.io.FileNotFoundException	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/output/part-00000 (No such file or directory)	java.base/java.io.FileInputStream.open0(Native Method), java.base/java.io.FileInputStream.open(FileInputStream.java:219), java.base/java.io.FileInputStream.<init>(FileInputStream.java:157), org.apache.hadoop.mapred.TestFileOutputCommitter.slurp(TestFileOutputCommitter.java:570), org.apache.hadoop.mapred.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:184), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:349), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.io.FileNotFoundException	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/output/part-00000 (No such file or directory)	java.base/java.io.FileInputStream.open0(Native Method), java.base/java.io.FileInputStream.open(FileInputStream.java:219), java.base/java.io.FileInputStream.<init>(FileInputStream.java:157), org.apache.hadoop.mapred.TestFileOutputCommitter.slurp(TestFileOutputCommitter.java:570), org.apache.hadoop.mapred.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:184), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:305), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:274), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV2	java.io.FileNotFoundException	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/output/part-00000 (No such file or directory)	java.base/java.io.FileInputStream.open0(Native Method), java.base/java.io.FileInputStream.open(FileInputStream.java:219), java.base/java.io.FileInputStream.<init>(FileInputStream.java:157), org.apache.hadoop.mapred.TestFileOutputCommitter.slurp(TestFileOutputCommitter.java:570), org.apache.hadoop.mapred.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:184), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:155), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:166), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV2/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true"}	["debug_000000"]																																																																																																																																																																					
	Bug-27	Repeated	3	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "io.file.buffer.size": "1510943493"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV2	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000007	{"mapreduce.output.fileoutputformat.compress": "true", "io.file.buffer.size": "1627720800"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV1	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:720), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000000	{"io.file.buffer.size": "1826478660", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000000"]																																																																																																																																																																																											
			8	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:92), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos"}	["debug_000007"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:226), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:510), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000002	{"hadoop.security.authentication": "kerberos"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000005	{"hadoop.security.authentication": "kerberos"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:366), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000010	{"hadoop.security.authentication": "kerberos"}	["debug_000010"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:214), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV2/campaign/failures/debug_000005	{"hadoop.security.authentication": "kerberos"}	["debug_000005"]																																																																																																																																				
	Bug-50	Repeated	6	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.lang.NegativeArraySizeException	-1682215051	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "file.bytes-per-checksum": "290305805"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.NegativeArraySizeException	-1270647049	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "file.bytes-per-checksum": "336035583"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.lang.NegativeArraySizeException	-270995421	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress": "true", "file.bytes-per-checksum": "1401545163"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.lang.NegativeArraySizeException	-1637205394	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress": "true", "file.bytes-per-checksum": "295306878"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.NegativeArraySizeException	-210701491	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:274), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000009	{"file.bytes-per-checksum": "1885463077", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000009"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NegativeArraySizeException	-694178464	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000017	{"file.bytes-per-checksum": "400087648", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000017"]																																																																																																																																																										
			9	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:92), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:161), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000003	{"hadoop.security.authentication": "kerberos"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:92), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000002	{"hadoop.security.authentication": "kerberos"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:226), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:510), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:394), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000006	{"hadoop.security.authentication": "kerberos"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:280), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000003	{"hadoop.security.authentication": "kerberos"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:639), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV1	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:442), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:341), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos"}	["debug_000004"]																																																																																																																									
			2	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheEnabled	org.apache.hadoop.ipc.RemoteException	Requested replication factor of 27740 exceeds maximum of 512 for /tmp/hadoop-yarn/staging/files/second-input-file at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.verifyReplication(BlockManager.java:1611) at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setReplication(FSDirAttrOp.java:135) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplication(FSNamesystem.java:2348) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setReplication(NameNodeRpcServer.java:851) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setReplication(ClientNamenodeProtocolServerSideTranslatorPB.java:540) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy30.setReplication(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setReplication(ClientNamenodeProtocolTranslatorPB.java:432), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy31.setReplication(Unknown Source), org.apache.hadoop.hdfs.DFSClient.setReplication(DFSClient.java:1490), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:737), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:734), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.setReplication(DistributedFileSystem.java:744), org.apache.hadoop.mapreduce.JobResourceUploader.copyRemoteFiles(JobResourceUploader.java:704), org.apache.hadoop.mapreduce.JobResourceUploader.uploadFiles(JobResourceUploader.java:258), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:205), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabled(TestJobResourceUploaderWithSharedCache.java:177), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabled$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.io.IOException	The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem	org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheEnabled/campaign/failures/debug_000000	{"dfs.namenode.support.allow.format": "false"}	["debug_000000"]	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheEnabledWithJobJarInSharedCache	org.apache.hadoop.ipc.RemoteException	Requested replication factor of 6723 exceeds maximum of 512 for /tmp/hadoop-yarn/staging/files/second-input-file at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.verifyReplication(BlockManager.java:1611) at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setReplication(FSDirAttrOp.java:135) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplication(FSNamesystem.java:2348) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setReplication(NameNodeRpcServer.java:851) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setReplication(ClientNamenodeProtocolServerSideTranslatorPB.java:540) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy30.setReplication(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setReplication(ClientNamenodeProtocolTranslatorPB.java:432), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy31.setReplication(Unknown Source), org.apache.hadoop.hdfs.DFSClient.setReplication(DFSClient.java:1490), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:737), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:734), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.setReplication(DistributedFileSystem.java:744), org.apache.hadoop.mapreduce.JobResourceUploader.copyRemoteFiles(JobResourceUploader.java:704), org.apache.hadoop.mapreduce.JobResourceUploader.uploadFiles(JobResourceUploader.java:258), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:205), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabledWithJobJarInSharedCache(TestJobResourceUploaderWithSharedCache.java:191), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabledWithJobJarInSharedCache$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.io.IOException	The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem	org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheEnabledWithJobJarInSharedCache/campaign/failures/debug_000000	{"dfs.namenode.support.allow.format": "false"}	["debug_000000"]																																																																																																																																																																																																						
			1	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheEnabled	org.apache.hadoop.ipc.RemoteException	Client (=DFSClient_NONMAPREDUCE_-106838092_1) is not the lease owner (=HDFS_NameNode-2023-07-18 13:59:22,307+0000: /tmp/distributed.second.jar (inode 16387) Holder DFSClient_NONMAPREDUCE_-106838092_1 does not have any open files. at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:3093) at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFileInternal(FSDirWriteFileOp.java:704) at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.completeFile(FSDirWriteFileOp.java:690) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile(FSNamesystem.java:3115) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete(NameNodeRpcServer.java:959) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.complete(ClientNamenodeProtocolServerSideTranslatorPB.java:639) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy30.complete(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.complete(ClientNamenodeProtocolTranslatorPB.java:570), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy31.complete(Unknown Source), org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:952), org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:909), org.apache.hadoop.hdfs.DFSOutputStream.closeImpl(DFSOutputStream.java:892), org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:847), org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:77), org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:70), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:419), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:391), org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2482), org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2448), org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:2411), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.copyToRemote(TestJobResourceUploaderWithSharedCache.java:219), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.makeJarAvailableInSharedCache(TestJobResourceUploaderWithSharedCache.java:226), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:262), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabled(TestJobResourceUploaderWithSharedCache.java:177), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabled$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	org.apache.hadoop.HadoopIllegalArgumentException	Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1073790719. Value configured is >= to the number of configured volumes (2).	org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.check(StorageLocationChecker.java:178), org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2821), org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2734), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1755), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheEnabled/campaign/failures/debug_000002	{"dfs.datanode.failed.volumes.tolerated": "1073790719"}	["debug_000002"]																																																																																																																																																																																																																	
			2	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheEnabled	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.io.IOException	The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem	org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheEnabled/campaign/failures/debug_000003	{"dfs.namenode.support.allow.format": "false"}	["debug_000003"]	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheEnabledWithJobJarInSharedCache	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:93), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:68), org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:129), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:419), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:391), org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:341), org.apache.hadoop.mapreduce.JobResourceUploader.copyRemoteFiles(JobResourceUploader.java:703), org.apache.hadoop.mapreduce.JobResourceUploader.uploadFiles(JobResourceUploader.java:258), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:205), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabledWithJobJarInSharedCache(TestJobResourceUploaderWithSharedCache.java:191), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabledWithJobJarInSharedCache$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79),	DIFFERENT	java.io.IOException	The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem	org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheEnabledWithJobJarInSharedCache/campaign/failures/debug_000003	{"dfs.namenode.support.allow.format": "false"}	["debug_000003"]																																																																																																																																																																																																						
	Bug-68	Repeated	3	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testLargeMemoryLimits	java.lang.NullPointerException		org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:106), org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:38), org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:141), org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream.wrapIfNecessary(IntermediateEncryptedStream.java:46), org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput.<init>(OnDiskMapOutput.java:87), org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:274), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.verifyReservedMapOutputType(TestMergeManager.java:309), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testLargeMemoryLimits(TestMergeManager.java:303), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testLargeMemoryLimits$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testLargeMemoryLimits/campaign/failures/debug_000000	{"mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000000"]	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testInterruptOnDisk	java.lang.NullPointerException		org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:106), org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:38), org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:141), org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream.wrapIfNecessary(IntermediateEncryptedStream.java:46), org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput.<init>(OnDiskMapOutput.java:87), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testInterruptOnDisk(TestFetcher.java:645), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testInterruptOnDisk$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testInterruptOnDisk/campaign/failures/debug_000000	{"mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000000"]	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testZeroShuffleMemoryLimitPercent	java.lang.NullPointerException		org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:106), org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:38), org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:141), org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream.wrapIfNecessary(IntermediateEncryptedStream.java:46), org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput.<init>(OnDiskMapOutput.java:87), org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:274), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.verifyReservedMapOutputType(TestMergeManager.java:309), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testZeroShuffleMemoryLimitPercent(TestMergeManager.java:323), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testZeroShuffleMemoryLimitPercent$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testZeroShuffleMemoryLimitPercent/campaign/failures/debug_000002	{"mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000002"]																																																																																																																																																																																											
			2	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testLargeMemoryLimits	java.lang.IllegalArgumentException	Minimum value of buffer size is 512.	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.CryptoStreamUtils.checkBufferSize(CryptoStreamUtils.java:70), org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:104), org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:38), org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:141), org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream.wrapIfNecessary(IntermediateEncryptedStream.java:46), org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput.<init>(OnDiskMapOutput.java:87), org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:274), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.verifyReservedMapOutputType(TestMergeManager.java:309), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testLargeMemoryLimits(TestMergeManager.java:303), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testLargeMemoryLimits$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496), org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:273), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.verifyReservedMapOutputType(TestMergeManager.java:309), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testLargeMemoryLimits(TestMergeManager.java:303), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testLargeMemoryLimits/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos"}	["debug_000007"]																																																																																																																																																																																																						
		FP	2	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testOnDiskMerger	java.lang.RuntimeException	Invalid configuration: maxSingleShuffleLimit should be less than mergeThreshold maxSingleShuffleLimit: 2041701mergeThreshold: -1737840	org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.<init>(MergeManagerImpl.java:215), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testOnDiskMerger(TestMergeManager.java:228), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testOnDiskMerger$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testOnDiskMerger/campaign/failures/debug_000000	{"mapreduce.reduce.shuffle.merge.percent": "-0.212793231010437"}	["debug_000000"]	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testZeroShuffleMemoryLimitPercent	java.lang.RuntimeException	Invalid configuration: maxSingleShuffleLimit should be less than mergeThreshold maxSingleShuffleLimit: 0mergeThreshold: -128	org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.<init>(MergeManagerImpl.java:215), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testZeroShuffleMemoryLimitPercent(TestMergeManager.java:320), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testZeroShuffleMemoryLimitPercent$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testZeroShuffleMemoryLimitPercent/campaign/failures/debug_000000	{"mapreduce.reduce.shuffle.merge.percent": "-0.31494325399398804"}	["debug_000000"]																																																																																																																																																																																																						
	Bug-7	Repeated	6	org.apache.hadoop.mapred.lib.TestCombineFileRecordReader#testInitNextRecordReader	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.<init>(UncompressedSplitLineReader.java:46), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:142), org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67), org.apache.hadoop.mapred.lib.CombineFileRecordReaderWrapper.<init>(CombineFileRecordReaderWrapper.java:60), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader$TextRecordReaderWrapper.<init>(TestCombineFileRecordReader.java:52), java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490), org.apache.hadoop.mapred.lib.CombineFileRecordReader.initNextRecordReader(CombineFileRecordReader.java:142), org.apache.hadoop.mapred.lib.CombineFileRecordReader.<init>(CombineFileRecordReader.java:117), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader.testInitNextRecordReader(TestCombineFileRecordReader.java:77), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader.testInitNextRecordReader$$CONFUZZ(TestCombineFileRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.lib.TestCombineFileRecordReader/testInitNextRecordReader/campaign/failures/debug_000000	{"io.file.buffer.size": "2038692353"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testBzip2SplitEndsAtCRThenLF	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:103), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:87), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:65), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzip2SplitEndsAtCRThenLF(TestLineRecordReader.java:131), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzip2SplitEndsAtCRThenLF$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testBzip2SplitEndsAtCRThenLF/campaign/failures/debug_000000	{"io.file.buffer.size": "1591603506"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testBzip2SplitEndsAtCR	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:103), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:87), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:65), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzip2SplitEndsAtCR(TestLineRecordReader.java:123), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzip2SplitEndsAtCR$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testBzip2SplitEndsAtCR/campaign/failures/debug_000000	{"io.file.buffer.size": "1557765380"}	["debug_000000"]	org.apache.hadoop.mapred.TestLineRecordReader#testBzip2SplitStartAtBlockMarker	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:123), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:62), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitStartAtBlockMarker(TestLineRecordReader.java:182), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitStartAtBlockMarker$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testBzip2SplitStartAtBlockMarker/campaign/failures/debug_000000	{"io.file.buffer.size": "1687382210"}	["debug_000000"]	org.apache.hadoop.mapred.TestLineRecordReader#testMultipleClose	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:123), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:96), org.apache.hadoop.mapred.TestLineRecordReader.testMultipleClose(TestLineRecordReader.java:335), org.apache.hadoop.mapred.TestLineRecordReader.testMultipleClose$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testMultipleClose/campaign/failures/debug_000000	{"io.file.buffer.size": "1894889483"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testMultipleClose	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:103), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testMultipleClose(TestLineRecordReader.java:285), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testMultipleClose$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testMultipleClose/campaign/failures/debug_000000	{"io.file.buffer.size": "1226710540"}	["debug_000000"]																																																																																																																																																										
			5	org.apache.hadoop.mapred.lib.TestCombineFileRecordReader#testInitNextRecordReader	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.<init>(UncompressedSplitLineReader.java:46), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:142), org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67), org.apache.hadoop.mapred.lib.CombineFileRecordReaderWrapper.<init>(CombineFileRecordReaderWrapper.java:60), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader$TextRecordReaderWrapper.<init>(TestCombineFileRecordReader.java:52), java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45), java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490), org.apache.hadoop.mapred.lib.CombineFileRecordReader.initNextRecordReader(CombineFileRecordReader.java:142), org.apache.hadoop.mapred.lib.CombineFileRecordReader.<init>(CombineFileRecordReader.java:117), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader.testInitNextRecordReader(TestCombineFileRecordReader.java:77), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.lib.TestCombineFileRecordReader/testInitNextRecordReader/campaign/failures/debug_000002	{"io.file.buffer.size": "1164790636"}	["debug_000002"]	org.apache.hadoop.mapred.TestLineRecordReader#testBzip2SplitEndsAtCR	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:123), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:62), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCR(TestLineRecordReader.java:156), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testBzip2SplitEndsAtCR/campaign/failures/debug_000002	{"io.file.buffer.size": "1194145134"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader#testProgressIsReportedIfInputASeriesOfEmptyFiles	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.<init>(UncompressedSplitLineReader.java:46), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:123), org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper.initialize(CombineFileRecordReaderWrapper.java:69), org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader.initialize(CombineFileRecordReader.java:59), org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader.testProgressIsReportedIfInputASeriesOfEmptyFiles(TestCombineFileRecordReader.java:86), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader/testProgressIsReportedIfInputASeriesOfEmptyFiles/campaign/failures/debug_000002	{"io.file.buffer.size": "1792686266"}	["debug_000002"]	org.apache.hadoop.mapred.TestLineRecordReader#testStripBOM	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.<init>(UncompressedSplitLineReader.java:46), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:142), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:96), org.apache.hadoop.mapred.TestLineRecordReader.testStripBOM(TestLineRecordReader.java:300), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testStripBOM/campaign/failures/debug_000001	{"io.file.buffer.size": "1146847779"}	["debug_000001"]	org.apache.hadoop.mapred.TestLineRecordReader#testBzipWithMultibyteDelimiter	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:123), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:93), org.apache.hadoop.mapred.TestLineRecordReader.testBzipWithMultibyteDelimiter(TestLineRecordReader.java:684), org.apache.hadoop.mapred.TestLineRecordReader.testBzipWithMultibyteDelimiter$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:123), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testBzipWithMultibyteDelimiter(TestLineRecordReader.java:684), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testBzipWithMultibyteDelimiter/campaign/failures/debug_000000	{"io.file.buffer.size": "1366615094"}	["debug_000000", "debug_000001"]																																																																																																																																																																					
			6	org.apache.hadoop.mapred.lib.TestCombineFileRecordReader#testInitNextRecordReader	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$124/0x000000084026ac40.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67), org.apache.hadoop.mapred.lib.CombineFileRecordReaderWrapper.<init>(CombineFileRecordReaderWrapper.java:60), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader$TextRecordReaderWrapper.<init>(TestCombineFileRecordReader.java:52), java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490), org.apache.hadoop.mapred.lib.CombineFileRecordReader.initNextRecordReader(CombineFileRecordReader.java:142), org.apache.hadoop.mapred.lib.CombineFileRecordReader.<init>(CombineFileRecordReader.java:117), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader.testInitNextRecordReader(TestCombineFileRecordReader.java:77), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader.testInitNextRecordReader$$CONFUZZ(TestCombineFileRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$75/0x00000008401e7c40.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67), org.apache.hadoop.mapred.lib.CombineFileRecordReaderWrapper.<init>(CombineFileRecordReaderWrapper.java:60), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader$TextRecordReaderWrapper.<init>(TestCombineFileRecordReader.java:52), java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45), java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490), org.apache.hadoop.mapred.lib.CombineFileRecordReader.initNextRecordReader(CombineFileRecordReader.java:142), org.apache.hadoop.mapred.lib.CombineFileRecordReader.<init>(CombineFileRecordReader.java:117), org.apache.hadoop.mapred.lib.TestCombineFileRecordReader.testInitNextRecordReader(TestCombineFileRecordReader.java:77), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.lib.TestCombineFileRecordReader/testInitNextRecordReader/campaign/failures/debug_000001	{"io.file.buffer.size": "2090371538"}	["debug_000001"]	org.apache.hadoop.mapred.TestLineRecordReader#testBzip2SplitEndsAtCRThenLF	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:123), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:62), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCRThenLF(TestLineRecordReader.java:164), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCRThenLF$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$70/0x00000008400f7040.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:62), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCRThenLF(TestLineRecordReader.java:164), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testBzip2SplitEndsAtCRThenLF/campaign/failures/debug_000000	{"io.file.buffer.size": "2130640638"}	["debug_000000"]	org.apache.hadoop.mapred.TestLineRecordReader#testStripBOM	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.<init>(UncompressedSplitLineReader.java:46), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:142), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:96), org.apache.hadoop.mapred.TestLineRecordReader.testStripBOM(TestLineRecordReader.java:300), org.apache.hadoop.mapred.TestLineRecordReader.testStripBOM$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$70/0x00000008400f7040.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:96), org.apache.hadoop.mapred.TestLineRecordReader.testStripBOM(TestLineRecordReader.java:300), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testStripBOM/campaign/failures/debug_000000	{"io.file.buffer.size": "2130640638"}	["debug_000000"]	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testCorruptedIFile	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCorruptedIFile(TestFetcher.java:588), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testCorruptedIFile/campaign/failures/debug_000003	{"io.file.buffer.size": "1397145930"}	["debug_000003"]	org.apache.hadoop.mapred.TestLineRecordReader#testMultipleClose	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$70/0x00000008400f7040.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:96), org.apache.hadoop.mapred.TestLineRecordReader.testMultipleClose(TestLineRecordReader.java:335), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testMultipleClose/campaign/failures/debug_000001	{"io.file.buffer.size": "2130640638"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testMultipleClose	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$122/0x000000084019f040.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testMultipleClose(TestLineRecordReader.java:285), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testMultipleClose$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$70/0x00000008400ef040.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testMultipleClose(TestLineRecordReader.java:285), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testMultipleClose/campaign/failures/debug_000001	{"io.file.buffer.size": "2131040794"}	["debug_000001"]																																																																																																																																																										
	Bug-179	BUG	13	org.apache.hadoop.mapred.TestIndexCache#testRemoveMap	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapred.TestIndexCache.writeFile(TestIndexCache.java:320), org.apache.hadoop.mapred.TestIndexCache.testRemoveMap(TestIndexCache.java:218), org.apache.hadoop.mapred.TestIndexCache.testRemoveMap$$CONFUZZ(TestIndexCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestIndexCache/testRemoveMap/campaign/failures/debug_000000	{"io.file.buffer.size": "2147417854"}	["debug_000000"]	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager#testDetermineTimestamps	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:333), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.createTempFile(TestClientDistributedCacheManager.java:249), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.setup(TestClientDistributedCacheManager.java:78), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager/testDetermineTimestamps/campaign/failures/debug_000001	{"io.file.buffer.size": "2146436014"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000001	{"io.file.buffer.size": "2130673407"}	["debug_000001"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithScheme	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:331), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:325), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentials(TestTokenCache.java:99), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithScheme(TestTokenCache.java:74), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithScheme$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithScheme/campaign/failures/debug_000003	{"io.file.buffer.size": "2123556607"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000002	{"io.file.buffer.size": "2120377958"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:285), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000002	{"io.file.buffer.size": "1864547432"}	["debug_000002"]	org.apache.hadoop.mapred.TestIndexCache#testInvalidReduceNumberOrLength	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapred.TestIndexCache.writeFile(TestIndexCache.java:320), org.apache.hadoop.mapred.TestIndexCache.testInvalidReduceNumberOrLength(TestIndexCache.java:169), org.apache.hadoop.mapred.TestIndexCache.testInvalidReduceNumberOrLength$$CONFUZZ(TestIndexCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestIndexCache/testInvalidReduceNumberOrLength/campaign/failures/debug_000000	{"io.file.buffer.size": "2147483550"}	["debug_000000"]	org.apache.hadoop.mapreduce.split.TestJobSplitWriter#testMaxBlockLocationsOldSplits	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:728), org.apache.hadoop.mapreduce.split.JobSplitWriter.writeJobSplitMetaInfo(JobSplitWriter.java:189), org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:95), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsOldSplits(TestJobSplitWriter.java:74), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsOldSplits$$CONFUZZ(TestJobSplitWriter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriter/testMaxBlockLocationsOldSplits/campaign/failures/debug_000000	{"io.file.buffer.size": "1869076490"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:482), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000002	{"io.file.buffer.size": "2067148020"}	["debug_000002"]	org.apache.hadoop.mapred.TestIndexCache#testCreateRace	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapred.TestIndexCache.writeFile(TestIndexCache.java:320), org.apache.hadoop.mapred.TestIndexCache.testCreateRace(TestIndexCache.java:262), org.apache.hadoop.mapred.TestIndexCache.testCreateRace$$CONFUZZ(TestIndexCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestIndexCache/testCreateRace/campaign/failures/debug_000000	{"io.file.buffer.size": "2078422113"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:285), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000000	{"io.file.buffer.size": "1223569515"}	["debug_000000"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithoutScheme	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:331), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:325), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentials(TestTokenCache.java:99), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithoutScheme(TestTokenCache.java:68), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithoutScheme$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithoutScheme/campaign/failures/debug_000004	{"io.file.buffer.size": "2131239812"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:438), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377), org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:154), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:166), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryV2/campaign/failures/debug_000006	{"io.file.buffer.size": "1530049726"}	["debug_000006"]																																																																													
			9	org.apache.hadoop.mapred.TestIndexCache#testRemoveMap	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapred.TestIndexCache.writeFile(TestIndexCache.java:320), org.apache.hadoop.mapred.TestIndexCache.testRemoveMap(TestIndexCache.java:218), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestIndexCache/testRemoveMap/campaign/failures/debug_000001	{"io.file.buffer.size": "2130640638"}	["debug_000001"]	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager#testDetermineTimestamps	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:333), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.createTempFile(TestClientDistributedCacheManager.java:249), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.setup(TestClientDistributedCacheManager.java:78), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager/testDetermineTimestamps/campaign/failures/debug_000003	{"io.file.buffer.size": "2070905297"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000008	{"io.file.buffer.size": "2130640638"}	["debug_000008"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:440), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377), org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:302), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000008	{"io.file.buffer.size": "1899461088"}	["debug_000008"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:285), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000004	{"io.file.buffer.size": "1073840975"}	["debug_000004"]	org.apache.hadoop.mapreduce.split.TestJobSplitWriter#testMaxBlockLocationsNewSplits	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:728), org.apache.hadoop.mapreduce.split.JobSplitWriter.createFile(JobSplitWriter.java:102), org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:78), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsNewSplits(TestJobSplitWriter.java:50), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriter/testMaxBlockLocationsNewSplits/campaign/failures/debug_000002	{"io.file.buffer.size": "2120626818"}	["debug_000002"]	org.apache.hadoop.mapred.TestIndexCache#testCreateRace	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapred.TestIndexCache.writeFile(TestIndexCache.java:320), org.apache.hadoop.mapred.TestIndexCache.testCreateRace(TestIndexCache.java:262), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestIndexCache/testCreateRace/campaign/failures/debug_000001	{"io.file.buffer.size": "2134274913"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:413), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:725), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000006	{"io.file.buffer.size": "2118613593"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV2	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:214), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV2/campaign/failures/debug_000002	{"io.file.buffer.size": "2107113331"}	["debug_000002"]																																																																																																																									
		FP	1	org.apache.hadoop.mapred.TestLineRecordReader#testRecordSpanningMultipleSplitsCompressed	java.lang.AssertionError	Wrong number of records expected:<4> but was:<3>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.apache.hadoop.mapred.TestLineRecordReader.checkRecordSpanningMultipleSplits(TestLineRecordReader.java:252), org.apache.hadoop.mapred.TestLineRecordReader.testRecordSpanningMultipleSplitsCompressed(TestLineRecordReader.java:279), org.apache.hadoop.mapred.TestLineRecordReader.testRecordSpanningMultipleSplitsCompressed$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testRecordSpanningMultipleSplitsCompressed/campaign/failures/debug_000000	{"mapreduce.input.linerecordreader.line.maxlength": "13018"}	["debug_000000"]																																																																																																																																																																																																																	
		FP	1	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testCopyFromHostWait	org.mockito.exceptions.verification.NeverWantedButInvoked	counter.increment(1L);Never wanted here:-> at org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCopyFromHostWait(TestFetcher.java:322)But invoked here:-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:585)	org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCopyFromHostWait(TestFetcher.java:322), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCopyFromHostWait$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testCopyFromHostWait/campaign/failures/debug_000000	{"mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000000"]																																																																																																																																																																																																																	
	Bug-2	Repeated	2	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager#testDetermineTimestamps	java.io.IOException	-443488098	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.createTempFile(TestClientDistributedCacheManager.java:254), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.setup(TestClientDistributedCacheManager.java:78), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager/testDetermineTimestamps/campaign/failures/debug_000000	{"file.bytes-per-checksum": "1382379310"}	["debug_000000"]	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager#testDetermineCacheVisibilities	java.io.IOException	-1358084722	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.createTempFile(TestClientDistributedCacheManager.java:254), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.setup(TestClientDistributedCacheManager.java:78), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager/testDetermineCacheVisibilities/campaign/failures/debug_000002	{"file.bytes-per-checksum": "326320286"}	["debug_000002"]																																																																																																																																																																																																						
	Bug-7	Repeated	11	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager#testDetermineTimestamps	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:333), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.createTempFile(TestClientDistributedCacheManager.java:249), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.setup(TestClientDistributedCacheManager.java:78), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager/testDetermineTimestamps/campaign/failures/debug_000002	{"file.bytes-per-checksum": "1964554009"}	["debug_000002"]	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputWithLargeSplitSize	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputWithLargeSplitSize(TestLineRecordReader.java:377), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputWithLargeSplitSize$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputWithLargeSplitSize/campaign/failures/debug_000000	{"file.bytes-per-checksum": "1510477730"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000003	{"file.bytes-per-checksum": "2081851168"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000005	{"file.bytes-per-checksum": "1606509685"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testUncompressedInputCustomDelimiterPosValue	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.createInputFile(TestLineRecordReader.java:312), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputCustomDelimiterPosValue(TestLineRecordReader.java:417), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputCustomDelimiterPosValue$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testUncompressedInputCustomDelimiterPosValue/campaign/failures/debug_000000	{"file.bytes-per-checksum": "1985861671"}	["debug_000000"]	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputCustomDelimiterPosValue	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputCustomDelimiterPosValue(TestLineRecordReader.java:484), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputCustomDelimiterPosValue$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputCustomDelimiterPosValue/campaign/failures/debug_000000	{"file.bytes-per-checksum": "217111000"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:639), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000001	{"file.bytes-per-checksum": "2110669750"}	["debug_000001"]	org.apache.hadoop.mapreduce.split.TestJobSplitWriter#testMaxBlockLocationsOldSplits	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:728), org.apache.hadoop.mapreduce.split.JobSplitWriter.createFile(JobSplitWriter.java:102), org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:91), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsOldSplits(TestJobSplitWriter.java:74), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsOldSplits$$CONFUZZ(TestJobSplitWriter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriter/testMaxBlockLocationsOldSplits/campaign/failures/debug_000001	{"file.bytes-per-checksum": "665441245"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testUncompressedInputDefaultDelimiterPosValue	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.createInputFile(TestLineRecordReader.java:312), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue(TestLineRecordReader.java:559), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testUncompressedInputDefaultDelimiterPosValue/campaign/failures/debug_000000	{"file.bytes-per-checksum": "2133575896"}	["debug_000000"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithoutScheme	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:331), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:325), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentials(TestTokenCache.java:99), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithoutScheme(TestTokenCache.java:68), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithoutScheme$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithoutScheme/campaign/failures/debug_000000	{"file.bytes-per-checksum": "621031299"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2(TestFileOutputCommitter.java:478), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV2/campaign/failures/debug_000000	{"file.bytes-per-checksum": "695819820"}	["debug_000000"]																																																																																																			
	Bug-27	Repeated	1	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000006	{"io.file.buffer.size": "796477728", "file.stream-buffer-size": "394561934", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000006"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.NegativeArraySizeException	-1878753217	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "file.bytes-per-checksum": "268468231"}	["debug_000002"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:220), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "io.file.buffer.size": "1471704817", "file.stream-buffer-size": "1713462465"}	["debug_000004"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000008	{"mapreduce.output.fileoutputformat.compress": "true", "io.file.buffer.size": "2081436765"}	["debug_000008"]																																																																																																																																																																																																																	
			2	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.io.FileNotFoundException	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/output/_temporary/1/task_200707121733_0001_m_000000/part-00000 (No such file or directory)	java.base/java.io.FileInputStream.open0(Native Method), java.base/java.io.FileInputStream.open(FileInputStream.java:219), java.base/java.io.FileInputStream.<init>(FileInputStream.java:157), org.apache.hadoop.mapred.TestFileOutputCommitter.slurp(TestFileOutputCommitter.java:570), org.apache.hadoop.mapred.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:184), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:121), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:92), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheDisabled	java.io.FileNotFoundException	File file:/tmp/first-archive.zip does not exist	org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462), org.apache.hadoop.mapreduce.JobResourceUploader.getFileStatus(JobResourceUploader.java:647), org.apache.hadoop.mapreduce.JobResourceUploader.explorePath(JobResourceUploader.java:629), org.apache.hadoop.mapreduce.JobResourceUploader.checkLocalizationLimits(JobResourceUploader.java:515), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:198), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheDisabled(TestJobResourceUploaderWithSharedCache.java:163), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheDisabled$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:85), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheDisabled/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]																																																																																																																																																																																																						
			21	org.apache.hadoop.mapred.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.NegativeArraySizeException	-734686068	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:109), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:98), org.apache.hadoop.mapred.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:171), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.NegativeArraySizeException	-1021545674	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:232), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000008	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000008"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.NegativeArraySizeException	-1077806379	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:243), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:232), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:214), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000003	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.NegativeArraySizeException	-1236464881	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:444), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000016	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000016"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.lang.NegativeArraySizeException	-299074197	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:328), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000002	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.NegativeArraySizeException	-483272639	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:266), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.NegativeArraySizeException	-1695874759	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:284), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.NegativeArraySizeException	-462345546	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:284), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000010	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000010"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV2	java.lang.NegativeArraySizeException	-1534454643	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:603), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:644), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000006	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV1	java.lang.NegativeArraySizeException	-299902557	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:639), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:603), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:639), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.NegativeArraySizeException	-661283355	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:370), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000009	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000009"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NegativeArraySizeException	-1184064011	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:370), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000006	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV1	java.lang.NegativeArraySizeException	-977728526	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:446), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000003	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV1	java.lang.NegativeArraySizeException	-1434055017	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:266), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000006	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV1	java.lang.NegativeArraySizeException	-512387378	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:144), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000000	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NegativeArraySizeException	-744295409	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:539), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000003	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.NegativeArraySizeException	-165769195	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:336), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:346), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:336), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000002	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.NegativeArraySizeException	-1824131272	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:144), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV1	java.lang.NegativeArraySizeException	-1627298071	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1(TestFileOutputCommitter.java:473), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:484), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1(TestFileOutputCommitter.java:473), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV1/campaign/failures/debug_000007	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV2	java.lang.NegativeArraySizeException	-171012487	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2(TestFileOutputCommitter.java:478), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:484), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2(TestFileOutputCommitter.java:478), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV2/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000001"]
			2	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV2	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.BlockCompressorStream.<init>(BlockCompressorStream.java:56), org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:92), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137),	DIFFERENT	java.lang.OutOfMemoryError	Direct buffer memory	java.base/java.nio.Bits.reserveMemory(Bits.java:175), java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118), java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317), org.apache.hadoop.io.compress.snappy.SnappyCompressor.<init>(SnappyCompressor.java:60), org.apache.hadoop.io.compress.SnappyCodec.createCompressor(SnappyCodec.java:116), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress": "true", "io.compression.codec.snappy.buffersize": "2055459171", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.SnappyCodec"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.OutOfMemoryError	Direct buffer memory	java.base/java.nio.Bits.reserveMemory(Bits.java:175), java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118), java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317), org.apache.hadoop.io.compress.snappy.SnappyDecompressor.<init>(SnappyDecompressor.java:56), org.apache.hadoop.io.compress.SnappyCodec.createDecompressor(SnappyCodec.java:172), org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:183), org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:2066), org.apache.hadoop.io.SequenceFile$Reader.initialize(SequenceFile.java:1940), org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1889), org.apache.hadoop.io.MapFile$Reader.createDataFileReader(MapFile.java:460), org.apache.hadoop.io.MapFile$Reader.open(MapFile.java:433), org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:403), org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:412), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getReaders(MapFileOutputFormat.java:108), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:566), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Direct buffer memory	java.base/java.nio.Bits.reserveMemory(Bits.java:175), java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:118), java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:317), org.apache.hadoop.io.compress.snappy.SnappyDecompressor.<init>(SnappyDecompressor.java:56), org.apache.hadoop.io.compress.SnappyCodec.createDecompressor(SnappyCodec.java:172), org.apache.hadoop.io.compress.CodecPool.getDecompressor(CodecPool.java:183), org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:2054), org.apache.hadoop.io.SequenceFile$Reader.initialize(SequenceFile.java:1940), org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1889), org.apache.hadoop.io.MapFile$Reader.createDataFileReader(MapFile.java:460), org.apache.hadoop.io.MapFile$Reader.open(MapFile.java:433), org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:403), org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:412), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getReaders(MapFileOutputFormat.java:108), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:566), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000015	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.SnappyCodec", "io.compression.codec.snappy.buffersize": "829956705"}	["debug_000015"]																																																																																																																																																																																																						
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.ZStandardCodec.createOutputStream(ZStandardCodec.java:125), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:511), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000005	{"hadoop.security.authentication": "kerberos", "fs.faildel.impl.disable.cache": "true"}	["debug_000005"]																																																																																																																																																																																																																	
		FP	2	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV2	java.lang.AssertionError	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/output/_temporary/1/_temporary/_attempt_200707121733_0001_m_000000_0/part-00000 does not exists	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:543), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV1	java.lang.AssertionError	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/output/_temporary/1/_temporary/_attempt_200707121733_0001_m_000000_0/part-00000 does not exists	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:543), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:560), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true"}	["debug_000000"]																																																																																																																																																																																																						
	Bug-52	Repeated	4	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV2	java.lang.ArrayIndexOutOfBoundsException		org.apache.hadoop.io.compress.snappy.SnappyCompressor.setInput(SnappyCompressor.java:86), org.apache.hadoop.io.compress.BlockCompressorStream.write(BlockCompressorStream.java:112), java.base/java.io.DataOutputStream.write(DataOutputStream.java:107), org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.writeObject(TextOutputFormat.java:71), org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.write(TextOutputFormat.java:86), org.apache.hadoop.mapred.TestFileOutputCommitter.writeOutput(TestFileOutputCommitter.java:61), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:531), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:565), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000003	{"io.compression.codec.snappy.buffersize": "10", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.SnappyCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000003"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.ArrayIndexOutOfBoundsException		org.apache.hadoop.io.compress.snappy.SnappyCompressor.setInput(SnappyCompressor.java:86), org.apache.hadoop.io.compress.BlockCompressorStream.write(BlockCompressorStream.java:112), java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:81), java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:142), java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:182), java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:191), org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.close(WritableSerialization.java:103), org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:1397), org.apache.hadoop.io.MapFile$Writer.close(MapFile.java:306), org.apache.hadoop.mapred.MapFileOutputFormat$1.close(MapFileOutputFormat.java:80), org.apache.hadoop.mapred.TestFileOutputCommitter.writeMapFileOutput(TestFileOutputCommitter.java:85), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:382), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000014	{"mapreduce.output.fileoutputformat.compress": "true", "io.compression.codec.snappy.buffersize": "31", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.SnappyCodec"}	["debug_000014"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV1	java.lang.ArrayIndexOutOfBoundsException		org.apache.hadoop.io.compress.snappy.SnappyCompressor.setInput(SnappyCompressor.java:86), org.apache.hadoop.io.compress.BlockCompressorStream.write(BlockCompressorStream.java:112), java.base/java.io.DataOutputStream.write(DataOutputStream.java:107), org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.writeObject(TextOutputFormat.java:71), org.apache.hadoop.mapred.TextOutputFormat$LineRecordWriter.write(TextOutputFormat.java:86), org.apache.hadoop.mapred.TestFileOutputCommitter.writeOutput(TestFileOutputCommitter.java:61), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:531), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:560), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.SnappyCodec", "io.compression.codec.snappy.buffersize": "5"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.ArrayIndexOutOfBoundsException		org.apache.hadoop.io.compress.snappy.SnappyCompressor.setInput(SnappyCompressor.java:86), org.apache.hadoop.io.compress.BlockCompressorStream.write(BlockCompressorStream.java:112), java.base/java.io.DataOutputStream.write(DataOutputStream.java:107), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat$LineRecordWriter.writeObject(TextOutputFormat.java:78), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat$LineRecordWriter.write(TextOutputFormat.java:93), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.writeOutput(TestFileOutputCommitter.java:114), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:162), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.SnappyCodec", "io.compression.codec.snappy.buffersize": "24"}	["debug_000006"]																																																																																																																																																																																
			7	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputWithLargeSplitSize	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputWithLargeSplitSize(TestLineRecordReader.java:377), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputWithLargeSplitSize/campaign/failures/debug_000001	{"file.bytes-per-checksum": "1977852333"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000008	{"file.bytes-per-checksum": "1506675358"}	["debug_000008"]	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputCustomDelimiterPosValue	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputCustomDelimiterPosValue(TestLineRecordReader.java:484), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputCustomDelimiterPosValue/campaign/failures/debug_000001	{"file.bytes-per-checksum": "1073791743"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testAbortV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:619), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:639), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000006	{"file.bytes-per-checksum": "1648090773"}	["debug_000006"]	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputContainingCRLF	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputContainingCRLF(TestLineRecordReader.java:467), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputContainingCRLF/campaign/failures/debug_000003	{"file.bytes-per-checksum": "639722835"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:341), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000003	{"file.bytes-per-checksum": "2052645381"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV2(TestFileOutputCommitter.java:478), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV2/campaign/failures/debug_000004	{"file.bytes-per-checksum": "1612767732"}	["debug_000004"]																																																																																																																																															
			6	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000012	{"mapreduce.output.fileoutputformat.compress.type": "BLOCK", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec"}	["debug_000012"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000008	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress.type": "NONE"}	["debug_000008"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000016	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000016"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000018	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress.type": "NONE", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000018"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000013	{"mapreduce.output.fileoutputformat.compress.type": "NONE", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000013"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000013	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.type": "NONE"}	["debug_000013"]																																																																																																																																																										
		FP	7	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000015	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress.type": "NONE", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000015"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress.type": "BLOCK", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000006"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000012	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress.type": "NONE"}	["debug_000012"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000017	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress.type": "NONE"}	["debug_000017"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000010	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000010"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress.type": "BLOCK", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000006	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000006"]																																																																																																																																															
		FP	4	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000014	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000014"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000011	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000011"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000014	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000014"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.PassthroughCodec"}	["debug_000002"]																																																																																																																																																																																
		FP	7	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress.type": "NONE"}	["debug_000004"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000012	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress.type": "NONE"}	["debug_000012"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000016	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000016"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000011	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.type": "NONE"}	["debug_000011"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000008	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.type": "NONE"}	["debug_000008"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress.type": "BLOCK", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.ZStandardCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000004"]																																																																																																																																															
		FP	7	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000011	{"mapreduce.output.fileoutputformat.compress.type": "NONE", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec"}	["debug_000011"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000007	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000007"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000015	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.type": "BLOCK", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec"}	["debug_000015"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000008	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000008"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress.type": "BLOCK", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000009	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec", "mapreduce.output.fileoutputformat.compress.type": "NONE", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000009"]																																																																																																																																															
		FP	5	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testProgressDuringMerge	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge(TestFileOutputCommitter.java:460), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testProgressDuringMerge$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testProgressDuringMerge/campaign/failures/debug_000007	{"mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000007"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000000	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000016	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec"}	["debug_000016"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000014	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.GzipCodec"}	["debug_000014"]																																																																																																																																																																					
			3	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:328), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000005	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:266), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000007	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000007"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:539), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000015	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000015"]																																																																																																																																																																																											
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000009	{"file.bytes-per-checksum": "713758458"}	["debug_000009"]																																																																																																																																																																																																																	
	Bug-7	Repeated	1	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:360), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000007	{"file.bytes-per-checksum": "649477926", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000007"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:339), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:355), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000005	{"file.bytes-per-checksum": "186253826", "mapreduce.output.fileoutputformat.compress": "true", "file.stream-buffer-size": "1699773305"}	["debug_000005"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestLineRecordReader#testBzip2SplitEndsAtCR	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$122/0x00000008401a7040.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:62), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCR(TestLineRecordReader.java:156), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCR$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$61/0x00000008400ff040.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:62), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCR(TestLineRecordReader.java:156), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testBzip2SplitEndsAtCR/campaign/failures/debug_000000	{"fs.file.impl.disable.cache": "true", "io.file.buffer.size": "2106158333", "hadoop.security.authentication": "kerberos"}	["debug_000000"]																																																																																																																																																																																																																	
			16	org.apache.hadoop.mapred.TestLineRecordReader#testBzip2SplitEndsAtCR	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:123), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:62), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCR(TestLineRecordReader.java:156), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCR$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:111), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:62), org.apache.hadoop.mapred.TestLineRecordReader.testBzip2SplitEndsAtCR(TestLineRecordReader.java:156), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testBzip2SplitEndsAtCR/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testStripBOM	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.<init>(UncompressedSplitLineReader.java:46), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:123), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testStripBOM(TestLineRecordReader.java:250), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testStripBOM$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:49), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:44), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testStripBOM(TestLineRecordReader.java:244), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testStripBOM/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testStripBOM	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:49), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:44), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testStripBOM(TestLineRecordReader.java:244), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testStripBOM/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testConcurrentCommitTaskWithSubDirV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testConcurrentCommitTaskWithSubDirV2/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:672), org.apache.hadoop.mapred.FileOutputFormat.setOutputPath(FileOutputFormat.java:145), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:366), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000002	{"hadoop.security.authentication": "kerberos"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testBzipWithMultibyteDelimiter	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.CompressedSplitLineReader.<init>(CompressedSplitLineReader.java:122), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:103), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:87), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzipWithMultibyteDelimiter(TestLineRecordReader.java:646), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzipWithMultibyteDelimiter$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:49), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:44), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzipWithMultibyteDelimiter(TestLineRecordReader.java:646), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testBzipWithMultibyteDelimiter/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testBzipWithMultibyteDelimiter	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:49), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:44), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzipWithMultibyteDelimiter(TestLineRecordReader.java:646), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testBzipWithMultibyteDelimiter/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader#testProgressIsReportedIfInputASeriesOfEmptyFiles	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$124/0x000000084028fc40.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92), org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper.initialize(CombineFileRecordReaderWrapper.java:69), org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader.initialize(CombineFileRecordReader.java:59), org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader.testProgressIsReportedIfInputASeriesOfEmptyFiles(TestCombineFileRecordReader.java:86), org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader.testProgressIsReportedIfInputASeriesOfEmptyFiles$$CONFUZZ(TestCombineFileRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:49), org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader.testProgressIsReportedIfInputASeriesOfEmptyFiles(TestCombineFileRecordReader.java:80), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader/testProgressIsReportedIfInputASeriesOfEmptyFiles/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV1	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.BlockCompressorStream.<init>(BlockCompressorStream.java:56), org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:92), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.SnappyCodec.createOutputStream(SnappyCodec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:720), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000003	{"hadoop.security.authentication": "kerberos"}	["debug_000003"]	org.apache.hadoop.mapred.TestLineRecordReader#testBzipWithMultibyteDelimiter	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:111), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapred.TestLineRecordReader.testBzipWithMultibyteDelimiter(TestLineRecordReader.java:684), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testBzipWithMultibyteDelimiter/campaign/failures/debug_000003	{"hadoop.security.authentication": "kerberos"}	["debug_000003"]	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheDisabled	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:85), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheDisabled/campaign/failures/debug_000006	{"hadoop.security.authentication": "kerberos"}	["debug_000006"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000003	{"hadoop.security.authentication": "kerberos"}	["debug_000003"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testBzip2SplitEndsAtCR	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:49), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:44), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:81), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:65), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzip2SplitEndsAtCR(TestLineRecordReader.java:123), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testBzip2SplitEndsAtCR/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheEnabledWithJobJarInSharedCache	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:85), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheEnabledWithJobJarInSharedCache/campaign/failures/debug_000004	{"hadoop.security.authentication": "kerberos"}	["debug_000004"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testMultipleClose	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:49), org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.<init>(TaskAttemptContextImpl.java:44), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testMultipleClose(TestLineRecordReader.java:280), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testMultipleClose/campaign/failures/debug_000002	{"hadoop.security.authentication": "kerberos"}	["debug_000002"]																																												
			1	org.apache.hadoop.mapreduce.TestJobSubmissionFiles#testGetStagingWhenFileOwnerNameAndCurrentUserNameDoesNotMatch	java.io.IOException	The ownership on the staging directory Mock for Path, hashCode: 61210602 is not as expected. It is owned by someuser. The directory must be owned by the submitter ctestfuzz or ctestfuzz or user1 or user1@HADOOP.APACHE.ORG	org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:150), org.apache.hadoop.mapreduce.TestJobSubmissionFiles.testGetStagingWhenFileOwnerNameAndCurrentUserNameDoesNotMatch(TestJobSubmissionFiles.java:98), org.apache.hadoop.mapreduce.TestJobSubmissionFiles.testGetStagingWhenFileOwnerNameAndCurrentUserNameDoesNotMatch$$CONFUZZ(TestJobSubmissionFiles.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobSubmissionFiles/testGetStagingWhenFileOwnerNameAndCurrentUserNameDoesNotMatch/campaign/failures/debug_000000	{}	["debug_000000"]																																																																																																																																																																																																																	
	Bug-7	Repeated	5	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000005	{"io.file.buffer.size": "1365512653", "file.stream-buffer-size": "1401372841"}	["debug_000005"]	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager#testDetermineCacheVisibilities	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:333), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.createTempFile(TestClientDistributedCacheManager.java:249), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.setup(TestClientDistributedCacheManager.java:78), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager/testDetermineCacheVisibilities/campaign/failures/debug_000000	{"file.stream-buffer-size": "1339278575", "io.file.buffer.size": "1693839041"}	["debug_000000", "debug_000003"]	org.apache.hadoop.mapreduce.split.TestJobSplitWriter#testMaxBlockLocationsNewSplits	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:728), org.apache.hadoop.mapreduce.split.JobSplitWriter.createFile(JobSplitWriter.java:102), org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:78), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsNewSplits(TestJobSplitWriter.java:50), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsNewSplits$$CONFUZZ(TestJobSplitWriter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriter/testMaxBlockLocationsNewSplits/campaign/failures/debug_000000	{"file.stream-buffer-size": "418241280", "io.file.buffer.size": "1662691481"}	["debug_000000"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2(TestFileOutputCommitter.java:482), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV2/campaign/failures/debug_000005	{"io.file.buffer.size": "795606604", "file.stream-buffer-size": "1343412531"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1(TestFileOutputCommitter.java:473), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV1/campaign/failures/debug_000004	{"io.file.buffer.size": "852326918", "file.stream-buffer-size": "1331628129"}	["debug_000004"]																																																																																																																																																																					
			8	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:223), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:315), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testConcurrentCommitTaskWithSubDirV2	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/SUB_DIR/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:223), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testConcurrentCommitTaskWithSubDir(TestFileOutputCommitter.java:817), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testConcurrentCommitTaskWithSubDirV2(TestFileOutputCommitter.java:828), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testConcurrentCommitTaskWithSubDirV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testConcurrentCommitTaskWithSubDirV2/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV2	java.lang.AssertionError	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/_temporary/1/_temporary/attempt_200707121733_0001_m_000000_0/part-m-00000 does not exists	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:703), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2(TestFileOutputCommitter.java:725), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV2/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV1	java.lang.AssertionError	job temp dir still exists	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertFalse(Assert.java:65), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:311), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV1	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:223), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:315), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1(TestFileOutputCommitter.java:321), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV1/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV2	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:223), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:203), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2(TestFileOutputCommitter.java:214), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV2/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:223), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:370), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:341), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.cleanup(TestFileOutputCommitter.java:95), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.setUp(TestFileOutputCommitter.java:101), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapred.TestLineRecordReader#testRecordSpanningMultipleSplits	java.lang.AssertionError	Wrong number of records expected:<4> but was:<3>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.apache.hadoop.mapred.TestLineRecordReader.checkRecordSpanningMultipleSplits(TestLineRecordReader.java:252), org.apache.hadoop.mapred.TestLineRecordReader.testRecordSpanningMultipleSplits(TestLineRecordReader.java:269), org.apache.hadoop.mapred.TestLineRecordReader.testRecordSpanningMultipleSplits$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:288), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:524), org.apache.hadoop.fs.Path.getFileSystem(Path.java:365), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:111), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:96), org.apache.hadoop.mapred.TestLineRecordReader.readRecords(TestLineRecordReader.java:215), org.apache.hadoop.mapred.TestLineRecordReader.checkRecordSpanningMultipleSplits(TestLineRecordReader.java:249), org.apache.hadoop.mapred.TestLineRecordReader.testRecordSpanningMultipleSplits(TestLineRecordReader.java:269), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testRecordSpanningMultipleSplits/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]																																																																																																																																				
			3	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.AssertionError	job temp dir still exists	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.junit.Assert.assertFalse(Assert.java:65), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:311), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.NegativeArraySizeException	-1344067225	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000001	{"file.bytes-per-checksum": "1759533551"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV1	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/_temporary/1/task_200707121733_0001_m_000000/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:171), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.NegativeArraySizeException	-420443167	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000002	{"file.bytes-per-checksum": "430502681"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterRepeatableV1	java.lang.AssertionError	Commit successful after retry: wrong behavior for version 1.	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:521), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1(TestFileOutputCommitter.java:473), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.NegativeArraySizeException	-2082050153	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRetryInternal(TestFileOutputCommitter.java:505), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterRepeatableV1(TestFileOutputCommitter.java:473), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterRepeatableV1/campaign/failures/debug_000001	{"file.bytes-per-checksum": "723098271"}	["debug_000001"]																																																																																																																																																																																											
			1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterV2	java.lang.NoClassDefFoundError	net/jpountz/lz4/LZ4Factory	org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterInternal(TestFileOutputCommitter.java:286), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterV2(TestFileOutputCommitter.java:326), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterV2/campaign/failures/debug_000003	{"file.bytes-per-checksum": "2029099278"}	["debug_000003"]																																																																																																																																																																																																																	
	Bug-2	BUG	5	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV2	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:438), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter$CommitterWithFailedThenSucceed.commitJobInternal(TestFileOutputCommitter.java:860), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:427), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2(TestFileOutputCommitter.java:394), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV2/campaign/failures/debug_000002	{"file.bytes-per-checksum": "150918984"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testUncompressedInputCustomDelimiterPosValue	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.createInputFile(TestLineRecordReader.java:312), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputCustomDelimiterPosValue(TestLineRecordReader.java:417), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputCustomDelimiterPosValue$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testUncompressedInputCustomDelimiterPosValue/campaign/failures/debug_000002	{"file.bytes-per-checksum": "710482511"}	["debug_000002"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV1	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000007	{"file.bytes-per-checksum": "235590617"}	["debug_000007"]	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputContainingCRLF	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputContainingCRLF(TestLineRecordReader.java:467), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputContainingCRLF$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputContainingCRLF/campaign/failures/debug_000002	{"file.bytes-per-checksum": "2145608438"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV2	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:438), org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:367), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2(TestFileOutputCommitter.java:341), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV2/campaign/failures/debug_000000	{"file.bytes-per-checksum": "167159392"}	["debug_000000"]																																																																																																																																																																					
	Bug-7	Repeated	3	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithFailureV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureInternal(TestFileOutputCommitter.java:420), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1(TestFileOutputCommitter.java:388), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithFailureV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithFailureV1/campaign/failures/debug_000005	{"file.stream-buffer-size": "2071891629"}	["debug_000005"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:285), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000001	{"file.stream-buffer-size": "1496541571"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testUncompressedInputContainingCRLF	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.createInputFile(TestLineRecordReader.java:312), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputContainingCRLF(TestLineRecordReader.java:400), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputContainingCRLF$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testUncompressedInputContainingCRLF/campaign/failures/debug_000001	{"file.stream-buffer-size": "2130640638"}	["debug_000001"]																																																																																																																																																																																											
	Bug-151	Repeated	1	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.AssertionError	Duplicate commit successful: wrong behavior for version 1.	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:311), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000000	{"mapreduce.fileoutputcommitter.cleanup.skipped": "true"}	["debug_000000"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.io.FileNotFoundException	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/output/part-00000 (No such file or directory)	java.base/java.io.FileInputStream.open0(Native Method), java.base/java.io.FileInputStream.open(FileInputStream.java:219), java.base/java.io.FileInputStream.<init>(FileInputStream.java:157), org.apache.hadoop.mapred.TestFileOutputCommitter.slurp(TestFileOutputCommitter.java:570), org.apache.hadoop.mapred.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:184), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:305), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.NegativeArraySizeException	-143938511	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:125), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:295), org.apache.hadoop.mapred.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:269), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000002	{"file.bytes-per-checksum": "938444009", "mapreduce.output.fileoutputformat.compress": "true"}	["debug_000002"]																																																																																																																																																																																																																	
			2	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testSafeguardSplittingUnsplittableFiles	java.io.IOException	not a gzip file	org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496), org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257), org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186), org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111), org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105), java.base/java.io.InputStream.read(InputStream.java:205), org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191), org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227), org.apache.hadoop.util.LineReader.readLine(LineReader.java:185), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:89), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:65), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSafeguardSplittingUnsplittableFiles(TestLineRecordReader.java:138), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSafeguardSplittingUnsplittableFiles$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testSafeguardSplittingUnsplittableFiles/campaign/failures/debug_000000	{}	["debug_000000"]	org.apache.hadoop.mapred.TestLineRecordReader#testSafeguardSplittingUnsplittableFiles	java.io.IOException	not a gzip file	org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.processBasicHeader(BuiltInGzipDecompressor.java:496), org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.executeHeaderState(BuiltInGzipDecompressor.java:257), org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor.decompress(BuiltInGzipDecompressor.java:186), org.apache.hadoop.io.compress.DecompressorStream.decompress(DecompressorStream.java:111), org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:105), java.base/java.io.InputStream.read(InputStream.java:205), org.apache.hadoop.util.LineReader.fillBuffer(LineReader.java:191), org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227), org.apache.hadoop.util.LineReader.readLine(LineReader.java:185), org.apache.hadoop.mapred.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:221), org.apache.hadoop.mapred.LineRecordReader.next(LineRecordReader.java:259), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:86), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecords(TestLineRecordReader.java:62), org.apache.hadoop.mapred.TestLineRecordReader.testSafeguardSplittingUnsplittableFiles(TestLineRecordReader.java:192), org.apache.hadoop.mapred.TestLineRecordReader.testSafeguardSplittingUnsplittableFiles$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testSafeguardSplittingUnsplittableFiles/campaign/failures/debug_000000	{}	["debug_000000"]																																																																																																																																																																																																						
			3	org.apache.hadoop.mapreduce.task.reduce.TestMerger#testEncryptedMerger	org.apache.hadoop.util.DiskChecker$DiskErrorException	Could not find any valid local directory for output/map_1.out	org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462), org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165), org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146), org.apache.hadoop.mapred.MROutputFiles.getInputFileForWrite(MROutputFiles.java:206), org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$InMemoryMerger.merge(MergeManagerImpl.java:467), org.apache.hadoop.mapreduce.task.reduce.TestMerger.testInMemoryAndOnDiskMerger(TestMerger.java:155), org.apache.hadoop.mapreduce.task.reduce.TestMerger.testEncryptedMerger(TestMerger.java:111), org.apache.hadoop.mapreduce.task.reduce.TestMerger.testEncryptedMerger$$CONFUZZ(TestMerger.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMerger/testEncryptedMerger/campaign/failures/debug_000000	{}	["debug_000000"]	org.apache.hadoop.mapreduce.task.reduce.TestMerger#testInMemoryAndOnDiskMerger	org.apache.hadoop.util.DiskChecker$DiskErrorException	Could not find any valid local directory for output/map_1.out	org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462), org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165), org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146), org.apache.hadoop.mapred.MROutputFiles.getInputFileForWrite(MROutputFiles.java:206), org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl$InMemoryMerger.merge(MergeManagerImpl.java:467), org.apache.hadoop.mapreduce.task.reduce.TestMerger.testInMemoryAndOnDiskMerger(TestMerger.java:155), org.apache.hadoop.mapreduce.task.reduce.TestMerger.testInMemoryAndOnDiskMerger$$CONFUZZ(TestMerger.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMerger/testInMemoryAndOnDiskMerger/campaign/failures/debug_000000	{}	["debug_000000"]	org.apache.hadoop.mapred.TestMapTask#testShufflePermissions	org.apache.hadoop.util.DiskChecker$DiskErrorException	Could not find any valid local directory for output/file.out	org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:462), org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:165), org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:146), org.apache.hadoop.mapred.MROutputFiles.getOutputFileForWrite(MROutputFiles.java:71), org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1893), org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1528), org.apache.hadoop.mapred.TestMapTask.testShufflePermissions(TestMapTask.java:75), org.apache.hadoop.mapred.TestMapTask.testShufflePermissions$$CONFUZZ(TestMapTask.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestMapTask/testShufflePermissions/campaign/failures/debug_000000	{}	["debug_000000"]																																																																																																																																																																																											
		FP	6	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithScheme	java.io.IOException	RM_HA_IDS property is not set for HA resource manager	org.apache.hadoop.yarn.client.util.YarnClientUtils.getYarnConfWithRmHaId(YarnClientUtils.java:198), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:99), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:77), org.apache.hadoop.mapred.Master.getMasterPrincipal(Master.java:58), org.apache.hadoop.mapreduce.security.TestTokenCache.setup(TestTokenCache.java:54), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithScheme/campaign/failures/debug_000000	{"yarn.resourcemanager.ha.enabled": "true"}	["debug_000000"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testObtainTokens	java.io.IOException	RM_HA_IDS property is not set for HA resource manager	org.apache.hadoop.yarn.client.util.YarnClientUtils.getYarnConfWithRmHaId(YarnClientUtils.java:198), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:99), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:77), org.apache.hadoop.mapred.Master.getMasterPrincipal(Master.java:58), org.apache.hadoop.mapreduce.security.TestTokenCache.setup(TestTokenCache.java:54), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testObtainTokens/campaign/failures/debug_000001	{"yarn.resourcemanager.ha.enabled": "true"}	["debug_000001"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testCleanUpTokenReferral	java.io.IOException	RM_HA_IDS property is not set for HA resource manager	org.apache.hadoop.yarn.client.util.YarnClientUtils.getYarnConfWithRmHaId(YarnClientUtils.java:198), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:99), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:77), org.apache.hadoop.mapred.Master.getMasterPrincipal(Master.java:58), org.apache.hadoop.mapreduce.security.TestTokenCache.setup(TestTokenCache.java:54), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testCleanUpTokenReferral/campaign/failures/debug_000000	{"yarn.resourcemanager.ha.enabled": "true"}	["debug_000000"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testGetTokensForNamenodes	java.io.IOException	RM_HA_IDS property is not set for HA resource manager	org.apache.hadoop.yarn.client.util.YarnClientUtils.getYarnConfWithRmHaId(YarnClientUtils.java:198), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:99), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:77), org.apache.hadoop.mapred.Master.getMasterPrincipal(Master.java:58), org.apache.hadoop.mapreduce.security.TestTokenCache.setup(TestTokenCache.java:54), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testGetTokensForNamenodes/campaign/failures/debug_000001	{"yarn.resourcemanager.ha.enabled": "true"}	["debug_000001"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithoutScheme	java.io.IOException	RM_HA_IDS property is not set for HA resource manager	org.apache.hadoop.yarn.client.util.YarnClientUtils.getYarnConfWithRmHaId(YarnClientUtils.java:198), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:99), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:77), org.apache.hadoop.mapred.Master.getMasterPrincipal(Master.java:58), org.apache.hadoop.mapreduce.security.TestTokenCache.setup(TestTokenCache.java:54), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithoutScheme/campaign/failures/debug_000002	{"yarn.resourcemanager.ha.enabled": "true"}	["debug_000002"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testSingleTokenFetch	java.io.IOException	RM_HA_IDS property is not set for HA resource manager	org.apache.hadoop.yarn.client.util.YarnClientUtils.getYarnConfWithRmHaId(YarnClientUtils.java:198), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:99), org.apache.hadoop.yarn.client.util.YarnClientUtils.getRmPrincipal(YarnClientUtils.java:77), org.apache.hadoop.mapred.Master.getMasterPrincipal(Master.java:58), org.apache.hadoop.mapreduce.security.TestTokenCache.setup(TestTokenCache.java:54), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testSingleTokenFetch/campaign/failures/debug_000000	{"yarn.resourcemanager.ha.enabled": "true"}	["debug_000000"]																																																																																																																																																										
	Bug-165	Repeated	1	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithScheme	java.lang.NullPointerException		org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentials(TestTokenCache.java:95), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithScheme(TestTokenCache.java:74), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithScheme$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithScheme/campaign/failures/debug_000001	{"mapreduce.framework.name": "classic"}	["debug_000001"]																																																																																																																																																																																																																	
		FP	2	org.apache.hadoop.mapreduce.security.TestTokenCache#testObtainTokens	java.io.IOException	Can't get Master Kerberos principal for use as renewer	org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:134), org.apache.hadoop.mapreduce.security.TestTokenCache.testObtainTokens(TestTokenCache.java:61), org.apache.hadoop.mapreduce.security.TestTokenCache.testObtainTokens$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testObtainTokens/campaign/failures/debug_000000	{"mapreduce.framework.name": "classic"}	["debug_000000"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testSingleTokenFetch	java.io.IOException	Can't get Master Kerberos principal for use as renewer	org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:134), org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:102), org.apache.hadoop.mapreduce.security.TestTokenCache.testSingleTokenFetch(TestTokenCache.java:175), org.apache.hadoop.mapreduce.security.TestTokenCache.testSingleTokenFetch$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testSingleTokenFetch/campaign/failures/debug_000001	{"mapreduce.framework.name": "classic"}	["debug_000001"]																																																																																																																																																																																																						
		FP	1	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testMemoryMerge	java.lang.AssertionError	Should be a memory merge	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testMemoryMerge(TestMergeManager.java:99), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testMemoryMerge$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testMemoryMerge/campaign/failures/debug_000000	{"mapreduce.reduce.merge.memtomem.enabled": "true", "mapreduce.reduce.merge.memtomem.threshold": "1"}	["debug_000000"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.TestJobResourceUploader#testErasureCodingDefault	org.mockito.exceptions.verification.WantedButNotInvoked	Wanted but not invoked:distributedFileSystem.setErasureCodingPolicy(    /,    "replication");-> at org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingSetting(TestJobResourceUploader.java:447)However, there were exactly 3 interactions with this mock:distributedFileSystem.getUri();-> at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:163)distributedFileSystem.exists(/test);-> at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:164)distributedFileSystem.makeQualified(/test);-> at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:170)	org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingSetting(TestJobResourceUploader.java:447), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingDefault(TestJobResourceUploader.java:375), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingDefault$$CONFUZZ(TestJobResourceUploader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingSetting(TestJobResourceUploader.java:440), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingDefault(TestJobResourceUploader.java:375), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploader/testErasureCodingDefault/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos"}	["debug_000000"]																																																																																																																																																																																																																	
			2	org.apache.hadoop.mapreduce.TestJobResourceUploader#testErasureCodingDefault	java.lang.NullPointerException		org.apache.hadoop.mapreduce.JobResourceUploader.jobIDToAppId(JobResourceUploader.java:91), org.apache.hadoop.mapreduce.JobResourceUploader.initSharedCache(JobResourceUploader.java:79), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:134), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingSetting(TestJobResourceUploader.java:442), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingDefault(TestJobResourceUploader.java:375), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingDefault$$CONFUZZ(TestJobResourceUploader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingSetting(TestJobResourceUploader.java:440), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingDefault(TestJobResourceUploader.java:375), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploader/testErasureCodingDefault/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithoutScheme	java.lang.NullPointerException		org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentials(TestTokenCache.java:95), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithoutScheme(TestTokenCache.java:68), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithoutScheme$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3746), org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736), org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540), org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:496), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentials(TestTokenCache.java:84), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithoutScheme(TestTokenCache.java:68), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithoutScheme/campaign/failures/debug_000001	{"hadoop.security.authentication": "kerberos"}	["debug_000001"]																																																																																																																																																																																																						
			1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testConcurrentCommitTaskWithSubDirV1	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/SUB_DIR/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:223), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testConcurrentCommitTaskWithSubDir(TestFileOutputCommitter.java:817), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testConcurrentCommitTaskWithSubDirV1(TestFileOutputCommitter.java:823), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testConcurrentCommitTaskWithSubDirV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testConcurrentCommitTaskWithSubDirV1/campaign/failures/debug_000000	{"fs.defaultFS": "file:///", "hadoop.security.auth_to_local.mechanism": "hadoop", "hadoop.security.groups.cache.secs": "51", "fs.client.resolve.remote.symlinks": "true", "hadoop.service.shutdown.timeout": "016461030s", "hadoop.security.auth_to_local": "RULE:[1:$1] RULE:[2:$1]", "hadoop.security.groups.cache.background.reload": "true", "hadoop.user.group.static.mapping.overrides": "dr.who=;", "mapreduce.output.textoutputformat.separator": "\t", "hadoop.kerberos.keytab.login.autorenewal.enabled": "true", "mapreduce.fileoutputcommitter.cleanup-failures.ignored": "false", "hadoop.security.groups.negative-cache.secs": "807", "mapreduce.fileoutputcommitter.marksuccessfuljobs": "false", "fs.creation.parallel.count": "1587021454", "file.bytes-per-checksum": "1728932227", "fs.automatic.close": "false", "hadoop.security.dns.log-slow-lookups.threshold.ms": "105", "mapreduce.output.basename": "part", "hadoop.security.authentication": "simple", "hadoop.security.dns.log-slow-lookups.enabled": "false", "hadoop.security.groups.cache.background.reload.threads": "3719", "mapreduce.output.fileoutputformat.compress": "false", "fs.local.block.size": "26721", "fs.file.impl.disable.cache": "false", "io.file.buffer.size": "1977423120", "mapreduce.fileoutputcommitter.cleanup.skipped": "false", "hadoop.security.token.service.use_ip": "true", "hadoop.kerberos.min.seconds.before.relogin": "1310558327", "hadoop.security.group.mapping": "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback", "mapreduce.job.application.attempt.id": "18828", "hadoop.security.groups.cache.warn.after.ms": "896039491"}	["debug_000000"]																																																																																																																																																																																																																	
	Bug-7	BUG	1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testConcurrentCommitTaskWithSubDirV1	java.lang.OutOfMemoryError	Java heap space		REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testConcurrentCommitTaskWithSubDirV1/campaign/failures/debug_000001	{"io.file.buffer.size": "2139062398"}	["debug_000001"]																																																																																																																																																																																																																	
		FP	3	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testInterruptInMemory	org.mockito.exceptions.verification.WantedButNotInvoked	Wanted but not invoked:inMemoryMapOutput.abort();-> at org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testInterruptInMemory(TestFetcher.java:636)Actually, there were zero interactions with this mock.	org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testInterruptInMemory(TestFetcher.java:636), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testInterruptInMemory$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testInterruptInMemory/campaign/failures/debug_000000	{"mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000000"]	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testReduceOutOfDiskSpace	org.mockito.exceptions.verification.WantedButNotInvoked	Wanted but not invoked:shuffleSchedulerImpl.reportLocalError(    <any java.io.IOException>);-> at org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testReduceOutOfDiskSpace(TestFetcher.java:155)However, there were exactly 5 interactions with this mock:shuffleSchedulerImpl.getMapsForHost(    localhost);-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:310)shuffleSchedulerImpl.hostFailed("localhost");-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:360)shuffleSchedulerImpl.copyFailed(    attempt_0_0001_m_000001_1,    localhost,    true,    false);-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:362)shuffleSchedulerImpl.putBackKnownMapOutput(    localhost,    attempt_0_0001_m_000002_1);-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:379)shuffleSchedulerImpl.putBackKnownMapOutput(    localhost,    attempt_0_0001_m_000001_1);-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:379)	org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testReduceOutOfDiskSpace(TestFetcher.java:155), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testReduceOutOfDiskSpace$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testReduceOutOfDiskSpace/campaign/failures/debug_000000	{"mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000000"]	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testCopyFromHostWithRetryUnreserve	org.mockito.exceptions.verification.WantedButNotInvoked	Wanted but not invoked:inMemoryMapOutput.abort();-> at org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCopyFromHostWithRetryUnreserve(TestFetcher.java:719)Actually, there were zero interactions with this mock.	org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCopyFromHostWithRetryUnreserve(TestFetcher.java:719), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCopyFromHostWithRetryUnreserve$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testCopyFromHostWithRetryUnreserve/campaign/failures/debug_000000	{"mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000000"]																																																																																																																																																																																											
			1	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputDefaultDelimiterPosValue	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSInputChecker.set(FSInputChecker.java:485), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:173), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$126/0x000000084019d840.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue(TestLineRecordReader.java:629), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue(TestLineRecordReader.java:607), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputDefaultDelimiterPosValue/campaign/failures/debug_000002	{"fs.file.impl.disable.cache": "true", "file.bytes-per-checksum": "625373155", "hadoop.security.authentication": "kerberos"}	["debug_000002"]																																																																																																																																																																																																																	
			2	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputDefaultDelimiterPosValue	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$126/0x000000084019d840.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue(TestLineRecordReader.java:629), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$74/0x00000008400fd840.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue(TestLineRecordReader.java:629), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputDefaultDelimiterPosValue/campaign/failures/debug_000001	{"file.stream-buffer-size": "1473962192", "fs.file.impl.disable.cache": "true"}	["debug_000001"]	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputDefaultDelimiterPosValue	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:166), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$76/0x00000008400fd840.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:115), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue(TestLineRecordReader.java:629), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputDefaultDelimiterPosValue/campaign/failures/debug_000003	{"file.stream-buffer-size": "1875821534", "fs.file.impl.disable.cache": "true"}	["debug_000003"]																																																																																																																																																																																																						
			2	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheArchivesAndLibjarsEnabled	java.io.FileNotFoundException	File file:/tmp/first-input-file does not exist	org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462), org.apache.hadoop.mapreduce.JobResourceUploader.getFileStatus(JobResourceUploader.java:647), org.apache.hadoop.mapreduce.JobResourceUploader.explorePath(JobResourceUploader.java:629), org.apache.hadoop.mapreduce.JobResourceUploader.checkLocalizationLimits(JobResourceUploader.java:511), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:198), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheArchivesAndLibjarsEnabled(TestJobResourceUploaderWithSharedCache.java:204), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheArchivesAndLibjarsEnabled$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.io.IOException	The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem	org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheArchivesAndLibjarsEnabled/campaign/failures/debug_000000	{"dfs.namenode.support.allow.format": "false"}	["debug_000000"]	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheDisabled	java.io.FileNotFoundException	File file:/tmp/first-input-file does not exist	org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462), org.apache.hadoop.mapreduce.JobResourceUploader.getFileStatus(JobResourceUploader.java:647), org.apache.hadoop.mapreduce.JobResourceUploader.explorePath(JobResourceUploader.java:629), org.apache.hadoop.mapreduce.JobResourceUploader.checkLocalizationLimits(JobResourceUploader.java:511), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:198), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheDisabled(TestJobResourceUploaderWithSharedCache.java:163), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheDisabled$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.io.IOException	The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem	org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheDisabled/campaign/failures/debug_000001	{"dfs.namenode.support.allow.format": "false"}	["debug_000001"]																																																																																																																																																																																																						
			1	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheArchivesAndLibjarsEnabled	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LightWeightGSet.<init>(LightWeightGSet.java:92), org.apache.hadoop.hdfs.server.namenode.INodeMap.newInstance(INodeMap.java:40), org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:320), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:977), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	DIFFERENT	java.io.FileNotFoundException	File file:/tmp/first-input-file does not exist	org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462), org.apache.hadoop.mapreduce.JobResourceUploader.getFileStatus(JobResourceUploader.java:647), org.apache.hadoop.mapreduce.JobResourceUploader.explorePath(JobResourceUploader.java:629), org.apache.hadoop.mapreduce.JobResourceUploader.checkLocalizationLimits(JobResourceUploader.java:511), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:198), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheArchivesAndLibjarsEnabled(TestJobResourceUploaderWithSharedCache.java:204), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheArchivesAndLibjarsEnabled/campaign/failures/debug_000002	{"mapreduce.job.cache.limit.max-resources-mb": "19600"}	["debug_000002"]																																																																																																																																																																																																																	
	Bug-101	Repeated	1	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheArchivesAndLibjarsEnabled	org.apache.hadoop.ipc.RemoteException	Requested replication factor of -32001 is less than the required minimum of 1 for /tmp/hadoop-yarn/staging/files/second-input-file at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.verifyReplication(BlockManager.java:1611) at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setReplication(FSDirAttrOp.java:135) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplication(FSNamesystem.java:2348) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setReplication(NameNodeRpcServer.java:851) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setReplication(ClientNamenodeProtocolServerSideTranslatorPB.java:540) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy30.setReplication(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setReplication(ClientNamenodeProtocolTranslatorPB.java:432), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy31.setReplication(Unknown Source), org.apache.hadoop.hdfs.DFSClient.setReplication(DFSClient.java:1490), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:737), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:734), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.setReplication(DistributedFileSystem.java:744), org.apache.hadoop.mapreduce.JobResourceUploader.copyRemoteFiles(JobResourceUploader.java:704), org.apache.hadoop.mapreduce.JobResourceUploader.uploadFiles(JobResourceUploader.java:258), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:205), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheArchivesAndLibjarsEnabled(TestJobResourceUploaderWithSharedCache.java:204), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheArchivesAndLibjarsEnabled$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheArchivesAndLibjarsEnabled/campaign/failures/debug_000001	{"mapreduce.client.submit.file.replication": "33587967"}	["debug_000001"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheArchivesAndLibjarsEnabled	java.lang.OutOfMemoryError	Java heap space		PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheArchivesAndLibjarsEnabled/campaign/failures/debug_000004	{"dfs.image.parallel.inode.threshold": "138551836", "ipc.client.connect.timeout": "767", "dfs.use.dfs.network.topology": "false", "dfs.datanode.du.reserved.disk": "767", "dfs.content-summary.limit": "767", "dfs.datanode.fsdatasetcache.max.threads.per.volume": "22060", "dfs.namenode.name.cache.threshold": "7242", "ipc.client.connect.retry.interval": "17936", "dfs.image.compress": "true", "dfs.namenode.tolerate.heartbeat.multiplier": "1396852607", "dfs.disk.balancer.enabled": "false", "dfs.client.read.shortcircuit.metrics.sampling.percentage": "436638143", "dfs.disk.balancer.block.tolerance.percent": "16048", "fs.client.resolve.topology.enabled": "true"}	["debug_000004"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheArchivesAndLibjarsEnabled	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LightWeightGSet.<init>(LightWeightGSet.java:92), org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$1.<init>(BlocksMap.java:77), org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.<init>(BlocksMap.java:77), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:496), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103),	DIFFERENT	org.apache.hadoop.HadoopIllegalArgumentException	Invalid value configured for dfs.datanode.failed.volumes.tolerated - 11081. Value configured is >= to the number of configured volumes (2).	org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.check(StorageLocationChecker.java:178), org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2821), org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2734), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1755), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheArchivesAndLibjarsEnabled/campaign/failures/debug_000003	{"dfs.datanode.failed.volumes.tolerated": "11081"}	["debug_000003"]																																																																																																																																																																																																																	
	Bug-50	Repeated	3	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.NegativeArraySizeException	-2022686140	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000015	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.type": "BLOCK", "file.bytes-per-checksum": "252475684"}	["debug_000015"]	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NegativeArraySizeException	-2102121242	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000014	{"mapreduce.output.fileoutputformat.compress": "true", "file.bytes-per-checksum": "720868150", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000014"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.NegativeArraySizeException	-609613437	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000010	{"file.bytes-per-checksum": "1363920939", "mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000010"]																																																																																																																																																																																											
		FP	2	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000005	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000005"]	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:582), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000001	{"mapreduce.output.fileoutputformat.compress": "true", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000001"]																																																																																																																																																																																																						
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV2	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV2(TestFileOutputCommitter.java:402), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV2/campaign/failures/debug_000003	{"file.bytes-per-checksum": "1983740620"}	["debug_000003"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1569), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:290), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000004	{"mapreduce.output.fileoutputformat.compress": "true", "file.bytes-per-checksum": "1485446340", "mapreduce.output.fileoutputformat.compress.type": "BLOCK"}	["debug_000004"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.IllegalArgumentException	SequenceFile doesn't work with GzipCodec without native-hadoop code!	org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1187), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:370), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000011	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000011"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.RuntimeException	native zStandard library not available: this version of libhadoop was built without zstd support.	org.apache.hadoop.io.compress.ZStandardCodec.checkNativeCodeLoaded(ZStandardCodec.java:65), org.apache.hadoop.io.compress.ZStandardCodec.getCompressorType(ZStandardCodec.java:153), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1170), org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1499), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:288), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapred.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:68), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:381), org.apache.hadoop.mapred.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:397), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000003	{"mapreduce.output.fileoutputformat.compress": "true", "file.bytes-per-checksum": "1596870039"}	["debug_000003"]																																																																																																																																																																																																																	
	Bug-77	Repeated	3	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testInterruptOnDisk	java.lang.IllegalArgumentException	Minimum value of buffer size is 512.	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.CryptoStreamUtils.checkBufferSize(CryptoStreamUtils.java:70), org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:104), org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:38), org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:141), org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream.wrapIfNecessary(IntermediateEncryptedStream.java:46), org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput.<init>(OnDiskMapOutput.java:87), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testInterruptOnDisk(TestFetcher.java:645), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testInterruptOnDisk$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testInterruptOnDisk/campaign/failures/debug_000001	{"mapreduce.job.encrypted-intermediate-data": "true", "mapreduce.job.encrypted-intermediate-data.buffer.kb": "632442548"}	["debug_000001"]	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testZeroShuffleMemoryLimitPercent	java.lang.IllegalArgumentException	Minimum value of buffer size is 512.	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.CryptoStreamUtils.checkBufferSize(CryptoStreamUtils.java:70), org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:104), org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:38), org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:141), org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream.wrapIfNecessary(IntermediateEncryptedStream.java:46), org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput.<init>(OnDiskMapOutput.java:87), org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.reserve(MergeManagerImpl.java:274), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.verifyReservedMapOutputType(TestMergeManager.java:309), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testZeroShuffleMemoryLimitPercent(TestMergeManager.java:323), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testZeroShuffleMemoryLimitPercent$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testZeroShuffleMemoryLimitPercent/campaign/failures/debug_000003	{"mapreduce.job.encrypted-intermediate-data.buffer.kb": "1429102949", "mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000003"]	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testCorruptedIFile	java.lang.IllegalArgumentException	Minimum value of buffer size is 512.	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.crypto.CryptoStreamUtils.checkBufferSize(CryptoStreamUtils.java:70), org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:104), org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:38), org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:141), org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream.wrapIfNecessary(IntermediateEncryptedStream.java:46), org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput.<init>(OnDiskMapOutput.java:87), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCorruptedIFile(TestFetcher.java:539), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCorruptedIFile$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testCorruptedIFile/campaign/failures/debug_000002	{"mapreduce.job.encrypted-intermediate-data.buffer.kb": "417711482", "mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000002"]																																																																																																																																																																																											
			1	org.apache.hadoop.mapred.TestTaskProgressReporter#testTaskProgress	org.junit.ComparisonFailure	expected:<[2]> but was:<[4]>	org.apache.hadoop.mapred.TestTaskProgressReporter.testTaskProgress(TestTaskProgressReporter.java:273), org.apache.hadoop.mapred.TestTaskProgressReporter.testTaskProgress$$CONFUZZ(TestTaskProgressReporter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestTaskProgressReporter/testTaskProgress/campaign/failures/debug_000000	{}	["debug_000000"]																																																																																																																																																																																																																	
	Bug-42	Repeated	1	org.apache.hadoop.mapreduce.task.reduce.TestShuffleScheduler#TestSucceedAndFailedCopyMap	java.lang.ArithmeticException	/ by zero	org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.checkAndInformMRAppMaster(ShuffleSchedulerImpl.java:347), org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:308), org.apache.hadoop.mapreduce.task.reduce.TestShuffleScheduler.TestSucceedAndFailedCopyMap(TestShuffleScheduler.java:285), org.apache.hadoop.mapreduce.task.reduce.TestShuffleScheduler.TestSucceedAndFailedCopyMap$$CONFUZZ(TestShuffleScheduler.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestShuffleScheduler/TestSucceedAndFailedCopyMap/campaign/failures/debug_000000	{"mapreduce.reduce.shuffle.notify.readerror": "false", "mapreduce.reduce.shuffle.maxfetchfailures": "0"}	["debug_000000"]																																																																																																																																																																																																																	
	Bug-69	Repeated	1	org.apache.hadoop.mapreduce.TestJobResourceUploader#testErasureCodingDisabled	java.lang.NullPointerException		org.apache.hadoop.mapreduce.JobResourceUploader.jobIDToAppId(JobResourceUploader.java:91), org.apache.hadoop.mapreduce.JobResourceUploader.initSharedCache(JobResourceUploader.java:79), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:134), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingSetting(TestJobResourceUploader.java:442), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingDisabled(TestJobResourceUploader.java:380), org.apache.hadoop.mapreduce.TestJobResourceUploader.testErasureCodingDisabled$$CONFUZZ(TestJobResourceUploader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploader/testErasureCodingDisabled/campaign/failures/debug_000000	{"mapreduce.job.sharedcache.mode": "archives", "mapreduce.framework.name": "yarn", "yarn.sharedcache.enabled": "true"}	["debug_000000"]																																																																																																																																																																																																																	
		FP	1	org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager#testDetermineCacheVisibilities	java.io.FileNotFoundException	File file:/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/secondcachefile does not exist	org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779), org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100), org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769), org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462), org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.getFileStatus(ClientDistributedCacheManager.java:325), org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.checkPermissionOfOther(ClientDistributedCacheManager.java:294), org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.isPublic(ClientDistributedCacheManager.java:258), org.apache.hadoop.mapreduce.filecache.ClientDistributedCacheManager.determineCacheVisibilities(ClientDistributedCacheManager.java:178), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.testDetermineCacheVisibilities(TestClientDistributedCacheManager.java:152), org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager.testDetermineCacheVisibilities$$CONFUZZ(TestClientDistributedCacheManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.filecache.TestClientDistributedCacheManager/testDetermineCacheVisibilities/campaign/failures/debug_000001	{"fs.file.impl.disable.cache": "true"}	["debug_000001"]																																																																																																																																																																																																																	
		FP	1	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testCopyFromHostWithRetryThenTimeout	org.mockito.exceptions.verification.junit.ArgumentsAreDifferent	Argument(s) are different! Wanted:shuffleSchedulerImpl.copyFailed(    attempt_0_0001_m_000001_1,    localhost,    false,    false);-> at org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCopyFromHostWithRetryThenTimeout(TestFetcher.java:476)Actual invocations have different arguments:shuffleSchedulerImpl.getMapsForHost(    localhost);-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:310)shuffleSchedulerImpl.hostFailed(    "localhost");-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:360)shuffleSchedulerImpl.copyFailed(    attempt_0_0001_m_000001_1,    localhost,    true,    false);-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:362)shuffleSchedulerImpl.putBackKnownMapOutput(    localhost,    attempt_0_0001_m_000002_1);-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:379)shuffleSchedulerImpl.putBackKnownMapOutput(    localhost,    attempt_0_0001_m_000001_1);-> at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:379)	org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCopyFromHostWithRetryThenTimeout(TestFetcher.java:476), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCopyFromHostWithRetryThenTimeout$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testCopyFromHostWithRetryThenTimeout/campaign/failures/debug_000000	{"mapreduce.reduce.shuffle.fetch.retry.timeout-ms": "0"}	["debug_000000"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testFailAbortV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.createOutputStream(PassthroughCodec.java:113), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:127), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:529), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:560), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:35), org.apache.hadoop.mapred.JobContextImpl.<init>(JobContextImpl.java:41), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:511), org.apache.hadoop.mapred.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:560), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000003	{"fs.faildel.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000003"]																																																																																																																																																																																																																	
			2	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerUriSubstitution	org.junit.internal.runners.model.MultipleFailureException	There were 2 errors:  java.lang.OutOfMemoryError(Java heap space)  java.lang.NullPointerException(null)	org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534), org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:511), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:111), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerUriSubstitution/campaign/failures/debug_000000	{"hadoop.http.acceptor.count": "2027683095"}	["debug_000000"]	org.apache.hadoop.mapred.TestJobEndNotifier#testNotificationTimeout	org.junit.internal.runners.model.MultipleFailureException	There were 2 errors:  java.lang.OutOfMemoryError(Java heap space)  java.lang.NullPointerException(null)	org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534), org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:511), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:111), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testNotificationTimeout/campaign/failures/debug_000002	{"hadoop.http.acceptor.count": "808263455"}	["debug_000002"]																																																																																																																																																																																																						
		FP	2	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerUriSubstitution	java.lang.IllegalStateException	Insufficient configured threads: required=1 < max=1 for QueuedThreadPool[qtp1299471046]@4d745ac6{STARTING,1<=0<=1,i=0,r=-1,q=0}[ReservedThreadExecutor@2262bc86{s=0/1,p=0}]	org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.util.thread.ReservedThreadExecutor.doStart(ReservedThreadExecutor.java:163), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117), org.eclipse.jetty.util.thread.QueuedThreadPool.doStart(QueuedThreadPool.java:214), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.server.Server.start(Server.java:423), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117), org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97), org.eclipse.jetty.server.Server.doStart(Server.java:387), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerUriSubstitution/campaign/failures/debug_000002	{"hadoop.http.max.threads": "1"}	["debug_000002"]	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerRetryCount	java.lang.IllegalStateException	Insufficient configured threads: required=1 < max=1 for QueuedThreadPool[qtp1532904892]@5b5e45bc{STARTING,1<=0<=1,i=0,r=-1,q=0}[ReservedThreadExecutor@678b3746{s=0/1,p=0}]	org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.util.thread.ReservedThreadExecutor.doStart(ReservedThreadExecutor.java:163), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117), org.eclipse.jetty.util.thread.QueuedThreadPool.doStart(QueuedThreadPool.java:214), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.server.Server.start(Server.java:423), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117), org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97), org.eclipse.jetty.server.Server.doStart(Server.java:387), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerRetryCount/campaign/failures/debug_000004	{"hadoop.http.max.threads": "1"}	["debug_000004"]																																																																																																																																																																																																						
		FP	3	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerUriSubstitution	java.lang.IllegalStateException	Insufficient configured threads: required=302022658 < max=200 for QueuedThreadPool[qtp1629827491]@612531a3{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@1d5958d3{s=0/2,p=0}]	org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:321), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81), org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:234), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.server.Server.doStart(Server.java:401), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerUriSubstitution/campaign/failures/debug_000001	{"hadoop.http.selector.count": "302022655"}	["debug_000001"]	org.apache.hadoop.mapred.TestJobEndNotifier#testNotificationTimeout	java.lang.IllegalStateException	Insufficient configured threads: required=11551 < max=200 for QueuedThreadPool[qtp1602310266]@5f81507a{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@37ab1b10{s=0/2,p=0}]	org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:321), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81), org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:234), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.server.Server.doStart(Server.java:401), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testNotificationTimeout/campaign/failures/debug_000000	{"hadoop.http.selector.count": "11548"}	["debug_000000"]	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerRetryCount	java.lang.IllegalStateException	Insufficient configured threads: required=515 < max=200 for QueuedThreadPool[qtp1706518410]@65b7678a{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@1ecd09d5{s=0/2,p=0}]	org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:321), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81), org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:234), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.server.Server.doStart(Server.java:401), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerRetryCount/campaign/failures/debug_000001	{"hadoop.http.selector.count": "512"}	["debug_000001"]																																																																																																																																																																																											
			1	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerUriSubstitution	java.lang.NullPointerException	config	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkNotNull(Preconditions.java:899), org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSink(MetricsSystemImpl.java:298), org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:277), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1279), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerUriSubstitution/campaign/failures/debug_000004	{"hadoop.security.token.service.use_ip": "false", "hadoop.prometheus.endpoint.enabled": "true", "hadoop.http.max.threads": "26310", "hadoop.security.dns.log-slow-lookups.enabled": "true", "hadoop.http.max.request.header.size": "849", "hadoop.http.staticuser.user": "XQITiMXrVAfhdRwIkxQHJyDrAMKxJ.bScJFExZlzMgKxhBpIdjqdgYRSuZzVSb.nHDrrxSYqkvxVQZNDaSzZumQkafANbshleRVUnOWcmXtDXxF.wiWwBNSPxJvCYMHsRU.fHJnlfGmXFKEDkDrLrFlSvm.sXqYVwzcvZTdDeojvhmiPGoCKimNFzbPGbiazaDdalQzUJvKVlrwvqdXJJtFtqLgRgw.uQXaOsgSfRoFnMAujXMtggsFYoAfSudVKdJDMMvYhSdJnrsqhWGearwmFwilWdrXkyZQfDaRknmSVAhCzSVkoKRXtj.eYaHDvvcdhgvXmhyATRNmwItlyOIdlJPyDvhjcwtConzdbvePPsCVZUhjEGbrOF.NKKFWxkVLJILkpadLCjPTdaevVhbNogDfiuDtAIIzwo.CeuUIMxghkhLOKhlMtCFUfvIqoLxQcZSXJ.aGFvQCaptSjmnzDaYfMGilimhJrFniWQQQMztTcQsQAiOeBdCSaBuQQVTtrTmzyKZQfDOPskePUmraTbArqSpekT.NZjtHEcCkkdrKhquhdboTanEAtflUuhWRdpylMfjZXvc.wxqBlA.sZemfWCocIDUGKFrZPDnsOgFTnyeahNPpggZZweg.oQoSdcDAoPi.nQuxthTCEKvinUemKUOQRBbxHwpGTERzAfHuSxFr.PhsrrUCJqLJAWFNdHrhsKkAWegzqjLGbUsfHswdVaymTjgBqUvzMUSxooRiGLZwGWfinJynRu.VgynNEj.gCbYoRguedQlbZXkZUToUnkdiKCQkNRODaTl.MsXsgLwEUheFklBfJbBceNfwHQEgIDxIivEOVwIbSfdpOJLWQPMTHMpLaelgxIOWDOVcOCZrwBoBfgarH.ZgBzKOQZcCgOFsSfPBPKquOIFiNRfDgHEOwJVJHPTbRMRbZkkxqAAzYKDv.mGTtjYBNrWFwPdCtfPEKjVRdjrZFluJNgaZbUPqbBiwbJKLX.ehCefMHMNGecKTDKIyiGgAmuNWIiOvPTsHxkIKIKVgMm.deaTiSNvQzrGlSMV.VUdyiZVkNrFMQqNMlp.pGdMVYseEChfaGuKNqSbShMVCFlgohOyGfKfMDkQHEYgRtcHypBIuQIKDdIgbgixrJlEQEMxBsxQivzjoHsmWvFYIWAp.UBLbMNXxRPEPWwHaODaOEWzKwAacyMjYoeJlMtswAWiqRNaUMzxKPrdhxxgeVbWNtOSSUPLsQBCcaVfpwYEmSzqTavPudXfO.pwbNTsiygpOpKvMydarHuQZhLvHkYEIzmbuHfnJBWywOCqt.ToZagpuwwbDzEiGACIgB.gNJUOekMcdKiVYVmbHCbcQScJRixGFFvDbWNBEpoKXfWNUqnNLkQNwRFWcsPepBPUehGiKvWSM.yulmHHmNqrxXavjeywsuwYKILBDyjdbRDBrdaEhJvZQZzmDUpDaceFsivLFA.YAlodyfgejQQMlDEdzPKoiylUsIksyfmtcDWkpzNotDArQBFFXGqmeyiOVAznUtNOH.VKCHOyOIhntTTHQVVJAynRPhfxxMW.Hn.cXlxsyWPEsAQdGROqpTuya.WVmMlQywRZVtoGSOcBUZylIBiRahmOTHMYzbiZoYNWFxWnRq.ibGsOetJZpvPLCAXdcDSeMJ.rZLjxINakPTejQRGPLyl.mCFBZxLqGBtbyQMXGnTTCzYaHWiWUalkXtPzeFHU.QwqgcXJDUaCqLXhTLYPozVEqeUIDTRGyozOjfqgoVLdI.VizkiSPFDFaYRvQq.fcLzAmUYeQdJZorkgDSimAvuBnvWRM.EqXRYaDpJYIVFOafnzooAnlmOLyvChZCMtmQxAdolujrzLaKWmNyFiEWfOuViAtHvLfCXNXmQIihOFUBNHWMdyjTVUMQBKdl.oGeGSSCaKEjDTkcwdUDUcAYxsPJpapKu.roVCvCUBJDNWZfwYW.FAAPdCkiQEZQzHXdxicIcri.WlJlfAtFEhbODhtpOqBMONmwSluhkFaegROQfdYdTMSCWzVekdZdkSGRzEJgzURkXuVWjffcMoSyeDxPCnVtLKSolTxHeeAGcrPc.SLJcdSUYJIKokJGdhrmDBqgTMqXLISdz.kRqdtGCxvGdboJVfLijKrkj.QPMervkHguzFDlyzcvyJQGjEygTrKHPZFnWRawARNLbwbYGHaOLtTEvoWtngJZSAiJqwbpxcjkRnrpQFmCvgXJhbSQoU.tcimPTDuZuwVRnlNaLdrjvNynaxrtOjPUJZVtbziqXGXHGkSgLDKVitRTkWTYNqZapXUJzIJqgIithQnTVPCVZgUFdv", "hadoop.http.max.response.header.size": "1246335045", "hadoop.http.socket.backlog.size": "13450"}	["debug_000004"]																																																																																																																																																																																																																	
		FP	3	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerUriSubstitution	java.lang.IllegalStateException	Insufficient configured threads: required=931 < max=200 for QueuedThreadPool[qtp1553033411]@5c9168c3{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@62c1259f{s=0/2,p=0}]	org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81), org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:234), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.server.Server.doStart(Server.java:401), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerUriSubstitution/campaign/failures/debug_000003	{"hadoop.http.acceptor.count": "929"}	["debug_000003"]	org.apache.hadoop.mapred.TestJobEndNotifier#testNotificationTimeout	java.lang.IllegalStateException	Insufficient configured threads: required=59665411 < max=200 for QueuedThreadPool[qtp1257841023]@4af9217f{STARTED,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@287c4e96{s=0/2,p=0}]	org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81), org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:234), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.server.Server.doStart(Server.java:401), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testNotificationTimeout/campaign/failures/debug_000003	{"hadoop.http.acceptor.count": "59665409"}	["debug_000003"]	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerRetryCount	java.lang.IllegalStateException	Insufficient configured threads: required=23868 < max=735 for QueuedThreadPool[qtp1109227776]@421d7900{STARTED,8<=8<=735,i=8,r=-1,q=0}[ReservedThreadExecutor@68feca3a{s=0/2,p=0}]	org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81), org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:234), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.server.Server.doStart(Server.java:401), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerRetryCount/campaign/failures/debug_000000	{"hadoop.http.acceptor.count": "23866"}	["debug_000000"]																																																																																																																																																																																											
	Bug-140	Repeated	1	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testBzipWithMultibyteDelimiter	java.lang.AssertionError	Unexpected number of records in split  expected:<60> but was:<61>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:114), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzipWithMultibyteDelimiter(TestLineRecordReader.java:646), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testBzipWithMultibyteDelimiter$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testBzipWithMultibyteDelimiter/campaign/failures/debug_000002	{"io.file.buffer.size": "52"}	["debug_000002"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.split.TestJobSplitWriterWithEC#testMaxBlockLocationsNewSplitsWithErasureCoding	org.junit.internal.runners.model.MultipleFailureException	There were 2 errors:  org.apache.hadoop.ipc.RpcException(RPC response exceeds maximum data length)  java.lang.NullPointerException(null)	org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	org.apache.hadoop.ipc.RpcException	RPC response exceeds maximum data length	org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1936), org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1238), org.apache.hadoop.ipc.Client$Connection.run(Client.java:1134),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriterWithEC/testMaxBlockLocationsNewSplitsWithErasureCoding/campaign/failures/debug_000000	{"ipc.maximum.response.length": "970"}	["debug_000000"]																																																																																																																																																																																																																	
		FP	1	org.apache.hadoop.mapreduce.split.TestJobSplitWriterWithEC#testMaxBlockLocationsNewSplitsWithErasureCoding	org.apache.hadoop.ipc.RemoteException	Requested replication factor of 10 is less than the required minimum of 337 for /job.split at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.verifyReplication(BlockManager.java:1611) at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setReplication(FSDirAttrOp.java:135) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplication(FSNamesystem.java:2348) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setReplication(NameNodeRpcServer.java:851) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setReplication(ClientNamenodeProtocolServerSideTranslatorPB.java:540) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy33.setReplication(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setReplication(ClientNamenodeProtocolTranslatorPB.java:432), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy34.setReplication(Unknown Source), org.apache.hadoop.hdfs.DFSClient.setReplication(DFSClient.java:1490), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:737), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:734), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.setReplication(DistributedFileSystem.java:744), org.apache.hadoop.mapreduce.split.JobSplitWriter.createFile(JobSplitWriter.java:105), org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:78), org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:72), org.apache.hadoop.mapreduce.split.TestJobSplitWriterWithEC.testMaxBlockLocationsNewSplitsWithErasureCoding(TestJobSplitWriterWithEC.java:99), org.apache.hadoop.mapreduce.split.TestJobSplitWriterWithEC.testMaxBlockLocationsNewSplitsWithErasureCoding$$CONFUZZ(TestJobSplitWriterWithEC.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriterWithEC/testMaxBlockLocationsNewSplitsWithErasureCoding/campaign/failures/debug_000001	{"dfs.namenode.replication.min": "337"}	["debug_000001"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestJobEndNotifier#testNotificationTimeout	java.lang.NullPointerException	config	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkNotNull(Preconditions.java:899), org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSink(MetricsSystemImpl.java:298), org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:277), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1279), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.AssertionError	expected:<1> but was:<0>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.mapred.TestJobEndNotifier.testNotificationTimeout(TestJobEndNotifier.java:187), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testNotificationTimeout/campaign/failures/debug_000001	{"hadoop.http.selector.count": "18336", "hadoop.http.max.threads": "20501"}	["debug_000001"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader#testProgressIsReportedIfInputASeriesOfEmptyFiles	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LineReader.<init>(LineReader.java:142), org.apache.hadoop.mapreduce.lib.input.SplitLineReader.<init>(SplitLineReader.java:37), org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.<init>(UncompressedSplitLineReader.java:46), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:123), org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper.initialize(CombineFileRecordReaderWrapper.java:69), org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader.initialize(CombineFileRecordReader.java:59), org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader.testProgressIsReportedIfInputASeriesOfEmptyFiles(TestCombineFileRecordReader.java:86), org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader.testProgressIsReportedIfInputASeriesOfEmptyFiles$$CONFUZZ(TestCombineFileRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:161), org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372), org.apache.hadoop.fs.ChecksumFileSystem.lambda$openFileWithOptions$0(ChecksumFileSystem.java:896), org.apache.hadoop.fs.ChecksumFileSystem$$Lambda$75/0x00000008401d5c40.call(Unknown Source), org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52), org.apache.hadoop.fs.ChecksumFileSystem.openFileWithOptions(ChecksumFileSystem.java:894), org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4768), org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:92), org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReaderWrapper.initialize(CombineFileRecordReaderWrapper.java:69), org.apache.hadoop.mapreduce.lib.input.CombineFileRecordReader.initialize(CombineFileRecordReader.java:59), org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader.testProgressIsReportedIfInputASeriesOfEmptyFiles(TestCombineFileRecordReader.java:86), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestCombineFileRecordReader/testProgressIsReportedIfInputASeriesOfEmptyFiles/campaign/failures/debug_000001	{"file.bytes-per-checksum": "807", "io.file.buffer.size": "2123520952", "hadoop.security.token.service.use_ip": "false"}	["debug_000001"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.split.TestJobSplitWriter#testMaxBlockLocationsOldSplits	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:728), org.apache.hadoop.mapreduce.split.JobSplitWriter.createFile(JobSplitWriter.java:102), org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:91), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsOldSplits(TestJobSplitWriter.java:74), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriter/testMaxBlockLocationsOldSplits/campaign/failures/debug_000002	{"file.stream-buffer-size": "2014900471", "fs.creation.parallel.count": "875", "fs.file.impl.disable.cache": "true"}	["debug_000002"]																																																																																																																																																																																																																	
		FP	2	org.apache.hadoop.mapreduce.split.TestJobSplitWriter#testMaxBlockLocationsOldSplits	java.io.IOException	Split metadata size exceeded 3. Aborting job job__0000	org.apache.hadoop.mapreduce.split.SplitMetaInfoReader.readSplitMetaInfo(SplitMetaInfoReader.java:53), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsOldSplits(TestJobSplitWriter.java:77), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsOldSplits$$CONFUZZ(TestJobSplitWriter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriter/testMaxBlockLocationsOldSplits/campaign/failures/debug_000003	{"mapreduce.job.split.metainfo.maxsize": "3"}	["debug_000003"]	org.apache.hadoop.mapreduce.split.TestJobSplitWriter#testMaxBlockLocationsNewSplits	java.io.IOException	Split metadata size exceeded 21. Aborting job job__0000	org.apache.hadoop.mapreduce.split.SplitMetaInfoReader.readSplitMetaInfo(SplitMetaInfoReader.java:53), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsNewSplits(TestJobSplitWriter.java:53), org.apache.hadoop.mapreduce.split.TestJobSplitWriter.testMaxBlockLocationsNewSplits$$CONFUZZ(TestJobSplitWriter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriter/testMaxBlockLocationsNewSplits/campaign/failures/debug_000001	{"mapreduce.job.split.metainfo.maxsize": "21"}	["debug_000001"]																																																																																																																																																																																																						
	Bug-165	BUG	1	org.apache.hadoop.mapreduce.security.TestTokenCache#testGetTokensForNamenodes	java.lang.NullPointerException		org.apache.hadoop.mapreduce.security.TestTokenCache.testGetTokensForNamenodes(TestTokenCache.java:202), org.apache.hadoop.mapreduce.security.TestTokenCache.testGetTokensForNamenodes$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testGetTokensForNamenodes/campaign/failures/debug_000000	{"mapreduce.framework.name": "classic"}	["debug_000000"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.security.TestTokenCache#testGetTokensForNamenodes	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:331), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:325), org.apache.hadoop.mapreduce.security.TestTokenCache.testGetTokensForNamenodes(TestTokenCache.java:205), org.apache.hadoop.mapreduce.security.TestTokenCache.testGetTokensForNamenodes$$CONFUZZ(TestTokenCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	FLAKY				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testGetTokensForNamenodes/campaign/failures/debug_000003	{"fs.defaultFS": "file:///", "mapreduce.jobtracker.address": "localhost:8012", "hadoop.security.auth_to_local.mechanism": "MIT", "hadoop.security.groups.cache.secs": "63975836", "fs.client.resolve.remote.symlinks": "false", "hadoop.service.shutdown.timeout": "588794799m", "file.stream-buffer-size": "2055066966", "hadoop.security.auth_to_local": "RULE:[1:$1] RULE:[2:$1]", "hadoop.security.groups.cache.background.reload": "false", "hadoop.user.group.static.mapping.overrides": "dr.who=;", "hadoop.kerberos.keytab.login.autorenewal.enabled": "false", "yarn.resourcemanager.ha.enabled": "false", "hadoop.security.groups.negative-cache.secs": "842", "yarn.resourcemanager.hostname": "0.0.0.0", "mapreduce.framework.name": "local", "fs.creation.parallel.count": "609600823", "file.bytes-per-checksum": "7019", "fs.automatic.close": "false", "hadoop.security.dns.log-slow-lookups.threshold.ms": "20955", "hadoop.security.authentication": "kerberos", "hadoop.security.dns.log-slow-lookups.enabled": "true", "hadoop.security.groups.cache.background.reload.threads": "11635", "fs.local.block.size": "581", "fs.file.impl.disable.cache": "true", "io.file.buffer.size": "26613", "hadoop.security.token.service.use_ip": "false", "hadoop.kerberos.min.seconds.before.relogin": "28136", "hadoop.security.group.mapping": "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback", "hadoop.security.groups.cache.warn.after.ms": "940"}	["debug_000003"]																																																																																																																																																																																																																	
		Filtered	1	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testIoSortDefaults	java.lang.AssertionError	expected:<10> but was:<4231207>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testIoSortDefaults(TestMergeManager.java:214), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testIoSortDefaults$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testIoSortDefaults/campaign/failures/debug_000000	{"mapreduce.task.io.sort.factor": "4231207"}	["debug_000000"]																																																																																																																																																																																																																	
		Filtered	1	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testIoSortDefaults	java.lang.AssertionError	expected:<100> but was:<767>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testIoSortDefaults(TestMergeManager.java:215), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testIoSortDefaults$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testIoSortDefaults/campaign/failures/debug_000001	{"mapreduce.task.io.sort.mb": "767"}	["debug_000001"]																																																																																																																																																																																																																	
		FP	1	org.apache.hadoop.mapreduce.task.reduce.TestMergeManager#testZeroShuffleMemoryLimitPercent	java.lang.IllegalArgumentException	Invalid value for mapreduce.reduce.shuffle.input.buffer.percent: -0.48889053	org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl.<init>(MergeManagerImpl.java:169), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testZeroShuffleMemoryLimitPercent(TestMergeManager.java:320), org.apache.hadoop.mapreduce.task.reduce.TestMergeManager.testZeroShuffleMemoryLimitPercent$$CONFUZZ(TestMergeManager.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestMergeManager/testZeroShuffleMemoryLimitPercent/campaign/failures/debug_000001	{"mapreduce.reduce.shuffle.input.buffer.percent": "-0.48889052867889404"}	["debug_000001"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestFileOutputCommitter#testAbortV1	java.lang.OutOfMemoryError	Java heap space		FLAKY	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1081), org.apache.hadoop.mapred.TextOutputFormat.getRecordWriter(TextOutputFormat.java:113), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortInternal(TestFileOutputCommitter.java:457), org.apache.hadoop.mapred.TestFileOutputCommitter.testAbortV1(TestFileOutputCommitter.java:477), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestFileOutputCommitter/testAbortV1/campaign/failures/debug_000008	{"fs.defaultFS": "file:///", "hadoop.security.auth_to_local.mechanism": "hadoop", "hadoop.security.groups.cache.secs": "22043", "fs.client.resolve.remote.symlinks": "true", "hadoop.service.shutdown.timeout": "46468m", "file.stream-buffer-size": "806001420", "hadoop.security.auth_to_local": "RULE:[1:$1] RULE:[2:$1]", "hadoop.security.groups.cache.background.reload": "false", "io.compression.codec.bzip2.library": "java-builtin", "hadoop.user.group.static.mapping.overrides": "dr.who=;", "mapreduce.output.textoutputformat.separator": "\t", "hadoop.kerberos.keytab.login.autorenewal.enabled": "true", "mapreduce.fileoutputcommitter.cleanup-failures.ignored": "false", "hadoop.security.groups.negative-cache.secs": "5344", "io.compression.codec.lz4.buffersize": "1366453062", "fs.creation.parallel.count": "1395495752", "file.bytes-per-checksum": "1916825071", "fs.automatic.close": "false", "zlib.compress.strategy": "DEFAULT_STRATEGY", "hadoop.security.dns.log-slow-lookups.threshold.ms": "21457", "hadoop.security.authentication": "simple", "hadoop.security.dns.log-slow-lookups.enabled": "false", "hadoop.security.groups.cache.background.reload.threads": "968", "mapreduce.output.fileoutputformat.compress": "false", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.Lz4Codec", "zlib.compress.level": "DEFAULT_COMPRESSION", "fs.local.block.size": "545", "io.compression.codec.snappy.buffersize": "1897724814", "fs.file.impl.disable.cache": "true", "io.file.buffer.size": "16830", "mapreduce.fileoutputcommitter.cleanup.skipped": "false", "io.compression.codec.lz4.use.lz4hc": "true", "hadoop.security.token.service.use_ip": "false", "hadoop.kerberos.min.seconds.before.relogin": "545715999", "hadoop.security.group.mapping": "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback", "mapreduce.job.application.attempt.id": "15142", "hadoop.security.groups.cache.warn.after.ms": "356085768"}	["debug_000008"]																																																																																																																																																																																																																	
			2	org.apache.hadoop.mapred.TestTask#testStatusUpdateExitsInNonUberMode	java.lang.NullPointerException		org.apache.hadoop.mapred.TestTask.setupTest(TestTask.java:70), org.apache.hadoop.mapred.TestTask.testStatusUpdateExitsInNonUberMode(TestTask.java:60), org.apache.hadoop.mapred.TestTask.testStatusUpdateExitsInNonUberMode$$CONFUZZ(TestTask.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestTask/testStatusUpdateExitsInNonUberMode/campaign/failures/debug_000000	{}	["debug_000000"]	org.apache.hadoop.mapred.TestTask#testStatusUpdateDoesNotExitInUberMode	java.lang.NullPointerException		org.apache.hadoop.mapred.TestTask.setupTest(TestTask.java:70), org.apache.hadoop.mapred.TestTask.testStatusUpdateDoesNotExitInUberMode(TestTask.java:53), org.apache.hadoop.mapred.TestTask.testStatusUpdateDoesNotExitInUberMode$$CONFUZZ(TestTask.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestTask/testStatusUpdateDoesNotExitInUberMode/campaign/failures/debug_000000	{}	["debug_000000"]																																																																																																																																																																																																						
		FP	4	org.apache.hadoop.mapred.TestJobConf#testNegativeValuesForMemoryParams	java.lang.AssertionError	expected:<1024> but was:<269>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.mapred.TestJobConf.testNegativeValuesForMemoryParams(TestJobConf.java:299), org.apache.hadoop.mapred.TestJobConf.testNegativeValuesForMemoryParams$$CONFUZZ(TestJobConf.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobConf/testNegativeValuesForMemoryParams/campaign/failures/debug_000000	{"mapred.task.maxvmem": "282993279"}	["debug_000000"]	org.apache.hadoop.mapred.TestJobConf#testDeprecatedPropertyNameForTaskVmem	java.lang.AssertionError	expected:<1024> but was:<0>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.mapred.TestJobConf.testDeprecatedPropertyNameForTaskVmem(TestJobConf.java:173), org.apache.hadoop.mapred.TestJobConf.testDeprecatedPropertyNameForTaskVmem$$CONFUZZ(TestJobConf.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobConf/testDeprecatedPropertyNameForTaskVmem/campaign/failures/debug_000000	{"mapred.task.maxvmem": "32896"}	["debug_000000"]	org.apache.hadoop.mapred.TestJobConf#testDeprecatedPropertyNameForTaskVmem	java.lang.AssertionError	expected:<1025> but was:<1024>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.mapred.TestJobConf.testDeprecatedPropertyNameForTaskVmem(TestJobConf.java:178), org.apache.hadoop.mapred.TestJobConf.testDeprecatedPropertyNameForTaskVmem$$CONFUZZ(TestJobConf.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobConf/testDeprecatedPropertyNameForTaskVmem/campaign/failures/debug_000001	{"mapred.task.maxvmem": "1073775872"}	["debug_000001"]	org.apache.hadoop.mapred.TestJobConf#testJobConf	java.lang.AssertionError	expected:<100000> but was:<0>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.mapred.TestJobConf.testJobConf(TestJobConf.java:146), org.apache.hadoop.mapred.TestJobConf.testJobConf$$CONFUZZ(TestJobConf.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobConf/testJobConf/campaign/failures/debug_000002	{"mapred.task.maxvmem": "767"}	["debug_000002"]																																																																																																																																																																																
		FP	1	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testRecordSpanningMultipleSplits	org.junit.ComparisonFailure	expected:<...ne, which will surel[y span multiple splits,]> but was:<...ne, which will surel[]>	org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.checkRecordSpanningMultipleSplits(TestLineRecordReader.java:201), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testRecordSpanningMultipleSplits(TestLineRecordReader.java:214), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testRecordSpanningMultipleSplits$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testRecordSpanningMultipleSplits/campaign/failures/debug_000000	{"mapreduce.input.linerecordreader.line.maxlength": "63"}	["debug_000000"]																																																																																																																																																																																																																	
		FP	2	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testRecordSpanningMultipleSplits	java.lang.AssertionError	Wrong number of records expected:<4> but was:<14>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.checkRecordSpanningMultipleSplits(TestLineRecordReader.java:197), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testRecordSpanningMultipleSplits(TestLineRecordReader.java:214), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testRecordSpanningMultipleSplits$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testRecordSpanningMultipleSplits/campaign/failures/debug_000001	{"mapreduce.input.linerecordreader.line.maxlength": "10"}	["debug_000001"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testRecordSpanningMultipleSplitsCompressed	java.lang.AssertionError	Wrong number of records expected:<4> but was:<3>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.checkRecordSpanningMultipleSplits(TestLineRecordReader.java:197), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testRecordSpanningMultipleSplitsCompressed(TestLineRecordReader.java:225), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testRecordSpanningMultipleSplitsCompressed$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testRecordSpanningMultipleSplitsCompressed/campaign/failures/debug_000000	{"mapreduce.input.linerecordreader.line.maxlength": "29377"}	["debug_000000"]																																																																																																																																																																																																						
		FP	1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV1	java.lang.AssertionError	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/_temporary/1/_temporary/attempt_200707121733_0001_m_000000_0/part-m-00000 does not exists	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:703), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:720), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000002	{"mapreduce.output.fileoutputformat.compress": "true"}	["debug_000002"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testFailAbortV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.io.compress.CompressorStream.<init>(CompressorStream.java:46), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:66), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:134), org.apache.hadoop.io.compress.DefaultCodec.createOutputStream(DefaultCodec.java:58), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortInternal(TestFileOutputCommitter.java:685), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testFailAbortV1(TestFileOutputCommitter.java:720), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testFailAbortV1/campaign/failures/debug_000007	{"mapreduce.output.fileoutputformat.compress": "true", "io.file.buffer.size": "1502584175"}	["debug_000007"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestJobConf#testJobConf	java.lang.AssertionError	mapreduce.reduce.java.opts should not be set by default expected null, but was:<>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotNull(Assert.java:756), org.junit.Assert.assertNull(Assert.java:738), org.apache.hadoop.mapred.TestJobConf.testJobConf(TestJobConf.java:158), org.apache.hadoop.mapred.TestJobConf.testJobConf$$CONFUZZ(TestJobConf.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobConf/testJobConf/campaign/failures/debug_000000	{"hadoop.security.token.service.use_ip": "true", "hadoop.security.dns.log-slow-lookups.threshold.ms": "257", "fs.client.resolve.remote.symlinks": "false", "mapreduce.job.heap.memory-mb.ratio": "7.338523864746094E-4", "fs.automatic.close": "true", "mapreduce.reduce.java.opts": ""}	["debug_000000"]																																																																																																																																																																																																																	
		Filtered	1	org.apache.hadoop.mapred.TestJobConf#testJobConf	java.lang.AssertionError	expected:<1> but was:<3136>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.mapred.TestJobConf.testJobConf(TestJobConf.java:63), org.apache.hadoop.mapred.TestJobConf.testJobConf$$CONFUZZ(TestJobConf.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobConf/testJobConf/campaign/failures/debug_000001	{"mapreduce.job.jvm.numtasks": "3136"}	["debug_000001"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestJobConf#testJobConf	java.lang.AssertionError	mapreduce.map.java.opts should not be set by default expected null, but was:<>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotNull(Assert.java:756), org.junit.Assert.assertNull(Assert.java:738), org.apache.hadoop.mapred.TestJobConf.testJobConf(TestJobConf.java:156), org.apache.hadoop.mapred.TestJobConf.testJobConf$$CONFUZZ(TestJobConf.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobConf/testJobConf/campaign/failures/debug_000003	{"hadoop.security.token.service.use_ip": "false", "hadoop.security.dns.log-slow-lookups.threshold.ms": "68", "hadoop.security.auth_to_local.mechanism": "MIT", "hadoop.security.groups.cache.secs": "134217731", "hadoop.security.groups.cache.background.reload.threads": "767", "mapreduce.job.jar.unpack.pattern": "(?:classes/|lib/).*", "fs.local.block.size": "1073741870", "fs.creation.parallel.count": "2105343", "mapreduce.map.java.opts": "", "fs.automatic.close": "false"}	["debug_000003"]																																																																																																																																																																																																																	
	Bug-140	Repeated	1	org.apache.hadoop.mapred.TestLineRecordReader#testBzipWithMultibyteDelimiter	java.lang.AssertionError	Unexpected number of records in split expected:<60> but was:<61>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.apache.hadoop.mapred.TestLineRecordReader.testSplitRecordsForFile(TestLineRecordReader.java:110), org.apache.hadoop.mapred.TestLineRecordReader.testBzipWithMultibyteDelimiter(TestLineRecordReader.java:684), org.apache.hadoop.mapred.TestLineRecordReader.testBzipWithMultibyteDelimiter$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testBzipWithMultibyteDelimiter/campaign/failures/debug_000002	{"io.file.buffer.size": "11"}	["debug_000002"]																																																																																																																																																																																																																	
			3	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testUncompressedInputContainingCRLF	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.createInputFile(TestLineRecordReader.java:312), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputContainingCRLF(TestLineRecordReader.java:400), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testUncompressedInputContainingCRLF/campaign/failures/debug_000002	{"file.stream-buffer-size": "2130640638"}	["debug_000002"]	org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader#testUncompressedInputDefaultDelimiterPosValue	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.createInputFile(TestLineRecordReader.java:312), org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader.testUncompressedInputDefaultDelimiterPosValue(TestLineRecordReader.java:601), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.input.TestLineRecordReader/testUncompressedInputDefaultDelimiterPosValue/campaign/failures/debug_000002	{"file.stream-buffer-size": "1097433075"}	["debug_000002"]	org.apache.hadoop.mapreduce.security.TestTokenCache#testBinaryCredentialsWithoutScheme	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:331), org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:325), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentials(TestTokenCache.java:99), org.apache.hadoop.mapreduce.security.TestTokenCache.testBinaryCredentialsWithoutScheme(TestTokenCache.java:68), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.security.TestTokenCache/testBinaryCredentialsWithoutScheme/campaign/failures/debug_000005	{"file.stream-buffer-size": "2130640638"}	["debug_000005"]																																																																																																																																																																																											
			1	org.apache.hadoop.mapreduce.split.TestJobSplitWriterWithEC#testMaxBlockLocationsOldSplitsWithErasureCoding	org.junit.internal.runners.model.MultipleFailureException	There were 2 errors:  java.io.IOException(The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem)  java.lang.NullPointerException(null)	org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.io.IOException	The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem	org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.split.TestJobSplitWriterWithEC.setup(TestJobSplitWriterWithEC.java:72), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.split.TestJobSplitWriterWithEC/testMaxBlockLocationsOldSplitsWithErasureCoding/campaign/failures/debug_000000	{"dfs.namenode.support.allow.format": "false"}	["debug_000000"]																																																																																																																																																																																																																	
			2	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheDisabled	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LightWeightGSet.<init>(LightWeightGSet.java:92), org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap$1.<init>(BlocksMap.java:77), org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.<init>(BlocksMap.java:77), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:496), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103),	DIFFERENT	java.lang.IllegalArgumentException	the minimum size of a bucket is 1 ms	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager.<init>(RollingWindowManager.java:229), org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.<init>(TopMetrics.java:93), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initAuditLoggers(FSNamesystem.java:1173), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:983), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheDisabled/campaign/failures/debug_000005	{"dfs.namenode.top.window.num.buckets": "5619871"}	["debug_000005"]	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheDisabled	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.util.LightWeightGSet.<init>(LightWeightGSet.java:92), org.apache.hadoop.hdfs.server.namenode.INodeMap.newInstance(INodeMap.java:40), org.apache.hadoop.hdfs.server.namenode.FSDirectory.<init>(FSDirectory.java:320), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:977), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),	DIFFERENT	java.lang.IllegalArgumentException	the minimum size of a bucket is 1 ms	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager.<init>(RollingWindowManager.java:229), org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.<init>(TopMetrics.java:93), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initAuditLoggers(FSNamesystem.java:1173), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:983), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheDisabled/campaign/failures/debug_000004	{"dfs.namenode.top.window.num.buckets": "5619871"}	["debug_000004"]																																																																																																																																																																																																						
			1	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheDisabled	org.apache.hadoop.ipc.RemoteException	Requested replication factor of 4549 exceeds maximum of 512 for /tmp/hadoop-yarn/staging/files/second-input-file at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.verifyReplication(BlockManager.java:1611) at org.apache.hadoop.hdfs.server.namenode.FSDirAttrOp.setReplication(FSDirAttrOp.java:135) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplication(FSNamesystem.java:2348) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setReplication(NameNodeRpcServer.java:851) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setReplication(ClientNamenodeProtocolServerSideTranslatorPB.java:540) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572) at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043) at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971) at java.base/java.security.AccessController.doPrivileged(Native Method) at java.base/javax.security.auth.Subject.doAs(Subject.java:423) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976)	org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129), com.sun.proxy.$Proxy30.setReplication(Unknown Source), org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setReplication(ClientNamenodeProtocolTranslatorPB.java:432), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157), org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95), org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359), com.sun.proxy.$Proxy31.setReplication(Unknown Source), org.apache.hadoop.hdfs.DFSClient.setReplication(DFSClient.java:1490), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:737), org.apache.hadoop.hdfs.DistributedFileSystem$12.doCall(DistributedFileSystem.java:734), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81), org.apache.hadoop.hdfs.DistributedFileSystem.setReplication(DistributedFileSystem.java:744), org.apache.hadoop.mapreduce.JobResourceUploader.copyRemoteFiles(JobResourceUploader.java:704), org.apache.hadoop.mapreduce.JobResourceUploader.uploadFiles(JobResourceUploader.java:258), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:205), org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:135), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:299), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheDisabled(TestJobResourceUploaderWithSharedCache.java:163), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheDisabled$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	org.apache.hadoop.HadoopIllegalArgumentException	UNIX domain socket data traffic is enabled but dfs.domain.socket.path is not set.	org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.<init>(DomainSocketFactory.java:113), org.apache.hadoop.hdfs.ClientContext.<init>(ClientContext.java:162), org.apache.hadoop.hdfs.ClientContext.get(ClientContext.java:198), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:405), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:299), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:291), org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2763), org.apache.hadoop.hdfs.MiniDFSCluster.waitActive(MiniDFSCluster.java:2820), org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1795), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:969), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheDisabled/campaign/failures/debug_000002	{"dfs.client.domain.socket.data.traffic": "true"}	["debug_000002"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheDisabled	org.mockito.exceptions.verification.NeverWantedButInvoked	sharedCacheClient.use(    <any org.apache.hadoop.yarn.api.records.ApplicationId>,    <any string>);Never wanted here:-> at org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:301)But invoked here:-> at org.apache.hadoop.mapreduce.JobResourceUploader.useSharedCache(JobResourceUploader.java:739)-> at org.apache.hadoop.mapreduce.JobResourceUploader.useSharedCache(JobResourceUploader.java:739)-> at org.apache.hadoop.mapreduce.JobResourceUploader.useSharedCache(JobResourceUploader.java:739)-> at org.apache.hadoop.mapreduce.JobResourceUploader.useSharedCache(JobResourceUploader.java:739)-> at org.apache.hadoop.mapreduce.JobResourceUploader.useSharedCache(JobResourceUploader.java:739)	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:301), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheDisabled(TestJobResourceUploaderWithSharedCache.java:163), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheDisabled$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	window size must be a multiplication of number of buckets	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager.<init>(RollingWindowManager.java:232), org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.<init>(TopMetrics.java:93), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initAuditLoggers(FSNamesystem.java:1173), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:983), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1256), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheDisabled/campaign/failures/debug_000003	{"dfs.namenode.top.window.num.buckets": "1536"}	["debug_000003"]																																																																																																																																																																																																																	
	Bug-7	Repeated	1	org.apache.hadoop.mapred.TestLineRecordReader#testUncompressedInputContainingCRLF	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052), org.apache.hadoop.mapred.TestLineRecordReader.createInputFile(TestLineRecordReader.java:363), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputContainingCRLF(TestLineRecordReader.java:467), org.apache.hadoop.mapred.TestLineRecordReader.testUncompressedInputContainingCRLF$$CONFUZZ(TestLineRecordReader.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestLineRecordReader/testUncompressedInputContainingCRLF/campaign/failures/debug_000001	{"file.bytes-per-checksum": "154525932", "file.stream-buffer-size": "1549476633"}	["debug_000001"]																																																																																																																																																																																																																	
	Bug-7	Repeated	1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryV1	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:433), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1(TestFileOutputCommitter.java:209), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryV1/campaign/failures/debug_000007	{"file.bytes-per-checksum": "2107284321", "io.file.buffer.size": "892717932"}	["debug_000007"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testMapFileOutputCommitterV1	java.lang.UnsupportedOperationException		org.apache.hadoop.io.compress.PassthroughCodec.getCompressorType(PassthroughCodec.java:124), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:150), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168), org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:1306), org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1194), org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:286), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:278), org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:131), org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getRecordWriter(MapFileOutputFormat.java:73), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:555), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterInternal(TestFileOutputCommitter.java:539), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testMapFileOutputCommitterV1(TestFileOutputCommitter.java:577), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testMapFileOutputCommitterV1/campaign/failures/debug_000007	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000007"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.OutOfMemoryError	Java heap space		DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.apache.hadoop.fs.FSOutputSummer.<init>(FSOutputSummer.java:55), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:430), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:362), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:336), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000008	{"fs.defaultFS": "file:///", "hadoop.security.auth_to_local.mechanism": "MIT", "hadoop.security.groups.cache.secs": "32059", "fs.client.resolve.remote.symlinks": "false", "hadoop.service.shutdown.timeout": "886s", "file.stream-buffer-size": "467", "hadoop.security.auth_to_local": "RULE:[1:$1] RULE:[2:$1]", "hadoop.security.groups.cache.background.reload": "false", "io.compression.codec.bzip2.library": "java-builtin", "hadoop.user.group.static.mapping.overrides": "dr.who=;", "mapreduce.output.textoutputformat.separator": "\t", "hadoop.kerberos.keytab.login.autorenewal.enabled": "false", "mapreduce.fileoutputcommitter.cleanup-failures.ignored": "false", "hadoop.security.groups.negative-cache.secs": "1430800066", "mapreduce.fileoutputcommitter.marksuccessfuljobs": "false", "io.compression.codec.lz4.buffersize": "6745", "fs.creation.parallel.count": "1892465529", "file.bytes-per-checksum": "221559085", "fs.automatic.close": "false", "zlib.compress.strategy": "DEFAULT_STRATEGY", "hadoop.security.dns.log-slow-lookups.threshold.ms": "812487938", "mapreduce.output.basename": "part", "hadoop.security.authentication": "simple", "hadoop.security.dns.log-slow-lookups.enabled": "false", "hadoop.security.groups.cache.background.reload.threads": "2020308064", "mapreduce.output.fileoutputformat.compress": "false", "mapreduce.output.fileoutputformat.compress.codec": "org.apache.hadoop.io.compress.DeflateCodec", "zlib.compress.level": "DEFAULT_COMPRESSION", "fs.local.block.size": "926", "io.compression.codec.snappy.buffersize": "816", "fs.file.impl.disable.cache": "true", "io.file.buffer.size": "28878", "mapreduce.fileoutputcommitter.cleanup.skipped": "false", "io.compression.codec.lz4.use.lz4hc": "false", "hadoop.security.token.service.use_ip": "false", "hadoop.kerberos.min.seconds.before.relogin": "1260032273", "hadoop.security.group.mapping": "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback", "mapreduce.job.application.attempt.id": "723", "hadoop.security.groups.cache.warn.after.ms": "561819990"}	["debug_000008"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:223), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:370), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:336), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:346), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:336), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000000	{"fs.file.impl.disable.cache": "true", "hadoop.security.authentication": "kerberos"}	["debug_000000"]																																																																																																																																																																																																																	
	Bug-151	Repeated	1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testCommitterWithDuplicatedCommitV1	java.lang.AssertionError	Duplicate commit success: wrong behavior for version 1.	org.junit.Assert.fail(Assert.java:89), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitInternal(TestFileOutputCommitter.java:376), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1(TestFileOutputCommitter.java:336), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testCommitterWithDuplicatedCommitV1$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testCommitterWithDuplicatedCommitV1/campaign/failures/debug_000001	{"mapreduce.fileoutputcommitter.cleanup.skipped": "true"}	["debug_000001"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.AssertionError	Could not find /home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/test-dir/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/_temporary/1/task_200707121733_0001_m_000000/part-m-00000	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.assertTrue(Assert.java:42), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.validateContent(TestFileOutputCommitter.java:228), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:171), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedOutputStream.<init>(BufferedOutputStream.java:75), org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream.<init>(BufferedIOStatisticsOutputStream.java:78), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459), org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:437), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:521), org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:126), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000000	{"file.bytes-per-checksum": "2130052711"}	["debug_000000"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter#testRecoveryUpgradeV1V2	java.lang.ClassNotFoundException	net.jpountz.lz4.LZ4Factory	java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581), java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178), java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522), org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66), org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119), org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152), org.apache.hadoop.io.compress.CompressionCodec$Util.createOutputStreamWithCodecPool(CompressionCodec.java:131), org.apache.hadoop.io.compress.Lz4Codec.createOutputStream(Lz4Codec.java:70), org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:129), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:161), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2$$CONFUZZ(TestFileOutputCommitter.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.task.JobContextImpl.<init>(JobContextImpl.java:72), org.apache.hadoop.mapreduce.Job.<init>(Job.java:152), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:195), org.apache.hadoop.mapreduce.Job.getInstance(Job.java:175), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryInternal(TestFileOutputCommitter.java:144), org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter.testRecoveryUpgradeV1V2(TestFileOutputCommitter.java:219), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.lib.output.TestFileOutputCommitter/testRecoveryUpgradeV1V2/campaign/failures/debug_000003	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true"}	["debug_000003"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache#testSharedCacheEnabledWithJobJarInSharedCache	org.mockito.exceptions.misusing.InvalidUseOfMatchersException	Misplaced or misused argument matcher detected here:-> at org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache$MyFileUploader.mockFileInSharedCache(TestJobResourceUploaderWithSharedCache.java:145)You cannot use argument matchers outside of verification or stubbing.Examples of correct usage of argument matchers:    when(mock.get(anyInt())).thenReturn(null);    doThrow(new RuntimeException()).when(mock).someVoidMethod(anyObject());    verify(mock).someMethod(contains("foo"))This message may appear after an NullPointerException if the last matcher is returning an object like any() but the stubbed method signature expect a primitive argument, in this case,use primitive alternatives.    when(mock.get(any())); // bad use, will raise NPE    when(mock.get(anyInt())); // correct usage useAlso, this error might show up because you use argument matchers with methods that cannot be mocked.Following methods *cannot* be stubbed/verified: final/private/equals()/hashCode().Mocking methods declared on non-public parent classes is not supported.	org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache$MyFileUploader.<init>(TestJobResourceUploaderWithSharedCache.java:114), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.uploadFilesToRemoteFS(TestJobResourceUploaderWithSharedCache.java:236), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabledWithJobJarInSharedCache(TestJobResourceUploaderWithSharedCache.java:191), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.testSharedCacheEnabledWithJobJarInSharedCache$$CONFUZZ(TestJobResourceUploaderWithSharedCache.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	javax.servlet.ServletException	Keytab does not exist: /home/ctestfuzz/hadoop.keytab	org.apache.hadoop.security.authentication.server.KerberosAuthenticationHandler.init(KerberosAuthenticationHandler.java:156), org.apache.hadoop.security.authentication.server.AuthenticationFilter.initializeAuthHandler(AuthenticationFilter.java:194), org.apache.hadoop.security.authentication.server.AuthenticationFilter.init(AuthenticationFilter.java:180), org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter.init(ProxyUserAuthenticationFilter.java:57), org.eclipse.jetty.servlet.FilterHolder.initialize(FilterHolder.java:140), org.eclipse.jetty.servlet.ServletHandler.lambda$initialize$0(ServletHandler.java:731), java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948), java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:734), java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:658), org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:755), org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:379), org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:910), org.eclipse.jetty.servlet.ServletContextHandler.doStart(ServletContextHandler.java:288), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117), org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:117), org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169), org.eclipse.jetty.server.Server.start(Server.java:423), org.eclipse.jetty.util.component.ContainerLifeCycle.doStart(ContainerLifeCycle.java:110), org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:97), org.eclipse.jetty.server.Server.doStart(Server.java:387), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1276), org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer.start(NameNodeHttpServer.java:170), org.apache.hadoop.hdfs.server.namenode.NameNode.startHttpServer(NameNode.java:954), org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:765), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:1020), org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:995), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1769), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1374), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1143), org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:1016), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:948), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache.setup(TestJobResourceUploaderWithSharedCache.java:90), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.TestJobResourceUploaderWithSharedCache/testSharedCacheEnabledWithJobJarInSharedCache/campaign/failures/debug_000002	{"hadoop.http.authentication.type": "kerberos"}	["debug_000002"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testCorruptedIFile	java.lang.NullPointerException		org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:106), org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:38), org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:141), org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream.wrapIfNecessary(IntermediateEncryptedStream.java:46), org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput.<init>(OnDiskMapOutput.java:87), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCorruptedIFile(TestFetcher.java:539), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCorruptedIFile$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.IllegalArgumentException	KrbException: Cannot locate default realm	java.security.jgss/javax.security.auth.kerberos.KerberosPrincipal.<init>(KerberosPrincipal.java:179), org.apache.hadoop.security.authentication.util.KerberosUtil.getDefaultRealm(KerberosUtil.java:120), org.apache.hadoop.security.HadoopKerberosName.setConfiguration(HadoopKerberosName.java:69), org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315), org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300), org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575), org.apache.hadoop.mapreduce.CryptoUtils.getEncryptionKey(CryptoUtils.java:90), org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:141), org.apache.hadoop.mapreduce.security.IntermediateEncryptedStream.wrapIfNecessary(IntermediateEncryptedStream.java:46), org.apache.hadoop.mapreduce.task.reduce.OnDiskMapOutput.<init>(OnDiskMapOutput.java:87), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCorruptedIFile(TestFetcher.java:539), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115), edu.illinois.confuzz.internal.DebugForkMain.runTestWithConfig(DebugForkMain.java:47), edu.illinois.confuzz.internal.DebugForkMain.main(DebugForkMain.java:28),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testCorruptedIFile/campaign/failures/debug_000000	{"hadoop.security.authentication": "kerberos", "fs.file.impl.disable.cache": "true", "mapreduce.job.encrypted-intermediate-data": "true"}	["debug_000000"]																																																																																																																																																																																																																	
	Bug-171	BUG	1	org.apache.hadoop.mapreduce.task.reduce.TestFetcher#testCorruptedIFile	java.lang.OutOfMemoryError	Java heap space	java.base/java.io.BufferedInputStream.<init>(BufferedInputStream.java:209), org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:56), org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:275), org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCorruptedIFile(TestFetcher.java:588), org.apache.hadoop.mapreduce.task.reduce.TestFetcher.testCorruptedIFile$$CONFUZZ(TestFetcher.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapreduce.task.reduce.TestFetcher/testCorruptedIFile/campaign/failures/debug_000001	{"io.file.buffer.size": "1847445678"}	["debug_000001"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerRetryCount	java.lang.NullPointerException	config	org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkNotNull(Preconditions.java:899), org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSink(MetricsSystemImpl.java:298), org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:277), org.apache.hadoop.http.HttpServer2.start(HttpServer2.java:1279), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:115), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	PASS				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerRetryCount/campaign/failures/debug_000005	{"hadoop.http.idle_timeout.ms": "32510", "hadoop.prometheus.endpoint.enabled": "true", "hadoop.http.authentication.simple.anonymous.allowed": "true", "hadoop.http.staticuser.user": "qzwcPxaKTtlZeQTDuCeatBOMlOvzEgxDUxDdFjYDqAYuvrErpMtSEuHoJfzVcQJKQPsNLMozxsTGR.pkTJiFzLP.fcERWDIPaDunXaRnCgtDXwFDlAiC.UcCeeMsgtTBAAaaIvDZgXEUoTfQMjWkzWQ.JluN.RMliGo.UySqvipWRFiUyOLniXzKWYEwPDifXfQyGjpTDHqcVDXxxxHYtRgexEUBC.zEtFWKXHOjcTCVQDZvanklEvobQRRE.NQoPJHxGYuXKOJjznhqRFRfYCXSevnKsCMaUSNsTIAigETdxDjI.zyjsRIPjqPBQZsyaeiRXXWRUAUfPVCEJVwnAQERSxvGFYkdnjRqiUBzAANZwnfcCHlBxHfJddNiftXrZyuB.ughaylSMQygdgbTXXEK.CDDuUWHmaNsNEEDcyMAwNXJWlQjROkgewvLnXJevBiRDThQzKxGrkFuweiHXDiRReog.sQTQhmqTNHEvYXAOvkgxbQlRvgmjRTsayyzZnAhxxNEUNLOwKiLPfWgUIoOMpyFHOLFaDrg.bTMCBWIRDxaLjVjgFifgOzdiaxa.xNuBoDbJkhiqcJWdbHkASntbQoSmAuVLUZWinzttiPcKMcyWnKKosvBpazWLcinghQSeMncpWcvCXAyQXVtJpCChRTjWYLAYMQV.YtJPZoVlaJMbeAISfLRlBMreOiDExbzaxzDZbH.kUVtFhjlvazGHyLdIBKIRgfwNtZvNrhFuVWAFEqPVhxUdLCFrdVotMCEywlkFPHTXqIWTTKKLUCTlGrmQzYvpeTK.MJHGlEOkOouytNmibOckhBeJmTodCHElnFjoWssAhyCYElvdORLmxXMdeFztcQtNQXmrePm.fKKfudxouKDLlwUdcPqZTfJJwREzMJFmJDbsgdsQautX.NiMOtaUctEKCfaAxCLIuSwbFasxTAEmtzqvfqSQbxaKqWTuBfmmaDSYDmBRjdJPUnEAcbHfUXAlsCsUQiUzd.ZfcjbKgdvkcksOeXEDjHEBMrBMeJTIEdkYdnmEVbvvklSKSLSWfhPJHQBQGPhYmijDlwyubIQG.QKXYrIxJSZWsqJSuYcTrBMcGVLMNwXzfNwuGDERpgCughoUAgiOMSseqaqnnWXTyWEGuiuMLdmJmAa.oolEYhhriYxKFjZOykCvjJbfSommJcpvgSYuHdnBWBVuATBMaIumRXetrdcxcYqwugGb.LhWabXCyRihsVUSRBIg.MVmctoefNljnSgUBlVymiQdqXJkeXRTvmmvDvsJLxAPETfEhbkfxCYgwWvPYwQZypJxMqFQmQnWduziYJRoRUiZwGDfXumw.rAW.BVpjvdZcOUeazeNBplygMeJguSyAPVRdcEOQEUPHgcAFxlzJJmRlymsuXEBoFGzxFPFrxHYlcufDIISifcsbu.phXZHSdBzPaVksheZXpExBzlpjwQdyvpTWbxJGjQZEYjoRMJFrkqJPWmcIApJBhfmAqrvTkVCzWhlUWwJndFWhtBL.uQAiIgzjBYRuHOeAtubyTTAjwNFSohHrTgCXBm.rsYPnrCTrluSzs.mxAJbYfNcBOGXVGbNNWlslSMfBlOqnTRgbBZyhfnJbohLLdhGYgWGmDGmpqKDZCkZAJHjZjKuCNKpvdOEgOpUAznLDgdNjYTqQi.OymlopJcXzytAizulsQSqDSoXvReiAQmbmuSSEfwMIAdagYSmaQFcydOneFCFHARyuzIMvkiL.PQAQwZIBpHoHFILjxxQNigeuvTrXrZFMclYeZyhHtIDaBtwMbcbPmgZqDHKrSNKrRwGJxJSKfEQseSCDNTMArlyiMcCzyX"}	["debug_000005"]																																																																																																																																																																																																																	
			1	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerRetryCount	org.junit.internal.runners.model.MultipleFailureException	There were 2 errors:  java.lang.OutOfMemoryError(Java heap space)  java.lang.NullPointerException(null)	org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	DIFFERENT	java.lang.OutOfMemoryError	Java heap space	org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534), org.apache.hadoop.http.HttpServer2$Builder.build(HttpServer2.java:511), org.apache.hadoop.mapred.TestJobEndNotifier.setUp(TestJobEndNotifier.java:111), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), org.junit.runner.JUnitCore.run(JUnitCore.java:115),	/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerRetryCount/campaign/failures/debug_000002	{"hadoop.http.selector.count": "792156289"}	["debug_000002"]																																																																																																																																																																																																																	
		FP	1	org.apache.hadoop.mapred.TestJobEndNotifier#testLocalJobRunnerRetryCount	java.lang.AssertionError	expected:<4> but was:<0>	org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:647), org.junit.Assert.assertEquals(Assert.java:633), org.apache.hadoop.mapred.TestJobEndNotifier.testLocalJobRunnerRetryCount(TestJobEndNotifier.java:164), org.apache.hadoop.mapred.TestJobEndNotifier.testLocalJobRunnerRetryCount$$CONFUZZ(TestJobEndNotifier.java), java.base/java.lang.reflect.Method.invoke(Method.java:566), org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59), org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12), org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59), org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.illinois.confuzz.internal.ConfuzzGuidance.run(ConfuzzGuidance.java:100), edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144), org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100), org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103), org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63), org.junit.runners.ParentRunner$4.run(ParentRunner.java:331), org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79), org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329), org.junit.runners.ParentRunner.access$100(ParentRunner.java:66), org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), org.junit.runners.ParentRunner.run(ParentRunner.java:413), org.junit.runner.JUnitCore.run(JUnitCore.java:137), edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208), edu.illinois.confuzz.internal.FuzzForkMain.main(FuzzForkMain.java:39),	REPRODUCIBLE				/home/ctestfuzz/fuzz-hadoop/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/meringue/org.apache.hadoop.mapred.TestJobEndNotifier/testLocalJobRunnerRetryCount/campaign/failures/debug_000003	{"hadoop.http.max.request.header.size": "15"}	["debug_000003"]																																																																																																																																																																																																																	