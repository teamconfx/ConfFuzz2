config;exception;times;lines
[dfs.datanode.volumes.replica-add.threadpool.size];org.junit.internal.runners.model.MultipleFailureException;21;['org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testDecommissionWithBusyNode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=876419938318456348888260589455020259252175100];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1594546343 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testDecommissionWithMissingBlock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1488923255 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testDecommissionWithBusyNode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=9080675287];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 715509208 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testFileMultipleBlockGroups;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=703523782539891329485075513388801509016123111010404857796201483245351];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1212289700 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testFileSmallerThanOneStripe;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=493703380219733818279];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1309651689 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testFileSmallerThanOneCell;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=5746922925970696010362189587381915468236650294293340];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1651986398 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testRecoveryWithDecommission;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=5834233];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1409392966 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testDecommissionTwoNodes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 624623518 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testFileFullBlockGroup;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1412630695 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testDecommissionWithMissingBlock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 289145712 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testCountNodes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=87909081882247777357116909617457782245696456583931566380326260263872786094837632161347];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 2146079944 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testFileSmallerThanOneCell;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=7278574576781431320695288135531062021841100220655129];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 40958468 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStripedBackoffMonitor#testDecommissionWithFailedReplicating;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1118878660 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testFileSmallerThanOneStripe;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3441175486772670138277736308728013];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1444564922 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testRecoveryWithDecommission;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1685680404 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testDecommissionWithFailedReplicating;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=63389806595487670207468132882626787597754583134333776181628672637];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 177407414 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeWithHdfsScheme#testInvalidOverloadSchemeTargetFS;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=9776824019523850];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(Could not initialize target File System for URI : hdfs://localhost:39295/),  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1887234656 as octal or symbolic umask.);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testDecommissionTwoNodes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 1772420073 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testCountNodes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=23364402240537620171879197291516012736380724631975105442];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 770598702 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithStriped#testFileMultipleBlockGroups;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.IllegalArgumentException(Unable to parse configuration fs.permissions.umask-mode with value 456880298 as octal or symbolic umask.),  java.lang.AssertionError(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCacheDirectives#testExpiryTimeConsistency;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=5771150638584388128981583700];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.io.IOException(The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem),  java.lang.NullPointerException(null);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n']
[dfs.datanode.volumes.replica-add.threadpool.size];java.io.IOException;311;['org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testGetMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testInternalDeleteExisting2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=60119197176571];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testGetDelegationTokensWithCredentials;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testInternalGetAllStoragePolicies;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSImage#testSupportBlockGroup;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testgetFSonDanglingLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testGetContentSummary;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsLinkFallback#testMkdirShouldFailWhenFallbackFSNotAvailable;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testGetDelegationTokens;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2341347869492595675550601631959017910341594510322298739661211813860559861];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testGetDelegationTokensWithCredentials;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testConfLinkFallback;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=782873838751823265230385453061789895442061445353399933];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testFileCreationWithOverwrite;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsLinkFallback#testMkdirOfNewDirWithOutMatchingToMountOrFallbackDirTree;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testCheckOwnerWithFileStatus;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=8699039499929959863081179397921562555183];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testInternalGetAllStoragePolicies;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkRegex#testCheckOwnerWithFileStatus;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=342417269044412328305146429095];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testResolvePathThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testTargetFileSystemLazyInitializationForChecksumMethods;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testCloseChildrenFileSystem;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=86852926064938133599019312174946442439272442378890696326195488041778798311282];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testGetBlockLocations;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=119435105783212945753891481344611119769016132259863];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testRenameAcrossMounts2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testListingWithFallbackLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testChildFileSystems;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testResolvePathDanglingLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testConfLinkMergeSlashWithRegularLinks;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testRenameAcrossMounts1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=7360469904452388774530689229229628510591700138277241349];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestDeadDatanode#testDeadDatanode;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testListLocatedStatus;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testInternalRename2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesWithNNRestart;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=68381827198433907996172645021403107];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.diskbalancer.command.TestDiskBalancerCommand#testRunMultipleCommandsUnderOneSetup;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.diskbalancer.command.TestDiskBalancerCommand#testDiskBalancerForceExecute;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=9024065845875967699128230100450596300761149899464177];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testEncryptionZonesWithSnapshots;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testConcat;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testListCacheDirectives;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testSatisfyFileWithHdfsAdmin;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithReconstructFail;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=3316056537518012166];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestLeaseRecoveryStriped#testLeaseRecovery;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication#testFencingStress;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testBlockTokenExpired;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=65713892344492324356060918092699217743276463910672704207800846582564];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes#testCircularLinkedListWrites;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=69810007409761665209725853105453100];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestMiniDFSCluster#testClusterNoStorageTypeSetForDatanodes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFSForHA#testMultipleNamespacesConfigured;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2388182392968008501574270631607163306578577623];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testgetFSonDanglingLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=5216088475466];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testAppendWhileInSafeMode;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testCorrectNumberOfBlocksAfterRestart;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testSnapshotDeleteWithConcat;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testNeededReconstructionWhileAppending;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 74518151 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRetryCacheOnStandbyNN;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=802888009479698006207401819774604558478533889310739];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testFileCreationError1;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=537123983625996073634348099159555157720685693401673937228927606033928476];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics#testHAInodeCount;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestPendingCorruptDnMessages#testChangedStorageId;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=33377189946449420002497];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestQuotasWithHA#testGetQuotaUsageOnStandby;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCreateEditsLog#testCanLoadCreatedEditsLog;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=201941288650618908039475704958309692308380417063036479350852250841902074278012651743137];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestCreateEditsLog.testCanLoadCreatedEditsLog(TestCreateEditsLog.java:77)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testGetTrashRoots;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=54947909077094347115684898175297541076492433004672868306448141];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testProvisionTrash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=333796990194541808689144590209233872115347479037798485687375627036];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions#testLeasesRenewedOnTransition;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testTransitionToActiveWhenSafeMode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestPendingDataNodeMessages#testPendingDataNodeMessagesWithEC;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.tools.TestECAdmin#testXOR21MinRacks;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4328929243904211948382816558110701351200349];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot#testSnapshotOpsOnRootReservedPath;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot#testSnapshotOpsOnReservedPath;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestPread#testHedgedPreadDFSBasic;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestAddBlockRetry#testAddBlockRetryShouldReturnBlockWithLocations;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSImageWithSnapshot#testSnapshotOnRoot;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestSequentialBlockId#testTriggerBlockIdCollision;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=965903441884290224483183029087138324368177879756241439831226175570095279914223793215496414473905751];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapAliasmap#testAliasmapBootstrap;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=265456203514352307505551443616350318705620348735352061197750503542143963024223954491232209];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestHFlush#testHFlushInterrupted;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testConcurrentFileCreation;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestINodeFile#testReservedFileNames;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testStandbyExceptionThrownDuringCheckpoint;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testViewFileSystemInnerCache;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=484238484302457848844097296421473207024691335961207461435610935297180917260266754421989219969212851];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testCreateFileSameAsInternalDirPath;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testGetMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=5151529327085831252439478362107343658189947957477697513572299949406053373];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testMkdirsOfRootWithFallbackLinkAndMountWithSameDirTree;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testLinkTarget;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testTargetFileSystemLazyInitialization;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1704792322558078112484961386845619673923159263213623357974641];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testUsed;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkRegex#testGetMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3884094753596988069722164469139711280824466753124031549024075242806556642218320];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testCreateFileOnInternalMountDirWithSameDirTreeExistInFallback;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testRenameAcrossMounts4;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testResolvePathMissingThroughMountPoints2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testCreateFileOnRootWithFallbackWithFileAlreadyExist;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=367240736261162157679307720862086880753051860014767493];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testCreateNonRecursive;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=859423881829748301467986005841997698726327948951280814882168288];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testInternalRename2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=389558547740911728574314210629618539462912967733299553327583983357056480308053929997962659702450768];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testInternalDeleteExisting2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testgetFSonDanglingLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=9387924301100];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testResolvePathMissingThroughMountPoints2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testCloseChildrenFileSystem;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=58923293003519635772929720525959725147227467444724672];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkRegex#testRenameAcrossMounts3;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=24216918205929666714148408905674599940063853969573232721083541];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testResolvePathThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4405786672553146325615280995882062381766532337760412783089270];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testInternalRename2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=18919790567004995807263228535780018680951827503];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testGetDelegationTokens;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testResolvePathMissingThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testInternalDirectoryPermissions;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testRenameAccorssFilesystem;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=80655325125047367918457969918217958326591725155601847600347408372261191766623175662];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testFileStatusOnMountLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=98152936117727082469902603863005562161949591094826065405557552591812215999077731830728953861172606950];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.TestErasureCodingPoliciesWithRandomECPolicy#testBasicSetECPolicy;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamUpdatePipeline#testDFSStripedOutputStreamUpdatePipeline;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement#testBlockMoveAcrossStorageInSameNode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes#testFullBlockReportAfterRemovingVolumes;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=95871089];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testSetXAttr;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics#testHAMetrics;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=62279249127550196991297039226897606763207112403809972111750];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDnRespectsBlockReportSplitThreshold#testCornerCaseUnderThreshold;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=66018018516618790097592441092092361129193128622];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestErasureCodingMultipleRacks#testSkewedRack2;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testCheckpointSucceedsWithLegacyOIVException;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testBlocksAddedWhileInSafeMode;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=6836069097419402194];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRemoveXAttr;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRename;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6717609662405826383524989008807492];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testUpdatePipeline;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=614812000116246977042290149];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testDelete;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testAppend;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=9533384184356772348072145186128568215724242584755246265657082193820969373764581162885];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRename2;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestFileAppend2#testSimpleAppend2;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=53087819084684502];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystemMBean#testWithFSNamesystemWriteLock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=261474953699636905420503156977120122078372361780878585909019293291657499324288041426243660431];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testTrashStickyBit;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestViewDistributedFileSystem#testStorageFavouredNodes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testWebHdfsPathWithSemicolon;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testDeleteSnapshotWithPermissionsDisabled;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=555836407716691499];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testCreateSnapshot;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=199612488132640018291451051574947768374864182374690263409736967662929459997799];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testInternalDeleteExisting2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=59082975584618245919352404171621791551359417114381927968136144377058561336571539074560352271828];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSImage#testLoadMtimeAtime;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testComplexFailoverIntoSafemode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testWriteOverGracefulFailoverWithDnFail;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics#testTimeoutMetric;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=157274980];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testInternalRenameToSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3924774723338641952715533561237046940880954389149565275762587654710187305106614656338668550738726839];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testResolvePathMissingThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testFileStatusOnMountLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade#testDatanodeRollingUpgradeWithRollback;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testMultipleLevelDirectoryForSatisfyStoragePolicy;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testGetMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=70769373473258761700522112755470266148278279372689206323794346743];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testGetDelegationTokensWithCredentials;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1075395380263570197619893775561717192652684465720657836164];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testGetContentSummary;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=464582603371719293733600053362164043390959574731542813370272013977];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testConfLinkMergeSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testFileStatusOnMountLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=176966532397981190170368759729556229214097994733879563938];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyBlockManagement#testInvalidateBlock;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testEnterSafeModeInANNShouldNotThrowNPE;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=24576594382038410158549977985884437209604899537134333250289531769255544919920702985821487284622018];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testEnterSafeModeInSBNShouldNotThrowNPE;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=49930454290545650622076526327134405364862251088492130111642452670021880702154505392870769142];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testCreate;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestFailoverWithBlockTokensEnabled#testFailoverAfterRegistration;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=454438756860036533100459081602891279555674576816730309583603402537];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestReconstructStripedBlocks#testMissingStripedBlock;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testDeleteOnExit;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=60823420226950160737888];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testCheckOwnerWithFileStatus;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testTargetFileSystemLazyInitializationForChecksumMethods;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testInternalRenameToSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testRenameAcrossMounts2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=583842774790847122063070873030872109974458846419610306128332832995899723102008545147094313016];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.TestMiniDFSCluster#testClusterSetDatanodeDifferentStorageType;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=62452237109318938908862525317031903779912703127320992911661433415343128348790215126950790974568842444];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.shell.TestHdfsTextCommand#testDisplayForAvroFiles;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.tools.TestDebugAdmin#testComputeMetaCommand;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDataTransferProtocol#testDataTransferProtocol;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestViewDistributedFileSystem#testStatistics2;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=47727222717993110281524];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestAddBlockRetry#testRetryAddBlockWhileInChooseTarget;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=337959498783410901883026013378839996249726158877178];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testGetHomeDirectory;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=13117909862523084725005236258445246711227575896403040798949];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDistributedFileSystem#testStatistics2;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testViewFileSystemInnerCache;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=9134879572813093112018961699243711244375808954088821231364233911132419920607395388198418];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testGetContentSummaryWithFileInLocalFS;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=8684750634445960537134241131091774100148312464539834799103];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testInternalDeleteExisting2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testRenameAcrossMounts4;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testInternalCreate2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testInternalRenameToSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testViewFileSystemUtil;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=957054748064484476064968];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testUsed;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=42290975898580006432941237043347622550676564068589648669129876635969642441630707690086827522083713];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.TestDisableConnCache#testDisableCache;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.tools.TestJMXGet#testDataNode;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestRollingUpgrade#testDFSAdminDatanodeUpgradeControlCommands;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=6667373668024367089566935938873854686010997679942];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testListEncryptionZonesWithSnapshots;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestConnCache#testReadFromOneDN;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSRename#testRename2Options;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions#testTransitionToCurrentStateIsANop;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3738842573306578647566645095832198916746602656019418858904502853579814973848071680511633276626];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSClientRetries#testDFSClientConfigurationLocateFollowingBlock;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=4324349560869066113262677559486509527137416400510722298023191370931038242868616552222638258333];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testCheckpointCancellation;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4322729384360857204855930812976208063];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testDeleteOnExit;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testCheckOwnerWithFileStatus;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=915419861724617278200375226763561778373971418613544];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testConfLinkSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=24911541849507366052626407723106831553817538942786];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testInternalGetAllStoragePolicies;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=970533203128922772791038844810254];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testViewFileSystemUtil;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=479029267114054104138107580460589702791017648715576444793026014289907146801];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsLinkFallback#testMkdirOfRootWithFallbackLinkAndMountWithSameDirTree;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsLinkFallback#testMkdirWithFallbackLinkWithMountPathMatchingDirExist;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testInternalCreate2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=14787795801388843762776543360749128520815126905634883726022];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testInternalCreate1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestQuotaByStorageType#testQuotaByStorageTypePersistenceInEditLog;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=245421514628843721489301985505300144316598708031761018583980651545345953522868009630300231050813];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestQuotaByStorageType#testQuotaByStorageTypePersistenceInFsImage;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestUnsetAndChangeDirectoryEcPolicy#testUnsetEcPolicyInEditLog;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=7463429960203498789866384897946266056160507121659579398915040153160852];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations#testMiniDFSClusterWithMultipleNN;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testLinkTarget;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testCloseChildrenFileSystem;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2011423520657680427058892019681];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testInternalCreateMissingDir2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testResolvePathDanglingLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testRenameAcrossMounts2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=508486949720579794182780432257072367780478484933205792747103];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testOpenFileWhenNNAndClientCrashAfterAddBlock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testListCachePools;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=75143846642247922269183111971431468562994912];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testRenameAcrossMounts3;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=35958032057739739688745651151361492875903510490147619035236267868461838742740600935191262166988294];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testCreateEZWithNoProvider;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testBlocksRemovedBeforeStandbyRestart;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testFsImageCorruption;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=26644796922699626139808422158689938484970167];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testGetContentSummary;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=58218195172039409236955703524796839375213440184189199211583742];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testListStatusIterator;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure#testAddBlockWhenNoSufficientParityNumOfNodes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure#testIdempotentCloseWithFailedStreams;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1149648147487252260753];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsck#testFsckUpgradeDomain;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestLargeBlock#testLargeBlockSize;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=63619034278795];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testReadsAllowedDuringCheckpoint;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=965708031728949278183036421531974003426859362607155100385009];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testCreateFileWhereThePathIsSameAsItsMountLinkPath;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=698961614807262906844082737194417083970891805677607763];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testCreateFileOnRootWithFallbackEnabled;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testPipelineRecoveryStress;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=1314599460210376222120401830916587226295965];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testLinkTarget;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1678435722358397383];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithRename;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testParentDirWithUCFileDeleteWithSnapShot;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=797678740610794955];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSClientExcludedNodes#testExcludedNodesForgiveness;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testDeleteSnapshot1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=488629158852483032054050867440101826418613359907319096359677108457580065259];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testDeleteSnapshot2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testSnapshotDiffReportWithConcat;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=1743055961398429761751844538346853589124394374518233221065109261179326752920008615895924418227];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS#testRead;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2131570437352624603227];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testInternalCreateMissingDir;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=952050911966531809263612904631242966012750389711367483160893273];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testResolvePathMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=75077049384083755134541144192024073649671513292241484394030632708];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testResolvePathMissingThroughMountPoints2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testInternalCreateMissingDir3;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testGetBlockLocations;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=74198902684396033826418411653737001823];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testRenameAcrossMounts1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testInternalGetAllStoragePolicies;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2473900827269970761683807914949047862479835091772304330335656804];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testCreateNonRecursive;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing#testNNClearsCommandsOnFailoverAfterStartup;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=68912779666723808584380519392010623631466216926223040167284267380358425451652718034968595];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testResolvePathMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=193920395930712629688383599344670396972];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testGetBlockLocations;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testListStatusIterator;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNamenodeRetryCache#testRetryCacheRebuild;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6566366925161075903108528992982263305877278548646114182162728546143142672104785156241];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testWhenOnlyFewTargetDatanodeAreAvailableToSatisfyStoragePolicy;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=65169374717913980846985878273466674602144868];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestLeaseRecovery#testLeaseRecoveryAndAppendWithViewDFS;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.mover.TestMover#testMoveWhenStoragePolicySatisfierIsRunning;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=66392995235775550422386532713319129385220254438016604922584107293118512481797145496];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testSaveNamespaceWithRenamedLease;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations#testFedSingleNN;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2385885674237370870284889996049498335416035186812051517538394609582976341];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.tools.TestECAdmin#testRS63MinDN;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot#testDatanodeRestarts;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=80879277064754304043127392137680008448786333594980444700497276001900];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testCheckpointBeforeNameNodeInitializationIsComplete;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testStandbyAndObserverState;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=96267256520691700785805958439847063033688410738062028224985895764030];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testSafeModeExitAfterTransition;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=27952261870593357819173573991740665285556643038368861994609497532];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testWebHdfsBackwardCompatibleSpecialCharacterFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRenameSnapshot;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation#testRWRShouldNotAddedOnDNRestart;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.mover.TestMover#testBalancerMaxIterationTimeNotAffectMover;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=22901545274951237632];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList#testInstanceOfAddReplicaThreadPool;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistLockedMemory#testReleaseOnFileDeletion;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=4267348281225281797140441064365781074911];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot#testSnapshotMtime;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testRenameAcrossMounts3;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testInternalRename2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testListingWithFallbackLinkWithSameMountDirectories;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testConfLinkFallbackWithMountPoint;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=86116513633408773131928361061469420891714111677688355840422747359712070357635345191460682];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testRenameAcrossMounts3;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=25585481854467880107020671903555309593762780431];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testResolvePathMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=8758535754210850771989192693728445416545368637693380429512223914811958775661838589436453776423];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testGetDelegationTokens;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testGetBlockLocations;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3266807938427284474798293216732865962980099813083885887586266];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testRenameAcrossMounts2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4548037658524484100];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkRegex#testRenameAcrossMounts4;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=69332078391418041185024134679063317221282616472366856];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkRegex#testViewFileSystemUtil;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=658105];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testResolvePathMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testChildrenFileSystemLeak;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testCreateNewFileWithOutMatchingToMountDirOrFallbackDirPath;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testConfLinkFallbackWithRegularLinks;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testRenameAcrossMounts1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testMkdirsWithFallbackLinkWithMountPathMatchingDirExist;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=49987538505456382657898886371920162537969391776167174399795608049430825122368007074246921];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testGetContentSummaryWithFileInLocalFS;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=378136150298406774364753129724087028140379411042665220551423617939044196845467045691200611676];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testChildrenFileSystemLeak;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testResolvePathMissingThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkMergeSlash#testCreateNonRecursive;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6622567856112737266112004380409];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testBasicOperationsRootDir;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestReplication#testBadBlockReportOnTransferMissingBlockFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=696190281660965573666521897034537730501443811164247324658094255];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure#testAddBlockWhenNoSufficientDataBlockNumOfNodes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=24055262625160355122366044704179623365624598022770240];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestFileLengthOnClusterRestart#testFileLengthWithHSyncAndClusterRestartWithOutDNsRegister;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=584060019468250617233048843884934062976];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean#testTotalBlocksMetrics;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=9686473644047203025586369424279509530366];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testInternalCreateMissingDir2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=7427587188382456457393997197561102083722897808474069298652341711925832635518966478224417];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testInternalCreateMissingDir;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testgetFSonDanglingLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=815185912902818284100563641570755182065];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testConfLinkSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testResolvePathMissingThroughMountPoints2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testInternalCreate2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=957812843809927402491256723463];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testRenameAcrossMounts1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=317725428322988687376324201612426367];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testUsed;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testNamenodeRestart;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=8722655237246664841529611526744687174556973172957710644817151785044519415991861120909158257837];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testWithCheckpoint;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=9414329582261687846044153002052995499393846741681190019321900925075134788652648960542377916580650124];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testUCFileDeleteWithSnapShot;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testFilesDeletionWithCheckpoint;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.diskbalancer.command.TestDiskBalancerCommand#testSubmitPlanInNonRegularStatus;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testBothNodesInStandbyState;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=139233075575969254956618819048908360368808030571094728194190190636974744873108];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testWebHdfsAppend;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks#testWithAllCorruptReplicas;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=2256680360632955496790331305069526497522];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testUpgradeAndRestart;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=541472199699524460937143];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testWhenStoragePolicySetToCOLD;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=63416601675168323910277409326229727359215650553431271697473597873975108622718215601445500016452];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testResolvePathThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=800019810989178551658150900794862299235194964357982632911611658735535710737918007170680503966];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testGetDelegationTokens;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=77747573457977989366428879860516830648450];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testResolvePathMissingThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=9730419143571071444676562354937903184988173456503656596421049181298329004817044759];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testFileStatusOnMountLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=29792787782037931810172879224330857850942936730405985];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testListLocatedStatus;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFs$1.<init>(ViewFs.java:239), org.apache.hadoop.fs.viewfs.ViewFs.<init>(ViewFs.java:238), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method), jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestBatchIbr#testIbr;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=862108998034295138055319655204955468662495556293158009230822146845];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemHdfs#testCloseChildrenFileSystem;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testTruncateFailure;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testRootDirEZTrash;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions#testTransitionActiveToStandby;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool#testDfsAdminDeleteBlockPool;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=247157896941383456458007364454218839210020723939547];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testModifyCacheDirectiveInfo;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testAddCacheDirectiveInfo;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRemoveCacheDescriptor;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testAddCachePool;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics#testReceivePacketMetrics;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=9026221314822875592766662746498360662601304517195129882214690035196312748321073570];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics#testDataNodeTimeSpend;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=113139239338352366289889995923088593157];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks#testWithReplicationFactorAsOne;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=652783062993047639990503];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemAtHdfsRoot#testGetDelegationTokensWithCredentials;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=768990285475295862707668401182084856270013556184004077080471877126894086];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testSafeBlockTracking;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2392401141247286382165838085352712752185082616980427493759619641458357693262229001];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testBlocksRemovedWhileInSafeMode;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=948301220];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestMultipleNNPortQOP#testAuxiliaryPortSendingQOP;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=650228367052656736245731401460889746345893156821796349150305154870011093006098269];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens#testSaveNamespace;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n']
[dfs.datanode.volumes.replica-add.threadpool.size];java.lang.OutOfMemoryError;10;['org.apache.hadoop.hdfs.server.namenode.TestReencryptionWithKMS#testReencryptionKMSACLs;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1217336859240179845214345270584271137384676846357622];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testNeededReconstructionWhileAppending;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=838931186719932046914838018598856682964895124714233363231821157978017];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestReencryptionWithKMS#testReencryptCommandsQueuedOrdering;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestReencryptionWithKMS#testReencryptCommandsQueuedOrdering;id_000002;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS#testBasicOperations;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS#testBasicOperations;id_000003;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.TestAclsEndToEnd#testCreateKey;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=469220536006465272152198582175330803221336350709950907231771543715818671203921384064223424];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.TestAclsEndToEnd#testCreateKey;id_000002;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.TestAclsEndToEnd#testDeleteKey;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1597012502913693013340924602695593061435413772361321710052894];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.TestAclsEndToEnd#testDeleteKey;id_000002;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.OutOfMemoryError;Java heap space;[]\n']
[dfs.datanode.volumes.replica-add.threadpool.size];java.lang.IllegalStateException;2;['org.apache.hadoop.hdfs.server.namenode.TestReencryptionWithKMS#testReencryptionKMSACLs;id_000002;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalStateException;Insufficient configured threads: required=321866426 < max=135428468 for QueuedThreadPool[qtp760865118]@2d59e15e{STARTED,8<=8<=135428468,i=8,r=-1,q=0}[ReservedThreadExecutor@61115af8{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]\n', 'org.apache.hadoop.hdfs.TestAclsEndToEnd#testDeleteKey;id_000003;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalStateException;Insufficient configured threads: required=125911424 < max=88328984 for QueuedThreadPool[qtp1685965918]@647dcc5e{STARTED,8<=8<=88328984,i=8,r=-1,q=0}[ReservedThreadExecutor@53dfbc8{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]\n']
[dfs.datanode.volumes.replica-add.threadpool.size];java.lang.IllegalArgumentException;261;['org.apache.hadoop.hdfs.server.namenode.TestFSImage#testSupportBlockGroup;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=66886373904530582275251655616706441004273009];java.lang.IllegalArgumentException;1541433965;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testDecommissionFederation;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=8118178221026323531108614390132650933760671036517959983708430753776716531583314];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1615108291 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.TestDecommission#testDecommissionFederation;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 117176010 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testDecommission;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1656870406 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestDeadDatanode#testDeadDatanode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=7943072158083582442741345737400585026363409872304785898888176910342820983668186848694918337];java.lang.IllegalArgumentException;841830193;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalRename2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=30526625285998363831650260362451666640699256795609454056817529315674597];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 395311102 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalCreateMissingDir;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1978476087 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testOperationsThroughMountLinks;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=8833304361309406086278861143168074807378682669];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 2136582545 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFileNonRecursive(FileContextTestHelper.java:140)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testResolvePathMissingThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1902966935 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalCreate1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=635315181698000192013];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 70732782 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFileNonRecursive(FileContextTestHelper.java:140)]\n', 'org.apache.hadoop.hdfs.TestDFSOutputStream#testEndLeaseCall;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=32204878334];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1831157118 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:299)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalRename2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6906605418352417046776120890649332944485635693292791733926458114503504064276762646315574884255217675];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 76999051 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesWithNNRestart;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1757729253;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.diskbalancer.command.TestDiskBalancerCommand#testRunMultipleCommandsUnderOneSetup;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;784360257;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.diskbalancer.command.TestDiskBalancerCommand#testDiskBalancerForceExecute;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1956196325;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=88716432429935240871650297056277024228006483944298067781148944723371552977059566];java.lang.IllegalArgumentException;1303598819;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testEncryptionZonesWithSnapshots;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=9637085129603858725994587331706157167905839395033877954175];java.lang.IllegalArgumentException;2074452742;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testConcat;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1414333520;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testListCacheDirectives;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=2074382994278255774480366361199044274719437636317273960620880606881836099478693];java.lang.IllegalArgumentException;664023188;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestMaintenanceState#testInvalidation;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=22706237920488895936237520445600913029813946062766388190522893330820851];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1773334777 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testSatisfyFileWithHdfsAdmin;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;609070491;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithReconstructFail;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=33888824691760958248893556316];java.lang.IllegalArgumentException;1600283732;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestLeaseRecoveryStriped#testLeaseRecovery;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;207396382;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencingWithReplication#testFencingStress;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;895877045;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testBlockTokenExpired;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;260695274;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestSeveralNameNodes#testCircularLinkedListWrites;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;16890232;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestMiniDFSCluster#testClusterNoStorageTypeSetForDatanodes;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=451127833275136979681570923919456002814411283923504513307211204904506434447557921872443231409677390];java.lang.IllegalArgumentException;557580177;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFSForHA#testMultipleNamespacesConfigured;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;363157183;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testAppendWhileInSafeMode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1314120543;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testCorrectNumberOfBlocksAfterRestart;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1891625954;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRetryCacheOnStandbyNN;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=988995845529];java.lang.IllegalArgumentException;47318317;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testFileCreationError1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1947810952;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics#testHAInodeCount;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;242984585;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestPendingCorruptDnMessages#testChangedStorageId;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1512111427;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestQuotasWithHA#testGetQuotaUsageOnStandby;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1065175513;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCreateEditsLog#testCanLoadCreatedEditsLog;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=8988137935];java.lang.IllegalArgumentException;1812463876;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testGetTrashRoots;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1797311060;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testProvisionTrash;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1006717360;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions#testLeasesRenewedOnTransition;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1501791752;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testTransitionToActiveWhenSafeMode;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=463349191700674521749916206851997887669468986618324];java.lang.IllegalArgumentException;358249223;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestHDFSFileSystemContract#testWriteReadAndDeleteHalfABlock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task42/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/p75rZNB7hH);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.TestHDFSFileSystemContract.setUp(TestHDFSFileSystemContract.java:45), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestPendingDataNodeMessages#testPendingDataNodeMessagesWithEC;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1573153101;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.tools.TestECAdmin#testXOR21MinRacks;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;834532860;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot#testSnapshotOpsOnRootReservedPath;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=72085955881];java.lang.IllegalArgumentException;785333865;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot#testSnapshotOpsOnReservedPath;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=357218579753100559111326707539369682013594347128069625913608255005556833392524705009483];java.lang.IllegalArgumentException;147202229;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestPread#testHedgedPreadDFSBasic;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=7787406124938035819139111073728920943011055142505600844539936492804088604899247528];java.lang.IllegalArgumentException;419938577;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestAddBlockRetry#testAddBlockRetryShouldReturnBlockWithLocations;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=970350571565394451415736817712221];java.lang.IllegalArgumentException;1220860711;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsck#testFsckOpenECFiles;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task37/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/2G00K1AdFr);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckOpenECFiles(TestFsck.java:705), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckOpenECFiles$$CONFUZZ(TestFsck.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsck#testFsckWithMaintenanceReplicas;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task37/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/N4aAWz1q6G);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckWithMaintenanceReplicas(TestFsck.java:1924), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckWithMaintenanceReplicas$$CONFUZZ(TestFsck.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsck#testFsck;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=133617695145495626110407950339873526214975347832649747826449463054678552796];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task37/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/spAwepkuLp);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsck(TestFsck.java:217), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsck$$CONFUZZ(TestFsck.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testDecommission2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 47743049 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSImageWithSnapshot#testSnapshotOnRoot;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4036072939900595132325174722];java.lang.IllegalArgumentException;1432966627;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestSequentialBlockId#testTriggerBlockIdCollision;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=4708944519196121233035729658297930830470111219093885];java.lang.IllegalArgumentException;980278402;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapAliasmap#testAliasmapBootstrap;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=795276164990364];java.lang.IllegalArgumentException;1761005295;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestHFlush#testHFlushInterrupted;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1251232460;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestHDFSFileSystemContract#testWriteReadAndDeleteTwoBlocks;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6722505777566603675957236392155532352300004600301755955358542540703];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task134/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/NZljMGk69d);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.TestHDFSFileSystemContract.setUp(TestHDFSFileSystemContract.java:45), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.TestHDFSFileSystemContract#testWriteReadAndDeleteOneBlock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6811009961269];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task134/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/BNx2PlkNhZ);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.TestHDFSFileSystemContract.setUp(TestHDFSFileSystemContract.java:45), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testConcurrentFileCreation;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;2053722380;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestINodeFile#testReservedFileNames;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;548482939;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommission#testDecommission;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1088216598 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testStandbyExceptionThrownDuringCheckpoint;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;245026279;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade#testDatanodeRUwithRegularUpgrade;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1606983167;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestErasureCodingPoliciesWithRandomECPolicy#testBasicSetECPolicy;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;291092204;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamUpdatePipeline#testDFSStripedOutputStreamUpdatePipeline;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=8991349150];java.lang.IllegalArgumentException;1925694968;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestBlockReplacement#testBlockMoveAcrossStorageInSameNode;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1932329077;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeHotSwapVolumes#testFullBlockReportAfterRemovingVolumes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1494078085;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testSetXAttr;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1873221549;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAMetrics#testHAMetrics;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;77553698;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDnRespectsBlockReportSplitThreshold#testCornerCaseUnderThreshold;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3719544171758469120904043793771];java.lang.IllegalArgumentException;1802267076;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations#testClusterIdMismatch;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=8602015665813233111495305199115901555844345];java.lang.IllegalArgumentException;2025609398;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestErasureCodingMultipleRacks#testSkewedRack2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=24650798158012563485323659104438330416571746577];java.lang.IllegalArgumentException;100084800;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testCheckpointSucceedsWithLegacyOIVException;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1793351305;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testBlocksAddedWhileInSafeMode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3317688732468961384558];java.lang.IllegalArgumentException;841964877;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRemoveXAttr;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=723425940883801364004929693684097448822617633];java.lang.IllegalArgumentException;1777757669;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRename;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1116732791;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testUpdatePipeline;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2679666960852];java.lang.IllegalArgumentException;1905447824;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testDelete;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=6933669195531644612343495317605351026550163144667589];java.lang.IllegalArgumentException;338727513;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testAppend;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=847657809632047204649164263];java.lang.IllegalArgumentException;1140652433;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRename2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1121617665892939671796651843838106115564058223783354278840814];java.lang.IllegalArgumentException;1710906532;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestBlockScanner#testDatanodeCursor;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task83/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/iPqXZA89Ip);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.server.datanode.TestBlockScanner$TestContext.<init>(TestBlockScanner.java:107), org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testDatanodeCursor(TestBlockScanner.java:554), org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testDatanodeCursor$$CONFUZZ(TestBlockScanner.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n', 'org.apache.hadoop.hdfs.TestFileAppend2#testSimpleAppend2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=80987682132366239501666715111533142871947853556308898553638509542463023555833087359545028801197];java.lang.IllegalArgumentException;761784159;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystemMBean#testWithFSNamesystemWriteLock;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=5029566593896380483044102];java.lang.IllegalArgumentException;137372455;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testTrashStickyBit;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;131164262;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestViewDistributedFileSystem#testStorageFavouredNodes;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=388600863887185406867435330185082167560245998948233865443];java.lang.IllegalArgumentException;1463690519;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testWebHdfsPathWithSemicolon;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=57814225039];java.lang.IllegalArgumentException;516216439;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testDeleteSnapshotWithPermissionsDisabled;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;406379313;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testCreateSnapshot;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=591432929843626119619144874];java.lang.IllegalArgumentException;1075199225;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSImage#testLoadMtimeAtime;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3408765349842701263472083608720807906739493];java.lang.IllegalArgumentException;157234982;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestFileAppend#testAppend2Twice;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task11/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/2d9r0NkO3W);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.TestFileAppend.testAppend2Twice(TestFileAppend.java:355), org.apache.hadoop.hdfs.TestFileAppend.testAppend2Twice$$CONFUZZ(TestFileAppend.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testComplexFailoverIntoSafemode;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;206680433;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testWriteOverGracefulFailoverWithDnFail;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1115597214;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics#testTimeoutMetric;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=84174517];java.lang.IllegalArgumentException;1537189348;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testCountOnDecommissionedNodeList;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3489829454256095539394032836886343172367222441474570393329120];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1130745433 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade#testDatanodeRollingUpgradeWithRollback;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=5593648031051033939174701];java.lang.IllegalArgumentException;1184962785;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testMultipleLevelDirectoryForSatisfyStoragePolicy;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1443951205;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyBlockManagement#testInvalidateBlock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=7306412899601450225869800116016473826947470662853486582];java.lang.IllegalArgumentException;551772355;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testEnterSafeModeInSBNShouldNotThrowNPE;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=91470122142893878684413563];java.lang.IllegalArgumentException;1933331182;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testCreate;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=52154089649521142971566457085915674889345027706599122588326473168317339562945049286545085];java.lang.IllegalArgumentException;856933284;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testPendingNodes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=323454693964137226123034600235353983587340951295906923587];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1348978815 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestFailoverWithBlockTokensEnabled#testFailoverAfterRegistration;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;94904636;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestReconstructStripedBlocks#testMissingStripedBlock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1604554787;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.TestGlobPaths#testLocalFilesystem;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 93768846 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)]\n', 'org.apache.hadoop.fs.TestSWebHdfsFileContextMainOperations#testGetFileContext1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=179177546769037301];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 48712169 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextMainOperationsBaseTest.testGetFileContext1(FileContextMainOperationsBaseTest.java:1447), org.apache.hadoop.fs.TestSWebHdfsFileContextMainOperations.testGetFileContext1$$CONFUZZ(TestSWebHdfsFileContextMainOperations.java)]\n', 'org.apache.hadoop.hdfs.TestMiniDFSCluster#testClusterSetDatanodeDifferentStorageType;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;973731513;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.shell.TestHdfsTextCommand#testDisplayForAvroFiles;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1123825173;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommission#testUsedCapacity;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=8780031783544996328661324234045302977679023459412906431997527646918028022471570516672793262391658];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1919684369 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.tools.TestDebugAdmin#testComputeMetaCommand;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;764652989;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDataTransferProtocol#testDataTransferProtocol;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;940000744;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestViewDistributedFileSystem#testStatistics2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;2038727510;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestAddBlockRetry#testRetryAddBlockWhileInChooseTarget;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=51519627283105734891186679344220057221698558904294401486193115417039617347];java.lang.IllegalArgumentException;1055879194;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testGetHomeDirectory;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1787409043;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDistributedFileSystem#testStatistics2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2708496029000975195673413035305509284028726845545072];java.lang.IllegalArgumentException;26984141;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalMkdirNew2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=311129715797579954828384531240109898602];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 821111781 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirNew2(ViewFsBaseTest.java:713), org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot.testInternalMkdirNew2$$CONFUZZ(TestViewFsAtHdfsRoot.java)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testResolvePathDanglingLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 172849685 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalCreateMissingDir;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=52288055098456766421190623650628612353070922675166];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 736562784 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalMkdirSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=350893664422689608117632902443926812483387502500621];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 137267383 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirSlash(ViewFsBaseTest.java:691), org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot.testInternalMkdirSlash$$CONFUZZ(TestViewFsAtHdfsRoot.java)]\n', 'org.apache.hadoop.hdfs.TestDisableConnCache#testDisableCache;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;471205709;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.tools.TestJMXGet#testDataNode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1335923170;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestRollingUpgrade#testDFSAdminDatanodeUpgradeControlCommands;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;807898434;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatus#testDecommissionStatus;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1464789789 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testListEncryptionZonesWithSnapshots;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1905865861;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestConnCache#testReadFromOneDN;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=7315093];java.lang.IllegalArgumentException;638582901;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testPendingNodeButDecommissioned;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 893119059 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestBlockScanner#testDisableVolumeScanner;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task93/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/Bdeu0Wfr1r);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.server.datanode.TestBlockScanner$TestContext.<init>(TestBlockScanner.java:107), org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testDisableVolumeScanner(TestBlockScanner.java:285), org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testDisableVolumeScanner$$CONFUZZ(TestBlockScanner.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n', 'org.apache.hadoop.hdfs.TestDFSRename#testRename2Options;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;692820215;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions#testTransitionToCurrentStateIsANop;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=270311814084309145929380726227625652705946403726577325144934656619832427163001768688246638984485];java.lang.IllegalArgumentException;1250420644;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSClientRetries#testDFSClientConfigurationLocateFollowingBlock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1129018010903];java.lang.IllegalArgumentException;621076879;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testCheckpointCancellation;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=6114505378900421612403377678076];java.lang.IllegalArgumentException;868615075;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestQuotaByStorageType#testQuotaByStorageTypePersistenceInEditLog;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;596659859;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestQuotaByStorageType#testQuotaByStorageTypePersistenceInFsImage;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=8703460988640042109934078984358455433666265036829963911887335];java.lang.IllegalArgumentException;1341393865;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalMkdirNew2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 118640141 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirNew2(ViewFsBaseTest.java:713), org.apache.hadoop.fs.viewfs.TestViewFsHdfs.testInternalMkdirNew2$$CONFUZZ(TestViewFsHdfs.java)]\n', 'org.apache.hadoop.hdfs.TestUnsetAndChangeDirectoryEcPolicy#testUnsetEcPolicyInEditLog;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;397708446;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSShell#testFilePermissions;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4355879078197978984708367110463827055955941];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1006764310 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations#testMiniDFSClusterWithMultipleNN;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=22341191475127146];java.lang.IllegalArgumentException;716228768;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalMkdirSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=7956412091384];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1619884033 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirSlash(ViewFsBaseTest.java:691), org.apache.hadoop.fs.viewfs.TestViewFsHdfs.testInternalMkdirSlash$$CONFUZZ(TestViewFsHdfs.java)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalMkdirExisting2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1821985078 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirExisting2(ViewFsBaseTest.java:702), org.apache.hadoop.fs.viewfs.TestViewFsHdfs.testInternalMkdirExisting2$$CONFUZZ(TestViewFsHdfs.java)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalCreate2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1218995309 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFileNonRecursive(FileContextTestHelper.java:140)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testRenameAcrossMounts1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 2023716071 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testResolvePathThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=21231289338037437364416536512290220850678198104729926127017416256303810290462582302868427940216777140];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 2015956765 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalCreate2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=64227135560782969500710035727308754159];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 129807056 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFileNonRecursive(FileContextTestHelper.java:140)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testOperationsThroughMountLinks;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3033497736404280409102172065296913383154792217484588528270062068892902894049];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 221356661 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFileNonRecursive(FileContextTestHelper.java:140)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalRenameToSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=53905];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 77919463 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalMkdirNew;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=57751271032434431018798043783294015738536771997422771563020946086173654176067];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1206445939 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirNew(ViewFsBaseTest.java:708), org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot.testInternalMkdirNew$$CONFUZZ(TestViewFsAtHdfsRoot.java)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalMkdirExisting1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=573911462723693620430193418516348];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1807480973 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirExisting1(ViewFsBaseTest.java:697), org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot.testInternalMkdirExisting1$$CONFUZZ(TestViewFsAtHdfsRoot.java)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalMkdirExisting1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1250113428 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirExisting1(ViewFsBaseTest.java:697), org.apache.hadoop.fs.viewfs.TestViewFsHdfs.testInternalMkdirExisting1$$CONFUZZ(TestViewFsHdfs.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testOpenFileWhenNNAndClientCrashAfterAddBlock;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;3273604;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testListCachePools;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;658533243;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testRenameAcrossMounts3;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 2137598189 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalDeleteExisting2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=647559050587031380876860371101416682168418042987002106133889];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1115526742 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testRenameAcrossMounts4;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1415212318 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testSymlinkTarget;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 666906718 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testCreateEZWithNoProvider;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=87486558522162870289365336589025696];java.lang.IllegalArgumentException;229631099;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testBlocksRemovedBeforeStandbyRestart;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;340970917;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestHDFSFileSystemContract#testOverWriteAndRead;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task133/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/A6LTW7lW6P);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.TestHDFSFileSystemContract.setUp(TestHDFSFileSystemContract.java:45), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testFsImageCorruption;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;2002185902;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testDecommissionWithOpenfile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=370588450];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 890255152 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testRecommission;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1926153207 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testResolvePathThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1412531883 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testgetFSonDanglingLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 706994285 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalCreateMissingDir3;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=826518876897302920182735299385763402665237792977897795507457637669165594616932155871];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 826090393 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testRespectsServerDefaults;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=284236];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1401466258 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testFileStatusOnMountLink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 31112189 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testResolvePathMissingThroughMountPoints2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 15648038 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testResolvePathMissingThroughMountPoints2(ViewFsBaseTest.java:672), org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot.testResolvePathMissingThroughMountPoints2$$CONFUZZ(TestViewFsAtHdfsRoot.java)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testResolvePathMissingThroughMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1551302797 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testRespectsServerDefaults;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=62694547320168635177696365642868369748811098345196009057873175839328620459141387947158493];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 253935834 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testResolvePathMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 159555742 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testResolvePathMissingThroughMountPoints2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 899441353 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testResolvePathMissingThroughMountPoints2(ViewFsBaseTest.java:672), org.apache.hadoop.fs.viewfs.TestViewFsHdfs.testResolvePathMissingThroughMountPoints2$$CONFUZZ(TestViewFsHdfs.java)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testgetFileLinkStatus;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 854624388 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure#testAddBlockWhenNoSufficientParityNumOfNodes;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=5437563495];java.lang.IllegalArgumentException;1239646456;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure#testIdempotentCloseWithFailedStreams;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;874015668;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsck#testFsckUpgradeDomain;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=84076435477538506973445162163220253101176802643069467799726609421060];java.lang.IllegalArgumentException;55442003;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommissionWithBackoffMonitor#testDecommissionWithOpenFileAndBlockRecovery;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=26314735203478761175212624082199163360828249191679];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 786460418 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalCreateMissingDir3;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=30997579609689514603096544424888864542053246040183328973034840514557498218774253128352212292];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 403304960 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testRenameAcrossMounts2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1790368206 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.hdfs.TestLargeBlock#testLargeBlockSize;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4847931719899784140006139211479686694096879412571172405219691195];java.lang.IllegalArgumentException;1190395696;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testReadsAllowedDuringCheckpoint;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=75129030240405672561786552362655001];java.lang.IllegalArgumentException;728882986;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testPipelineRecoveryStress;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=599808534148994665067209162832958776818897];java.lang.IllegalArgumentException;368729453;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithRename;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=839705211179990517387016777215419129833884667912263840500068218775211009408483793];java.lang.IllegalArgumentException;174528147;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testParentDirWithUCFileDeleteWithSnapShot;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=303210198811746539778112195661958];java.lang.IllegalArgumentException;536959865;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSClientExcludedNodes#testExcludedNodesForgiveness;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;286252195;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestFSMainOperationsWebHdfs#testCopyToLocalWithUseRawLocalFileSystemOption;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1451433978 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:424)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testDeleteSnapshot1;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=8345843209330799340018961];java.lang.IllegalArgumentException;740993228;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testDeleteSnapshot2;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1000756847;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testSnapshotDiffReportWithConcat;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;758807213;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockTokenWithDFS#testRead;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;860529368;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testGetBlockLocations;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1756422525 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestDNFencing#testNNClearsCommandsOnFailoverAfterStartup;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=571731829176254617962123485527897668081297863918356887733586021778004324293031602277131];java.lang.IllegalArgumentException;1335365743;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalRenameToSlash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=308076347673082049356440];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1581650448 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNamenodeRetryCache#testRetryCacheRebuild;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=961192899215505702682484824372868918327400539864];java.lang.IllegalArgumentException;122074692;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalMkdirExisting2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=8805404495522972535778637488702376465445124705420675];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1898882459 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirExisting2(ViewFsBaseTest.java:702), org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot.testInternalMkdirExisting2$$CONFUZZ(TestViewFsAtHdfsRoot.java)]\n', 'org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testWhenOnlyFewTargetDatanodeAreAvailableToSatisfyStoragePolicy;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=82263090297359285];java.lang.IllegalArgumentException;1550628087;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommission#testCountOnDecommissionedNodeList;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=3396336133128496165169120473218234992331638369922640406598120050957403437582776];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1210300476 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.TestLeaseRecovery#testLeaseRecoveryAndAppendWithViewDFS;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;331183594;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSShellGenericOptions#testDFSCommand;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=15762526707430624422];java.lang.IllegalArgumentException;2118812047;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestHDFSFileSystemContract#testWriteReadAndDeleteOneAndAHalfBlocks;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task62/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/wt5km5O6mO);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.TestHDFSFileSystemContract.setUp(TestHDFSFileSystemContract.java:45), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.mover.TestMover#testMoveWhenStoragePolicySatisfierIsRunning;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;194852796;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testSaveNamespaceWithRenamedLease;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1766766167;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations#testFedSingleNN;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1019411290;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.tools.TestECAdmin#testRS63MinDN;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=72130858990681014582072806245018736253689379100529926830920722827690834868939904979213];java.lang.IllegalArgumentException;759823471;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyIsHot#testDatanodeRestarts;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=62841270961061964338949499755683195];java.lang.IllegalArgumentException;9163663;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testCheckpointBeforeNameNodeInitializationIsComplete;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;693365761;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testStandbyAndObserverState;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1219291610;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testSafeModeExitAfterTransition;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1416146101;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testWebHdfsBackwardCompatibleSpecialCharacterFile;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1748106634;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRenameSnapshot;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;2082343388;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestRBWBlockInvalidation#testRWRShouldNotAddedOnDNRestart;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=818537];java.lang.IllegalArgumentException;877549236;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.mover.TestMover#testBalancerMaxIterationTimeNotAffectMover;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1217542078;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList#testInstanceOfAddReplicaThreadPool;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;915423975;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestDecommissioningStatusWithBackoffMonitor#testDecommissionStatusAfterDNRestart;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1118163310 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestLazyPersistLockedMemory#testReleaseOnFileDeletion;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;2083549018;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDecommission#testDecommission2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=852916401212593040867791369914017769013639870283447081];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1333809218 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.TestDecommission#testDecommissionWithOpenFileAndBlockRecovery;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 2030621508 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot#testSnapshotMtime;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;468815594;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testRenameAcrossMounts1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1163361493 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks#testNodeDecomissionRespectsRackPolicy;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=43756129699110736841634625];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 391052163 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testBasicOperationsRootDir;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;461955698;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestReplication#testBadBlockReportOnTransferMissingBlockFile;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=809605057861062];java.lang.IllegalArgumentException;1197669866;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure#testAddBlockWhenNoSufficientDataBlockNumOfNodes;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=4015863466438355392935646920765484807116457984055650219128644708735202442680];java.lang.IllegalArgumentException;1700580601;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestFileLengthOnClusterRestart#testFileLengthWithHSyncAndClusterRestartWithOutDNsRegister;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1569475597;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean#testTotalBlocksMetrics;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=212415676029343187059669901907344];java.lang.IllegalArgumentException;262875264;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.TestHDFSFileContextMainOperations#testGetFileContext1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1486827898874628390158294049282088689210051274419107131];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 737066282 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextMainOperationsBaseTest.testGetFileContext1(FileContextMainOperationsBaseTest.java:1447), org.apache.hadoop.fs.TestHDFSFileContextMainOperations.testGetFileContext1$$CONFUZZ(TestHDFSFileContextMainOperations.java)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestBlockScanner#testMultipleBlockPoolScanning;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=12725279794238572887117];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task19/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/yWjglBaPEE);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.server.datanode.TestBlockScanner$TestContext.<init>(TestBlockScanner.java:107), org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testMultipleBlockPoolScanning(TestBlockScanner.java:645), org.apache.hadoop.hdfs.server.datanode.TestBlockScanner.testMultipleBlockPoolScanning$$CONFUZZ(TestBlockScanner.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlocksWithNotEnoughRacks#testNodeDecomissionWithOverreplicationRespectsRackPolicy;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 985329956 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testGetDelegationTokens;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1606730377 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalMkdirNew;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=630312948641118754803539178397620365826046050946436193730625];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1483261148 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:804), org.apache.hadoop.fs.viewfs.ViewFsBaseTest.testInternalMkdirNew(ViewFsBaseTest.java:708), org.apache.hadoop.fs.viewfs.TestViewFsHdfs.testInternalMkdirNew$$CONFUZZ(TestViewFsHdfs.java)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testNamenodeRestart;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;339123403;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testWithCheckpoint;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=45416142383924342825179624048123755270312325801993041094];java.lang.IllegalArgumentException;146509407;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testUCFileDeleteWithSnapShot;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1045084626;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testFilesDeletionWithCheckpoint;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;7934015;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.diskbalancer.command.TestDiskBalancerCommand#testSubmitPlanInNonRegularStatus;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;613261203;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyCheckpoints#testBothNodesInStandbyState;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=551896613292476371985107293991143676891555738145582633516858629091630287390420534071];java.lang.IllegalArgumentException;1335060865;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testWebHdfsAppend;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1158517932;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks#testWithAllCorruptReplicas;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1373707151;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testUpgradeAndRestart;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1767754230;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.sps.TestExternalStoragePolicySatisfier#testWhenStoragePolicySetToCOLD;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1339026458;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalCreateMissingDir2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=57071637];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 801156581 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testInternalDeleteExisting2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1735084656 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsHdfs#testRenameAcrossMounts2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=754393723598345432759648455204172755631688470718697184070270547308852768116763708287767532620];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 552600933 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testInternalCreateMissingDir2;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1224225369 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:110), org.apache.hadoop.fs.FileContextTestHelper.createFile(FileContextTestHelper.java:124)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFsAtHdfsRoot#testResolvePathMountPoints;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=501677484935293708489991986631214148039150165750105407564010235909985975875868259561978579];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 429765168 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot#testSnapshot;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;424968101;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestBatchIbr#testIbr;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=35767874];java.lang.IllegalArgumentException;593043724;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.TestWebHdfsFileContextMainOperations#testGetFileContext1;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1023955972 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileContext.getUMask(FileContext.java:585), org.apache.hadoop.fs.FileContext.create(FileContext.java:693), org.apache.hadoop.fs.FileContextMainOperationsBaseTest.testGetFileContext1(FileContextMainOperationsBaseTest.java:1447), org.apache.hadoop.fs.TestWebHdfsFileContextMainOperations.testGetFileContext1$$CONFUZZ(TestWebHdfsFileContextMainOperations.java)]\n', 'org.apache.hadoop.hdfs.TestHDFSFileSystemContract#testAppend;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=66207];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task109/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/GiR63y6a9K);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.TestHDFSFileSystemContract.setUp(TestHDFSFileSystemContract.java:45), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsck#testFsckReplicaDetails;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;MiniDFSCluster base directory already defined (/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task109/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/RYd02qEKjG);[org.apache.hadoop.hdfs.MiniDFSCluster$Builder.<init>(MiniDFSCluster.java:250), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckReplicaDetails(TestFsck.java:944), org.apache.hadoop.hdfs.server.namenode.TestFsck.testFsckReplicaDetails$$CONFUZZ(TestFsck.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testTruncateFailure;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2661254903715864270371293770187818495410825987105895705220];java.lang.IllegalArgumentException;817244057;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZones#testRootDirEZTrash;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=806563038112352422659691848462902563208065464059236235179005721489895261495959603];java.lang.IllegalArgumentException;182160760;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions#testTransitionActiveToStandby;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=2882162782];java.lang.IllegalArgumentException;809617281;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDeleteBlockPool#testDfsAdminDeleteBlockPool;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=486053552095208562579776441083712138977816665346487239373212677];java.lang.IllegalArgumentException;549627708;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testModifyCacheDirectiveInfo;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;1236737047;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testAddCacheDirectiveInfo;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=581746844286004742596737];java.lang.IllegalArgumentException;160891284;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testRemoveCacheDescriptor;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4070648860750075579738926695];java.lang.IllegalArgumentException;478491352;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestRetryCacheWithHA#testAddCachePool;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;174128391;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics#testReceivePacketMetrics;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=7242043438631004107875331445370340639169794686273542830458984330395603511531574];java.lang.IllegalArgumentException;644540743;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMetrics#testDataNodeTimeSpend;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=8026171447684490567851307731912502967648536886892638587957753524956611764303200880];java.lang.IllegalArgumentException;559547371;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestProcessCorruptBlocks#testWithReplicationFactorAsOne;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=69892869272556498692623954688216275900444521304213502674555552438538668743246576187269844];java.lang.IllegalArgumentException;1254021327;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.fs.TestResolveHdfsSymlink#testFcResolveAfs;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=526621234224989];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 2033018676 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testSafeBlockTracking;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=529846911463275777060936849687025217];java.lang.IllegalArgumentException;930274552;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testBlocksRemovedWhileInSafeMode;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4882277153059837680120906445050348564215516063362201855846907710255496641862762273643207994171963];java.lang.IllegalArgumentException;1712745254;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestMultipleNNPortQOP#testAuxiliaryPortSendingQOP;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6866680690804716501881588073631651045282462988];java.lang.IllegalArgumentException;1315416977;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckPointForSecurityTokens#testSaveNamespace;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.lang.IllegalArgumentException;709564467;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[dfs.datanode.volumes.replica-add.threadpool.size];org.apache.hadoop.HadoopIllegalArgumentException;30;['org.apache.hadoop.fs.TestSWebHdfsFileContextMainOperations#testCreateFlagAppendOverwrite;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=697302397285767097740349295447880932056186941240];org.apache.hadoop.HadoopIllegalArgumentException;[OVERWRITE, APPEND]Both append and overwrite options cannot be enabled.;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:159), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.fs.TestWebHdfsFileContextMainOperations#testCreateFlagAppendCreateOverwrite;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;[CREATE, OVERWRITE, APPEND]Both append and overwrite options cannot be enabled.;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:159), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testAddVolumeWithSameStorageUuid;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=968019007252221546194713742395079070543667745213121721080647421];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1065992129. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.fs.TestSWebHdfsFileContextMainOperations#testNullCreateFlag;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;null does not specify any options;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:151), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl#testProvidedReplicaWithPathHandle;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 723860763. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl.setUp(TestProvidedImpl.java:342), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testMoveBlockFailure;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 2121594449. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testTransferAndNativeCopyMetrics;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1942786256. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.fs.TestWebHdfsFileContextMainOperations#testCreateFlagAppendOverwrite;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=66774170833210714526897955041786474835200207081];org.apache.hadoop.HadoopIllegalArgumentException;[OVERWRITE, APPEND]Both append and overwrite options cannot be enabled.;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:159), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.fs.TestSWebHdfsFileContextMainOperations#testCreateFlagAppendCreateOverwrite;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6582589841396370433198495993037375146002538944980784520153817];org.apache.hadoop.HadoopIllegalArgumentException;[CREATE, OVERWRITE, APPEND]Both append and overwrite options cannot be enabled.;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:159), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.fs.TestHDFSFileContextMainOperations#testCreateFlagAppendOverwrite;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;[OVERWRITE, APPEND]Both append and overwrite options cannot be enabled.;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:159), org.apache.hadoop.hdfs.DFSClient.primitiveCreate(DFSClient.java:1322), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:105), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:60), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl#testProvidedVolumeContents;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=77604017242840712531414458020265650535];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 203052693. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl.setUp(TestProvidedImpl.java:342), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.fs.TestHDFSFileContextMainOperations#testNullCreateFlag;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=31896380996897073792095911258714836765144107604757523812441210039];org.apache.hadoop.HadoopIllegalArgumentException;null does not specify any options;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:151), org.apache.hadoop.hdfs.DFSClient.primitiveCreate(DFSClient.java:1322), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:105), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:60), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.fs.TestHDFSFileContextMainOperations#testCreateFlagAppendCreateOverwrite;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=132410367421288253387712342156402382788487965764628014217837338710736884433700511045453741743];org.apache.hadoop.HadoopIllegalArgumentException;[CREATE, OVERWRITE, APPEND]Both append and overwrite options cannot be enabled.;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:159), org.apache.hadoop.hdfs.DFSClient.primitiveCreate(DFSClient.java:1322), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:105), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:60), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testLoadingDfsUsedForVolumes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=61465550216332999853498292277997126];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1477553815. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testAddVolumes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=65333497867603701173143777334663573451223222474014928759553806099575099785232238311486987010];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 149921005. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testDataDirWithPercent;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 339157592. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testRemoveOneVolume;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=709851987142956745751444526401];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 443834701. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl#testScannerWithProvidedVolumes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1279866397. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl.setUp(TestProvidedImpl.java:342), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl#testProvidedBlockRead;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1128514461. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl.setUp(TestProvidedImpl.java:342), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl#testProvidedVolumeImpl;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1428318293. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestProvidedImpl.setUp(TestProvidedImpl.java:342), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testAddVolumeFailureReleasesInUseLock;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 2039049167. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testLoadingDfsUsedForVolumesExpired;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=28806254908220155403988824741562184796932863];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 2016286782. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.fs.TestHDFSFileContextMainOperations#testEmptyCreateFlag;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=27370404306580994163114445592388936127132739145030886458581826772996116277436138815];org.apache.hadoop.HadoopIllegalArgumentException;[] does not specify any options;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:151), org.apache.hadoop.hdfs.DFSClient.primitiveCreate(DFSClient.java:1322), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:105), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:60), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testRemoveTwoVolumes;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 729142947. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testRemoveNewlyAddedVolume;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1216462581. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.fs.TestWebHdfsFileContextMainOperations#testEmptyCreateFlag;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;[] does not specify any options;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:151), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.fs.TestWebHdfsFileContextMainOperations#testNullCreateFlag;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6924524367679104034900528754710145806021868814315076412080290446660535911871351];org.apache.hadoop.HadoopIllegalArgumentException;null does not specify any options;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:151), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.fs.TestSWebHdfsFileContextMainOperations#testEmptyCreateFlag;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;[] does not specify any options;[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:151), org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:174), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testMoveBlockSuccess;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1497385297. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testDeletingBlocks;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=654399715819061340319047121528633078423521898860568694154707830157703901134760];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1392589990. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n']
[dfs.client.short.circuit.num];java.io.IOException;59;['org.apache.hadoop.hdfs.server.namenode.TestProtectedDirectories#testRename;id_000000;[dfs.client.short.circuit.num=590143458];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestProtectedDirectories#testDelete;id_000000;[dfs.client.short.circuit.num=1030335102];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy#testStatefulRead;id_000000;[dfs.client.short.circuit.num=1469319102];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions#testTransitionSynchronization;id_000000;[dfs.client.short.circuit.num=1371131147];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestStorageRestore#testStorageRestoreFailure;id_000000;[dfs.client.short.circuit.num=1255930625];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testNamespaceVerifiedOnFileTransfer;id_000000;[dfs.client.short.circuit.num=935968742];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testServerDefaultsWithCaching;id_000001;[dfs.client.short.circuit.num=1062590729];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testSaveNamespaceBeforeShutdown;id_000000;[dfs.client.short.circuit.num=1779536902];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean#testNNDirectorySize;id_000001;[dfs.client.short.circuit.num=1157732117];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby#testSharedEditsMissingLogs;id_000001;[dfs.client.short.circuit.num=1277778474];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA#testRollbackWithNfs;id_000000;[dfs.client.short.circuit.num=676829533];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testSetStoragePolicyWhenPolicyDisabled;id_000000;[dfs.client.short.circuit.num=614350627];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.mover.TestMover#testMoverCliWithFederationHA;id_000000;[dfs.client.short.circuit.num=1861951861];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAFsck#testHaFsck;id_000001;[dfs.client.short.circuit.num=2077605047];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeRollingUpgrade#testDatanodeRUwithRegularUpgrade;id_000000;[dfs.client.short.circuit.num=1247845939];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer#testBalancer0Integrity;id_000001;[dfs.client.short.circuit.num=1683553250];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeMultipleRegistrations#testClusterIdMismatch;id_000001;[dfs.client.short.circuit.num=692228461];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes#testBalancerWithHANameNodes;id_000000;[dfs.client.short.circuit.num=2057054135];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.balancer.TestBalancerLongRunningTasks#testMaxIterationTime;id_000001;[dfs.client.short.circuit.num=1673438449];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogsDuringFailover#testFailoverFinalizesAndReadsInProgressWithPartialTxAtEnd;id_000001;[dfs.client.short.circuit.num=141896772];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testWebHdfsAllowandDisallowSnapshots;id_000000;[dfs.client.short.circuit.num=528664014];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeMetrics#testVolumeMetrics;id_000001;[dfs.client.short.circuit.num=1478795374];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testContentSummary;id_000001;[dfs.client.short.circuit.num=992105633];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testServerDefaultsWithMinimalCaching;id_000000;[dfs.client.short.circuit.num=2108070853];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestRollingUpgrade#testDFSAdminRollingUpgradeCommands;id_000000;[dfs.client.short.circuit.num=726504140];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby#testDownloadingLaterCheckpoint;id_000000;[dfs.client.short.circuit.num=938763317];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby#testSuccessfulBaseCase;id_000001;[dfs.client.short.circuit.num=587549256];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testNameDirLocking;id_000000;[dfs.client.short.circuit.num=1860687956];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testStorageAlreadyLockedErrorMessage;id_000001;[dfs.client.short.circuit.num=1238216935];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDistributedFileSystem#testFileSystemCloseAll;id_000001;[dfs.client.short.circuit.num=783634499];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby#testOtherNodeNotActive;id_000000;[dfs.client.short.circuit.num=1774381588];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics#testReadWriteOps;id_000003;[dfs.client.short.circuit.num=1922275450];java.io.IOException;Premature EOF reading from org.apache.hadoop.net.SocketInputStream@56bfd8cb;[org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.readChannelFully(PacketReceiver.java:258), org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doReadFully(PacketReceiver.java:207), org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.doRead(PacketReceiver.java:134), org.apache.hadoop.hdfs.protocol.datatransfer.PacketReceiver.receiveNextPacket(PacketReceiver.java:102), org.apache.hadoop.hdfs.client.impl.BlockReaderRemote.readNextPacket(BlockReaderRemote.java:187)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testExistingUser;id_000001;[dfs.client.short.circuit.num=1890510068];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestSetTimes#testAtimeUpdate;id_000000;[dfs.client.short.circuit.num=1505449739];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSeparateEditsDirLocking;id_000001;[dfs.client.short.circuit.num=1093295851];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean#testQueueLength;id_000000;[dfs.client.short.circuit.num=468579781];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.mover.TestMover#testMoverCli;id_000001;[dfs.client.short.circuit.num=1431224820];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.tools.TestGetGroups#testExistingInterleavedWithNonExistentUsers;id_000000;[dfs.client.short.circuit.num=1493007516];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNetworkTopologyServlet#testPrintTopologyNoDatanodesJsonFormat;id_000001;[dfs.client.short.circuit.num=1252644799];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsTokens#testLazyTokenFetchForWebhdfs;id_000001;[dfs.client.short.circuit.num=657977948];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestProtectedDirectories#testMoveToTrash;id_000000;[dfs.client.short.circuit.num=1865101691];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemLinkFallback#testTrashRoot;id_000000;[dfs.client.short.circuit.num=541432957];java.io.IOException;ViewFs: Cannot initialize: Invalid entry in Mount table in config: name.key;[org.apache.hadoop.fs.viewfs.InodeTree.<init>(InodeTree.java:587), org.apache.hadoop.fs.viewfs.ViewFileSystem$1.<init>(ViewFileSystem.java:321), org.apache.hadoop.fs.viewfs.ViewFileSystem.initialize(ViewFileSystem.java:320), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469), org.apache.hadoop.fs.FileSystem.get(FileSystem.java:537)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSImage#testDigest;id_000001;[dfs.client.short.circuit.num=1696181213];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSShellGenericOptions#testDFSCommand;id_000000;[dfs.client.short.circuit.num=7953686];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testECPolicyCommands;id_000000;[dfs.client.short.circuit.num=596596543];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testStoragePolicy;id_000001;[dfs.client.short.circuit.num=1410164244];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystemMBean#testFsEditLogMetrics;id_000000;[dfs.client.short.circuit.num=915505680];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestNNHealthCheck#testNNHealthCheckWithSafemodeAsUnhealthy;id_000001;[dfs.client.short.circuit.num=1005934911];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testExistingInterleavedWithNonExistentUsers;id_000000;[dfs.client.short.circuit.num=1398775041];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testMultipleExistingUsers;id_000000;[dfs.client.short.circuit.num=333672058];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testNoUserGiven;id_000000;[dfs.client.short.circuit.num=1489775429];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testNonExistentUser;id_000001;[dfs.client.short.circuit.num=677482630];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testMultipleNonExistingUsers;id_000001;[dfs.client.short.circuit.num=1973968623];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestSetTimes#testGetBlockLocationsOnlyUsesReadLock;id_000001;[dfs.client.short.circuit.num=1896439702];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.TestDFSMkdirs#testMkdirRpcNonCanonicalPath;id_000000;[dfs.client.short.circuit.num=2061171254];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestInitializeSharedEdits#testDontOverWriteExistingDir;id_000000;[dfs.client.short.circuit.num=1333259219];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.tools.TestGetGroups#testExistingUser;id_000001;[dfs.client.short.circuit.num=95353684];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshot#testSnapshot;id_000000;[dfs.client.short.circuit.num=1984812201];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.balancer.TestBalancer#testUnknownDatanodeSimple;id_000000;[dfs.client.short.circuit.num=775016031];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n']
[dfs.client.short.circuit.num];java.lang.IllegalArgumentException;53;['org.apache.hadoop.hdfs.server.namenode.TestProtectedDirectories#testRename;id_000001;[dfs.client.short.circuit.num=1196747983];java.lang.IllegalArgumentException;13356361;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestProtectedDirectories#testDelete;id_000001;[dfs.client.short.circuit.num=1142038062];java.lang.IllegalArgumentException;1571475896;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testFileCreationWithOverwrite;id_000001;[dfs.client.short.circuit.num=2111066665];java.lang.IllegalArgumentException;1597764427;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSStripedInputStreamWithRandomECPolicy#testStatefulRead;id_000001;[dfs.client.short.circuit.num=738902686];java.lang.IllegalArgumentException;47559400;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAStateTransitions#testTransitionSynchronization;id_000001;[dfs.client.short.circuit.num=1072111176];java.lang.IllegalArgumentException;605337370;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestStorageRestore#testStorageRestoreFailure;id_000001;[dfs.client.short.circuit.num=760448171];java.lang.IllegalArgumentException;1672944244;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testNamespaceVerifiedOnFileTransfer;id_000001;[dfs.client.short.circuit.num=605249531];java.lang.IllegalArgumentException;750300708;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testServerDefaultsWithCaching;id_000000;[dfs.client.short.circuit.num=1807622376];java.lang.IllegalArgumentException;1137447692;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDeletion#testSnapshotDeleteWithConcat;id_000001;[dfs.client.short.circuit.num=515463238];java.lang.IllegalArgumentException;1098215593;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean#testNNDirectorySize;id_000000;[dfs.client.short.circuit.num=936831198];java.lang.IllegalArgumentException;1540930875;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby#testSharedEditsMissingLogs;id_000000;[dfs.client.short.circuit.num=1096551009];java.lang.IllegalArgumentException;1531107962;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestDFSUpgradeWithHA#testRollbackWithNfs;id_000001;[dfs.client.short.circuit.num=973685679];java.lang.IllegalArgumentException;1480209139;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testSetStoragePolicyWhenPolicyDisabled;id_000001;[dfs.client.short.circuit.num=1988669711];java.lang.IllegalArgumentException;1048162097;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.mover.TestMover#testMoverCliWithFederationHA;id_000001;[dfs.client.short.circuit.num=601707520];java.lang.IllegalArgumentException;1464229258;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestHAFsck#testHaFsck;id_000000;[dfs.client.short.circuit.num=1339583942];java.lang.IllegalArgumentException;132574183;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.balancer.TestBalancerWithSaslDataTransfer#testBalancer0Integrity;id_000000;[dfs.client.short.circuit.num=1854060812];java.lang.IllegalArgumentException;51225168;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testCheckpointTriggerOnTxnCount;id_000001;[dfs.client.short.circuit.num=1048764258];java.lang.IllegalArgumentException;1122991989;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.balancer.TestBalancerWithHANameNodes#testBalancerWithHANameNodes;id_000001;[dfs.client.short.circuit.num=1414886666];java.lang.IllegalArgumentException;1834791961;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.balancer.TestBalancerLongRunningTasks#testMaxIterationTime;id_000000;[dfs.client.short.circuit.num=886709563];java.lang.IllegalArgumentException;1355895704;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestEditLogsDuringFailover#testFailoverFinalizesAndReadsInProgressWithPartialTxAtEnd;id_000000;[dfs.client.short.circuit.num=1536496385];java.lang.IllegalArgumentException;430694927;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testWebHdfsAllowandDisallowSnapshots;id_000001;[dfs.client.short.circuit.num=521201729];java.lang.IllegalArgumentException;1582104421;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestDataNodeVolumeMetrics#testVolumeMetrics;id_000000;[dfs.client.short.circuit.num=187114575];java.lang.IllegalArgumentException;29493417;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testContentSummary;id_000000;[dfs.client.short.circuit.num=1474426754];java.lang.IllegalArgumentException;766939332;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestFileCreation#testServerDefaultsWithMinimalCaching;id_000001;[dfs.client.short.circuit.num=1261714833];java.lang.IllegalArgumentException;1074682180;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestRollingUpgrade#testDFSAdminRollingUpgradeCommands;id_000001;[dfs.client.short.circuit.num=714152095];java.lang.IllegalArgumentException;1273025254;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby#testSuccessfulBaseCase;id_000000;[dfs.client.short.circuit.num=1777961345];java.lang.IllegalArgumentException;1754696060;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testNameDirLocking;id_000001;[dfs.client.short.circuit.num=2091830036];java.lang.IllegalArgumentException;1288617740;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testStorageAlreadyLockedErrorMessage;id_000000;[dfs.client.short.circuit.num=892760688];java.lang.IllegalArgumentException;660490937;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDistributedFileSystem#testFileSystemCloseAll;id_000000;[dfs.client.short.circuit.num=1476784824];java.lang.IllegalArgumentException;1993479161;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby#testOtherNodeNotActive;id_000001;[dfs.client.short.circuit.num=390485366];java.lang.IllegalArgumentException;1027856174;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testExistingUser;id_000000;[dfs.client.short.circuit.num=79880314];java.lang.IllegalArgumentException;1493273825;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestSetTimes#testAtimeUpdate;id_000001;[dfs.client.short.circuit.num=1806393890];java.lang.IllegalArgumentException;218656192;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSeparateEditsDirLocking;id_000000;[dfs.client.short.circuit.num=621024995];java.lang.IllegalArgumentException;596570712;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeMXBean#testQueueLength;id_000001;[dfs.client.short.circuit.num=148505442];java.lang.IllegalArgumentException;1918122408;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.mover.TestMover#testMoverCli;id_000000;[dfs.client.short.circuit.num=665473674];java.lang.IllegalArgumentException;2141598229;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.tools.TestGetGroups#testExistingInterleavedWithNonExistentUsers;id_000001;[dfs.client.short.circuit.num=1037208354];java.lang.IllegalArgumentException;414448303;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNetworkTopologyServlet#testPrintTopologyNoDatanodesJsonFormat;id_000000;[dfs.client.short.circuit.num=1142258079];java.lang.IllegalArgumentException;2031527159;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestProtectedDirectories#testMoveToTrash;id_000001;[dfs.client.short.circuit.num=1642985421];java.lang.IllegalArgumentException;749109563;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSImage#testDigest;id_000000;[dfs.client.short.circuit.num=1971903110];java.lang.IllegalArgumentException;1052089574;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testECPolicyCommands;id_000001;[dfs.client.short.circuit.num=159111348];java.lang.IllegalArgumentException;1387971625;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFS#testStoragePolicy;id_000000;[dfs.client.short.circuit.num=968819065];java.lang.IllegalArgumentException;1604069239;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystemMBean#testFsEditLogMetrics;id_000001;[dfs.client.short.circuit.num=2059084055];java.lang.IllegalArgumentException;1803617659;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestNNHealthCheck#testNNHealthCheckWithSafemodeAsUnhealthy;id_000000;[dfs.client.short.circuit.num=967188450];java.lang.IllegalArgumentException;1294477398;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testExistingInterleavedWithNonExistentUsers;id_000001;[dfs.client.short.circuit.num=1539966073];java.lang.IllegalArgumentException;1988817976;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testMultipleExistingUsers;id_000001;[dfs.client.short.circuit.num=26209181];java.lang.IllegalArgumentException;1450034972;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testNoUserGiven;id_000001;[dfs.client.short.circuit.num=2062901199];java.lang.IllegalArgumentException;1970184631;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testNonExistentUser;id_000000;[dfs.client.short.circuit.num=897997626];java.lang.IllegalArgumentException;979377593;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestGetGroupsWithHA#testMultipleNonExistingUsers;id_000000;[dfs.client.short.circuit.num=1464024093];java.lang.IllegalArgumentException;1540532215;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestSetTimes#testGetBlockLocationsOnlyUsesReadLock;id_000000;[dfs.client.short.circuit.num=1239367640];java.lang.IllegalArgumentException;804192883;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.TestDFSMkdirs#testMkdirRpcNonCanonicalPath;id_000001;[dfs.client.short.circuit.num=770378448];java.lang.IllegalArgumentException;1875941627;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestInitializeSharedEdits#testDontOverWriteExistingDir;id_000001;[dfs.client.short.circuit.num=1627294014];java.lang.IllegalArgumentException;10624392;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.tools.TestGetGroups#testExistingUser;id_000000;[dfs.client.short.circuit.num=2007180804];java.lang.IllegalArgumentException;832396294;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.balancer.TestBalancer#testUnknownDatanodeSimple;id_000001;[dfs.client.short.circuit.num=634569690];java.lang.IllegalArgumentException;687220121;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[dfs.namenode.blocks.per.postponedblocks.rescan];java.lang.OutOfMemoryError;23;['org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testSafeModeIBR;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=2013252918];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode#testExtensionConfig;id_000001;[dfs.namenode.blocks.per.postponedblocks.rescan=891866378];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode.setupMockCluster(TestBlockManagerSafeMode.java:100), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testBasicReplication;id_000002;[dfs.namenode.blocks.per.postponedblocks.rescan=726695550];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testSafeModeIBRBeforeFirstFullBR;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1621734302];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations#testResolveReservedPath;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=2014745553];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.setupFileSystem(TestGetBlockLocations.java:135), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testResolveReservedPath(TestGetBlockLocations.java:56)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testSafemodeReplicationConf;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1165290445];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testSafemodeReplicationConf(TestFSNamesystem.java:233)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testReset;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1716674733];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testReset(TestFSNamesystem.java:185)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testParentDirectoryNameIsCorrect;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1861742818];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxComponentLength;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1762115788];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxDirItems;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1528391275];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode#testGetSafeModeTipsWithoutNumLiveDatanode;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=890210119];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode.setupMockCluster(TestBlockManagerSafeMode.java:100), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testOneOfTwoRacksDecommissioned;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=942910287];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxDirItemsRename;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1476666711];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxComponentLengthRename;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1806339742];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxDirItemsLimits;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1519815462];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog#testEditsForCancelOnTokenExpire;id_000001;[dfs.namenode.blocks.per.postponedblocks.rescan=1105179261];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog.testEditsForCancelOnTokenExpire(TestSecurityTokenEditLog.java:205)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations#testGetBlockLocationsRacingWithDelete;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=863147924];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.setupFileSystem(TestGetBlockLocations.java:135), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testGetBlockLocationsRacingWithDelete(TestGetBlockLocations.java:65)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations#testGetBlockLocationsRacingWithRename;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=633627913];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.setupFileSystem(TestGetBlockLocations.java:135), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testGetBlockLocationsRacingWithRename(TestGetBlockLocations.java:98)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testHAStateInNamespaceInfo;id_000000;[dfs.namenode.blocks.per.postponedblocks.rescan=1242430771];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testHAStateInNamespaceInfo(TestFSNamesystem.java:170)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testSafeModeWithProvidedStorageBR;id_000001;[dfs.namenode.blocks.per.postponedblocks.rescan=1587878588];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testDuringEditLogs;id_000001;[dfs.namenode.blocks.per.postponedblocks.rescan=2070967976];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testNoLimits;id_000001;[dfs.namenode.blocks.per.postponedblocks.rescan=1256201304];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testStartupSafemode;id_000001;[dfs.namenode.blocks.per.postponedblocks.rescan=892603477];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testStartupSafemode(TestFSNamesystem.java:108)]\n']
[dfs.namenode.safemode.extension];java.io.IOException;1;['org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode#testExtensionConfig;id_000000;[dfs.namenode.safemode.extension=1000];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1936538213 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode.setupMockCluster(TestBlockManagerSafeMode.java:100), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n']
[dfs.replication.max];java.io.IOException;5;['org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testBasicReplication;id_000000;[dfs.replication.max=854435799];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 854435799 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testSafeModeIBRBeforeFirstFullBR;id_000002;[dfs.replication.max=25061767];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 25061767 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode#testGetSafeModeTipsWithoutNumLiveDatanode;id_000001;[dfs.replication.max=112487267];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 112487267 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManagerSafeMode.setupMockCluster(TestBlockManagerSafeMode.java:100), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testOneOfTwoRacksDecommissioned;id_000002;[dfs.replication.max=1507602994];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1507602994 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager#testSafeModeWithProvidedStorageBR;id_000000;[dfs.replication.max=44338062];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 44338062 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.blockmanagement.TestBlockManager.setupMockCluster(TestBlockManager.java:170), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n']
[dfs.client.short.circuit.num];java.lang.OutOfMemoryError;1;['org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA#testRestoreFailedStorageNN1UpNN2Down;id_000000;[dfs.client.short.circuit.num=82716893];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n']
[dfs.namenode.name.dir.restore];java.lang.OutOfMemoryError;1;['org.apache.hadoop.hdfs.tools.TestDFSAdminWithHA#testRestoreFailedStorageNN1UpNN2Down;id_000002;[dfs.namenode.name.dir.restore=true];java.lang.OutOfMemoryError;Java heap space;[]\n']
[dfs.namenode.delegation.token.always-use];java.lang.IllegalArgumentException;2;['org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testSaveNamespaceBeforeShutdown;id_000001;[dfs.namenode.delegation.token.always-use=true];java.lang.IllegalArgumentException;374959108;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.ha.TestBootstrapStandby#testDownloadingLaterCheckpoint;id_000001;[dfs.namenode.delegation.token.always-use=true];java.lang.IllegalArgumentException;1510302197;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[hadoop.http.authentication.simple.anonymous.allowed];java.lang.OutOfMemoryError;13;['org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsOutOfSync1;id_000000;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsOutOfSync1;id_000001;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashBetweenSyncLogAndPersistPaxosData;id_000000;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashBetweenSyncLogAndPersistPaxosData;id_000001;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashBetweenSyncLogAndPersistPaxosData;id_000002;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsOutOfSync2;id_000001;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOutOfSyncAtBeginningOfSegment0;id_000000;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOutOfSyncAtBeginningOfSegment0;id_000001;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcTwoJNsError;id_000000;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testHttpServer;id_000000;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOneJNMissingSegments;id_000001;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcAfterJNRestart;id_000000;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOutOfSyncAtBeginningOfSegment1;id_000000;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n']
[hadoop.http.authentication.simple.anonymous.allowed];java.lang.IllegalStateException;2;['org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsOutOfSync1;id_000002;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.IllegalStateException;Insufficient configured threads: required=364182473 < max=106559661 for QueuedThreadPool[qtp1687516112]@649573d0{STARTED,8<=8<=106559661,i=8,r=-1,q=0}[ReservedThreadExecutor@ef342c2{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOneJNMissingSegments;id_000002;[hadoop.http.authentication.simple.anonymous.allowed=false];java.lang.IllegalStateException;Insufficient configured threads: required=490717800 < max=260093180 for QueuedThreadPool[qtp588054552]@230d0018{STARTED,8<=8<=260093180,i=8,r=-1,q=0}[ReservedThreadExecutor@67d85d5f{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)]\n']
[hadoop.http.selector.count];java.lang.OutOfMemoryError;73;['org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsInSync;id_000000;[hadoop.http.selector.count=2177718];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsInSync;id_000001;[hadoop.http.selector.count=936415947];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsInSync;id_000002;[hadoop.http.selector.count=785089590];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testFailToStartWithBadConfig;id_000000;[hadoop.http.selector.count=188290498];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testFailToStartWithBadConfig;id_000001;[hadoop.http.selector.count=1082430388];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testFailToStartWithBadConfig;id_000002;[hadoop.http.selector.count=501736871];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.TestMiniJournalCluster#testStartStop;id_000000;[hadoop.http.selector.count=1071160726];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.TestMiniJournalCluster#testStartStop;id_000001;[hadoop.http.selector.count=1869039488];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.TestMiniJournalCluster#testStartStop;id_000002;[hadoop.http.selector.count=2122924847];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeMXBean#testJournalNodeMXBean;id_000000;[hadoop.http.selector.count=1090007731];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeMXBean#testJournalNodeMXBean;id_000002;[hadoop.http.selector.count=1241172477];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalNodeStartupFailsCleanly;id_000000;[hadoop.http.selector.count=407305388];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalNodeStartupFailsCleanly;id_000002;[hadoop.http.selector.count=937146073];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectThreadCounts;id_000000;[hadoop.http.selector.count=1070360253];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectThreadCounts;id_000001;[hadoop.http.selector.count=1379068042];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOutOfSyncAtBeginningOfSegment2;id_000000;[hadoop.http.selector.count=543612040];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOutOfSyncAtBeginningOfSegment2;id_000001;[hadoop.http.selector.count=1339532116];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testNewerVersionOfSegmentWins;id_000000;[hadoop.http.selector.count=398724677];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testNewerVersionOfSegmentWins;id_000002;[hadoop.http.selector.count=1485161145];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalNodeSyncerNotStartWhenSyncEnabledIncorrectURI;id_000000;[hadoop.http.selector.count=1316009480];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalNodeSyncerNotStartWhenSyncEnabledIncorrectURI;id_000001;[hadoop.http.selector.count=2094924838];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalNodeSyncerNotStartWhenSyncEnabledIncorrectURI;id_000002;[hadoop.http.selector.count=470344668];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalCommonDirAcrossNameSpace;id_000000;[hadoop.http.selector.count=1078665022];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServerXFrame#testNameNodeXFrameOptionsDisabled;id_000000;[hadoop.http.selector.count=2097260256];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServerXFrame#testNameNodeXFrameOptionsDisabled;id_000001;[hadoop.http.selector.count=1434668649];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServerXFrame#testNameNodeXFrameOptionsDisabled;id_000002;[hadoop.http.selector.count=1996105731];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServerXFrame#testNameNodeXFrameOptionsEnabled;id_000000;[hadoop.http.selector.count=1184549620];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServerXFrame#testNameNodeXFrameOptionsEnabled;id_000001;[hadoop.http.selector.count=482270764];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsOutOfSync3;id_000000;[hadoop.http.selector.count=127729210];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsOutOfSync3;id_000002;[hadoop.http.selector.count=975363185];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testInProgressRecovery;id_000000;[hadoop.http.selector.count=306301147];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testInProgressRecovery;id_000001;[hadoop.http.selector.count=1974432973];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testInProgressRecovery;id_000002;[hadoop.http.selector.count=115414158];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testMissFinalizeAndNextStart;id_000000;[hadoop.http.selector.count=963485402];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testMissFinalizeAndNextStart;id_000002;[hadoop.http.selector.count=865228493];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsOutOfSync2;id_000000;[hadoop.http.selector.count=946468712];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsOutOfSync2;id_000002;[hadoop.http.selector.count=468618019];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOutOfSyncAtBeginningOfSegment0;id_000002;[hadoop.http.selector.count=703336296];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeRespectsBindHostKeys#testHttpsBindHostKey;id_000000;[hadoop.http.selector.count=2117491583];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testPurgeLogs;id_000000;[hadoop.http.selector.count=913275233];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testPurgeLogs;id_000002;[hadoop.http.selector.count=304820582];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectInputStreamsNotOnBoundary;id_000000;[hadoop.http.selector.count=540458327];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testReaderWhileAnotherWrites;id_000000;[hadoop.http.selector.count=224041038];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testReaderWhileAnotherWrites;id_000001;[hadoop.http.selector.count=1500015207];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcWithDurableTransactions;id_000000;[hadoop.http.selector.count=1788183625];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcWithDurableTransactions;id_000002;[hadoop.http.selector.count=1186035479];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcOneDeadJN;id_000000;[hadoop.http.selector.count=366533216];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcOneDeadJN;id_000001;[hadoop.http.selector.count=732947978];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcTwoJNsError;id_000001;[hadoop.http.selector.count=1292612511];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testFormat;id_000000;[hadoop.http.selector.count=1174286133];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcTwoDeadJNs;id_000000;[hadoop.http.selector.count=1073552471];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcTwoDeadJNs;id_000001;[hadoop.http.selector.count=2059138487];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcTwoDeadJNs;id_000002;[hadoop.http.selector.count=1819053301];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcWithoutDurableTransactions;id_000000;[hadoop.http.selector.count=1858471887];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcWithoutDurableTransactions;id_000001;[hadoop.http.selector.count=1630628450];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcWithoutDurableTransactions;id_000002;[hadoop.http.selector.count=197881602];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage#testImageUploadTimeout;id_000000;[hadoop.http.selector.count=1912991712];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage#testImageUploadTimeout;id_000001;[hadoop.http.selector.count=994711262];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournal;id_000000;[hadoop.http.selector.count=2058058967];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournal;id_000002;[hadoop.http.selector.count=262485311];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testHttpServer;id_000002;[hadoop.http.selector.count=1679190917];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestEpochsAreUnique#testSingleThreaded;id_000000;[hadoop.http.selector.count=1159563002];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestEpochsAreUnique#testSingleThreaded;id_000002;[hadoop.http.selector.count=1302861043];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOneJNMissingSegments;id_000000;[hadoop.http.selector.count=415236148];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcAfterJNRestart;id_000001;[hadoop.http.selector.count=1033253795];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testSelectViaRpcAfterJNRestart;id_000002;[hadoop.http.selector.count=1575048049];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashAtBeginningOfSegment;id_000000;[hadoop.http.selector.count=1474714004];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashAtBeginningOfSegment;id_000001;[hadoop.http.selector.count=120378603];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testCrashAtBeginningOfSegment;id_000002;[hadoop.http.selector.count=1539368840];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalNodeSyncerNotStartWhenSyncDisabled;id_000000;[hadoop.http.selector.count=37929133];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalNodeSyncerNotStartWhenSyncDisabled;id_000002;[hadoop.http.selector.count=1733534583];java.lang.OutOfMemoryError;Java heap space;[]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeRespectsBindHostKeys#testRpcBindHostKey;id_000000;[hadoop.http.selector.count=255413321];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNodeRespectsBindHostKeys#testRpcBindHostKey;id_000002;[hadoop.http.selector.count=188339101];java.lang.OutOfMemoryError;Java heap space;[]\n']
[hadoop.http.selector.count];java.lang.IllegalStateException;4;['org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsInSync;id_000003;[hadoop.http.selector.count=425707432];java.lang.IllegalStateException;Insufficient configured threads: required=58731061 < max=55154296 for QueuedThreadPool[qtp593210287]@235babaf{STARTED,8<=8<=55154296,i=8,r=-1,q=0}[ReservedThreadExecutor@5e8372b8{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.server.AbstractConnector.doStart(AbstractConnector.java:320), org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:81)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOutOfSyncAtBeginningOfSegment2;id_000002;[hadoop.http.selector.count=147328005];java.lang.IllegalStateException;Insufficient configured threads: required=402389039 < max=370237045 for QueuedThreadPool[qtp673063799]@281e2377{STARTED,8<=8<=370237045,i=8,r=-1,q=0}[ReservedThreadExecutor@a28296e{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testChangeWritersLogsOutOfSync3;id_000003;[hadoop.http.selector.count=393627251];java.lang.IllegalStateException;Insufficient configured threads: required=451493488 < max=135593087 for QueuedThreadPool[qtp177586678]@a95c1f6{STARTED,8<=8<=135593087,i=8,r=-1,q=0}[ReservedThreadExecutor@6277551c{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)]\n', 'org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManager#testOutOfSyncAtBeginningOfSegment1;id_000002;[hadoop.http.selector.count=154245674];java.lang.IllegalStateException;Insufficient configured threads: required=236537656 < max=111912915 for QueuedThreadPool[qtp369405199]@1604ad0f{STARTED,8<=8<=111912915,i=8,r=-1,q=0}[ReservedThreadExecutor@64d347b0{s=0/2,p=0}];[org.eclipse.jetty.util.thread.ThreadPoolBudget.check(ThreadPoolBudget.java:165), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseTo(ThreadPoolBudget.java:141), org.eclipse.jetty.util.thread.ThreadPoolBudget.leaseFrom(ThreadPoolBudget.java:191), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:255), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)]\n']
[dfs.namenode.resource.checked.volumes.minimum];java.lang.NumberFormatException;3;['org.apache.hadoop.hdfs.server.namenode.TestNameNodeResourceChecker#testChecking2NameDirsOnOneVolume;id_000000;[dfs.namenode.resource.checked.volumes.minimum=72554g];java.lang.NumberFormatException;For input string: "72554g";[java.lang.NumberFormatException.forInputString(NumberFormatException.java:65), java.lang.Integer.parseInt(Integer.java:652), java.lang.Integer.parseInt(Integer.java:770), org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1611), org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker.<init>(NameNodeResourceChecker.java:140)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeResourceChecker#testCheckingExtraVolumes;id_000000;[dfs.namenode.resource.checked.volumes.minimum=6344p];java.lang.NumberFormatException;For input string: "6344p";[java.lang.NumberFormatException.forInputString(NumberFormatException.java:65), java.lang.Integer.parseInt(Integer.java:652), java.lang.Integer.parseInt(Integer.java:770), org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1611), org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker.<init>(NameNodeResourceChecker.java:140)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeResourceChecker#testCheckAvailability;id_000000;[dfs.namenode.resource.checked.volumes.minimum=73569m];java.lang.NumberFormatException;For input string: "73569m";[java.lang.NumberFormatException.forInputString(NumberFormatException.java:65), java.lang.Integer.parseInt(Integer.java:652), java.lang.Integer.parseInt(Integer.java:770), org.apache.hadoop.conf.Configuration.getInt(Configuration.java:1611), org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker.<init>(NameNodeResourceChecker.java:140)]\n']
[dfs.client.short.circuit.num];java.io.EOFException;1;['org.apache.hadoop.fs.contract.hdfs.TestHDFSContractSeek#testSeekPastEndOfFileThenReseekAndRead;id_000000;[dfs.client.short.circuit.num=1767534088];java.io.EOFException;Cannot seek after EOF;[org.apache.hadoop.hdfs.DFSInputStream.seek(DFSInputStream.java:1598), org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:71), org.apache.hadoop.fs.contract.AbstractContractSeekTest.testSeekPastEndOfFileThenReseekAndRead(AbstractContractSeekTest.java:242), org.apache.hadoop.fs.contract.hdfs.TestHDFSContractSeek.testSeekPastEndOfFileThenReseekAndRead$$CONFUZZ(TestHDFSContractSeek.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n']
[dfs.namenode.stale.datanode.interval];java.lang.IllegalArgumentException;19;["org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testPendingRecoveryTasks;id_000000;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.verifyPendingRecoveryTasks(TestDatanodeManager.java:951)]\n", "org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testgoodScript;id_000000;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n", "org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoad;id_000001;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad(TestDatanodeManager.java:540)]\n", 'org.apache.hadoop.hdfs.server.blockmanagement.TestHostFileManager#testIncludeExcludeLists;id_000000;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]\n', "org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testNumVersionsCorrectAfterReregister;id_000001;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testNumVersionsCorrectAfterReregister(TestDatanodeManager.java:119)]\n", "org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageType;id_000001;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageType(TestDatanodeManager.java:688)]\n", 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageType;id_000002;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageType(TestDatanodeManager.java:688)]\n', "org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testRemoveIncludedNode;id_000000;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testRemoveIncludedNode(TestDatanodeManager.java:858)]\n", 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testRemoveIncludedNode;id_000001;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testRemoveIncludedNode(TestDatanodeManager.java:858)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageTypeAndLoad;id_000000;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad(TestDatanodeManager.java:774)]\n', "org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageTypeAndLoad;id_000001;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad(TestDatanodeManager.java:774)]\n", 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageTypeAndLoad;id_000002;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageTypeAndLoad(TestDatanodeManager.java:774)]\n', "org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoadWithNodesOfSameDistance;id_000003;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoadWithNodesOfSameDistance(TestDatanodeManager.java:621)]\n", 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testWithProvidedTypes;id_000001;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testSortLocatedBlocks;id_000001;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n', "org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testSortLocatedBlocks;id_000002;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n", 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testBadScript;id_000002;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testBadScript;id_000003;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n', "org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testBadScript;id_000004;[dfs.namenode.stale.datanode.interval=0];java.lang.IllegalArgumentException;dfs.namenode.stale.datanode.interval = '0' is invalid. It should be a positive non-zero value.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.getStaleIntervalFromConf(DatanodeManager.java:418), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:357), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n"]
[dfs.namenode.full.block.report.lease.length.ms];java.lang.IllegalArgumentException;13;['org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testPendingRecoveryTasks;id_000002;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.verifyPendingRecoveryTasks(TestDatanodeManager.java:951)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testPendingRecoveryTasks;id_000003;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.verifyPendingRecoveryTasks(TestDatanodeManager.java:951)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testgoodScript;id_000001;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testgoodScript;id_000002;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoad;id_000000;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad(TestDatanodeManager.java:540)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoad;id_000002;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad(TestDatanodeManager.java:540)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testNumVersionsCorrectAfterReregister;id_000000;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testNumVersionsCorrectAfterReregister(TestDatanodeManager.java:119)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testNumVersionsCorrectAfterReregister;id_000002;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testNumVersionsCorrectAfterReregister(TestDatanodeManager.java:119)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoadWithNodesOfSameDistance;id_000001;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoadWithNodesOfSameDistance(TestDatanodeManager.java:621)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoadWithNodesOfSameDistance;id_000002;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoadWithNodesOfSameDistance(TestDatanodeManager.java:621)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testWithProvidedTypes;id_000000;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testWithProvidedTypes;id_000002;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testSortLocatedBlocks;id_000000;[dfs.namenode.full.block.report.lease.length.ms=0];java.lang.IllegalArgumentException;Cannot set full block report lease expiry period to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:174), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:376)]\n']
[25,5,dfs.namenode.top.enabled,dfs.namenode.top.num.users,dfs.namenode.top.window.num.buckets,dfs.namenode.top.windows.minutes];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics#testPresence;id_000000;[dfs.namenode.top.enabled=true, dfs.namenode.top.window.num.buckets=1347575913, dfs.namenode.top.windows.minutes=1,5,25, dfs.namenode.top.num.users=890664029];java.lang.IllegalArgumentException;the minimum size of a bucket is 1 ms;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager.<init>(RollingWindowManager.java:229), org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.<init>(TopMetrics.java:93), org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics.testPresence(TestTopMetrics.java:41), org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics.testPresence$$CONFUZZ(TestTopMetrics.java)]\n']
[81885762,930303731,dfs.namenode.top.enabled,dfs.namenode.top.num.users,dfs.namenode.top.window.num.buckets,dfs.namenode.top.windows.minutes];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics#testPresence;id_000001;[dfs.namenode.top.enabled=true, dfs.namenode.top.window.num.buckets=1098280918, dfs.namenode.top.windows.minutes=1458232,81885762,930303731, dfs.namenode.top.num.users=2101701028];java.lang.IllegalArgumentException;Out of range: 87493920000;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:205), org.apache.hadoop.thirdparty.com.google.common.primitives.Ints.checkedCast(Ints.java:89), org.apache.hadoop.hdfs.server.namenode.top.TopConf.<init>(TopConf.java:56), org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics.testPresence(TestTopMetrics.java:40), org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics.testPresence$$CONFUZZ(TestTopMetrics.java)]\n']
[66,66,dfs.namenode.top.enabled,dfs.namenode.top.num.users,dfs.namenode.top.window.num.buckets,dfs.namenode.top.windows.minutes];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics#testPresence;id_000002;[dfs.namenode.top.enabled=true, dfs.namenode.top.window.num.buckets=85852220, dfs.namenode.top.windows.minutes=0,66,66, dfs.namenode.top.num.users=44039961];java.lang.IllegalArgumentException;minimum reporting period is 1 min!;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.top.TopConf.<init>(TopConf.java:60), org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics.testPresence(TestTopMetrics.java:40), org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics.testPresence$$CONFUZZ(TestTopMetrics.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n']
[210,971,dfs.namenode.top.enabled,dfs.namenode.top.num.users,dfs.namenode.top.window.num.buckets,dfs.namenode.top.windows.minutes];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics#testPresence;id_000006;[dfs.namenode.top.enabled=true, dfs.namenode.top.window.num.buckets=109122947, dfs.namenode.top.windows.minutes=19147,210,971, dfs.namenode.top.num.users=887767056];java.lang.IllegalArgumentException;window size must be a multiplication of number of buckets;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager.<init>(RollingWindowManager.java:232), org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics.<init>(TopMetrics.java:93), org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics.testPresence(TestTopMetrics.java:41), org.apache.hadoop.hdfs.server.namenode.metrics.TestTopMetrics.testPresence$$CONFUZZ(TestTopMetrics.java)]\n']
[dfs.datanode.simulateddatastorage.capacity];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDatasetWithMultipleStorages#testInjectionNonEmpty;id_000000;[dfs.datanode.simulateddatastorage.capacity=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]\n']
[dfs.datanode.simulateddatastorage.capacity];java.lang.NoClassDefFoundError;1;['org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDatasetWithMultipleStorages#testInjectionNonEmpty;id_000001;[dfs.datanode.simulateddatastorage.capacity=0];java.lang.NoClassDefFoundError;Could not initialize class org.apache.hadoop.hdfs.server.common.Util;[org.apache.hadoop.hdfs.server.datanode.ProfilingFileIoEvents.<init>(ProfilingFileIoEvents.java:52), org.apache.hadoop.hdfs.server.datanode.FileIoProvider.<init>(FileIoProvider.java:96), org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset.<init>(SimulatedFSDataset.java:712), org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset.<init>(SimulatedFSDataset.java:694), org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset.getSimulatedFSDataset(TestSimulatedFSDataset.java:378)]\n']
[dfs.ha.tail-edits.in-progress,dfs.journalnode.edit-cache-size.bytes,dfs.journalnode.edits.dir.perm];java.lang.IllegalArgumentException;2;['org.apache.hadoop.hdfs.qjournal.server.TestJournal#testFormatNonEmptyStorageDirectoriesWhenforceOptionIsTrue;id_000000;[dfs.ha.tail-edits.in-progress=true, dfs.journalnode.edits.dir.perm=374193160, dfs.journalnode.edit-cache-size.bytes=469559164];java.lang.IllegalArgumentException;374193160;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:70), org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:163)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournal#testJournalLocking;id_000000;[dfs.ha.tail-edits.in-progress=true, dfs.journalnode.edits.dir.perm=747587575, dfs.journalnode.edit-cache-size.bytes=1210850897];java.lang.IllegalArgumentException;747587575;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:70), org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:163)]\n']
[dfs.provided.aliasmap.inmemory.dnrpc-address,dfs.provided.aliasmap.inmemory.leveldb.dir,dfs.provided.aliasmap.inmemory.server.log,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.rpc.socket.factory.class.default,hadoop.security.auth_to_local,hadoop.security.auth_to_local.mechanism,hadoop.security.authentication,hadoop.security.authorization,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.group.mapping,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.security.token.service.use_ip,hadoop.user.group.static.mapping.overrides,ipc.client.async.calls.max,ipc.client.connect.timeout,ipc.client.connection.idle-scan-interval.ms,ipc.client.connection.maxidletime,ipc.client.idlethreshold,ipc.client.low-latency,ipc.client.tcpnodelay,ipc.ping.interval,ipc.server.handler.queue.size,ipc.server.max.response.size,ipc.server.read.connection-queue.size,ipc.server.read.threadpool.size,ipc.server.reuseaddr,ipc.server.tcpnodelay,rpc.engine.org.apache.hadoop.hdfs.protocolPB.AliasMapProtocolPB];, ipc.9876.faircallqueue.priority-levels=735472618, ipc.maximum.data.length=836224488, ipc.client.connect.max.retries.on.sasl=1683242988, ipc.client.bind.wildcard.addr=true, hadoop.security.groups.negative-cache.secs=2066558219, ipc.client.kill.max=1942055979, ipc.server.listen.queue.size=1558533416, ipc.9876.backoff.enable=false, ipc.client.connect.max.retries.on.timeouts=1966229240, ipc.9876.scheduler.priority.levels=1839954462, rpc.metrics.timeunit=MILLISECONDS, hadoop.security.dns.log-slow-lookups.enabled=false, ipc.client.fallback-to-simple-auth-allowed=true, ipc.client.connect.max.retries=219192333, ipc.9876.callqueue.overflow.trigger.failover=false, ipc.client.ping=true, ipc.server.log.slow.rpc=false, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, ipc.client.connect.retry.interval=949302373, ipc.server.purge.interval=1062068888, ipc.server.max.connections=1477207133, ipc.maximum.response.length=2045212679];1;['org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient#testNonExistentBlock;id_000000;[hadoop.security.groups.cache.secs=1825051803, ipc.client.async.calls.max=966811415, dfs.provided.aliasmap.inmemory.dnrpc-address=localhost:9876, ipc.server.tcpnodelay=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], ipc.client.low-latency=false, ipc.client.idlethreshold=2109562094, ipc.ping.interval=2119421782, ipc.server.max.response.size=1484550398, ipc.client.connection.idle-scan-interval.ms=972044008, hadoop.kerberos.keytab.login.autorenewal.enabled=true, ipc.server.handler.queue.size=643527860, hadoop.security.authorization=false, ipc.client.connection.maxidletime=19724758, dfs.provided.aliasmap.inmemory.leveldb.dir=/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task125/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/test18096113492692409640, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, ipc.server.read.connection-queue.size=72585844, ipc.client.tcpnodelay=true, hadoop.security.dns.log-slow-lookups.threshold.ms=1947761334, ipc.client.connect.timeout=1801546218, hadoop.security.authentication=simple, hadoop.security.groups.cache.background.reload.threads=1535503051, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1116798557, ipc.server.reuseaddr=false, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, rpc.engine.org.apache.hadoop.hdfs.protocolPB.AliasMapProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, dfs.provided.aliasmap.inmemory.server.log=false, hadoop.security.groups.cache.warn.after.ms=1530080246, ipc.server.read.threadpool.size=1339272277, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.background.reload=true, hadoop.user.group.static.mapping.overrides=dr.who=;, ipc.9876.faircallqueue.priority-levels=735472618, ipc.maximum.data.length=836224488, ipc.client.connect.max.retries.on.sasl=1683242988, ipc.client.bind.wildcard.addr=true, hadoop.security.groups.negative-cache.secs=2066558219, ipc.client.kill.max=1942055979, ipc.server.listen.queue.size=1558533416, ipc.9876.backoff.enable=false, ipc.client.connect.max.retries.on.timeouts=1966229240, ipc.9876.scheduler.priority.levels=1839954462, rpc.metrics.timeunit=MILLISECONDS, hadoop.security.dns.log-slow-lookups.enabled=false, ipc.client.fallback-to-simple-auth-allowed=true, ipc.client.connect.max.retries=219192333, ipc.9876.callqueue.overflow.trigger.failover=false, ipc.client.ping=true, ipc.server.log.slow.rpc=false, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, ipc.client.connect.retry.interval=949302373, ipc.server.purge.interval=1062068888, ipc.server.max.connections=1477207133, ipc.maximum.response.length=2045212679];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.RPC$Server.<init>(RPC.java:1062), org.apache.hadoop.ipc.ProtobufRpcEngine2$Server.<init>(ProtobufRpcEngine2.java:468), org.apache.hadoop.ipc.ProtobufRpcEngine2.getServer(ProtobufRpcEngine2.java:371)]\n']
[dfs.provided.aliasmap.inmemory.dnrpc-address,dfs.provided.aliasmap.inmemory.leveldb.dir,dfs.provided.aliasmap.inmemory.server.log,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.rpc.socket.factory.class.default,hadoop.security.auth_to_local,hadoop.security.auth_to_local.mechanism,hadoop.security.authentication,hadoop.security.authorization,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.group.mapping,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.security.token.service.use_ip,hadoop.user.group.static.mapping.overrides,ipc.client.async.calls.max,ipc.client.connect.timeout,ipc.client.connection.idle-scan-interval.ms,ipc.client.connection.maxidletime,ipc.client.idlethreshold,ipc.client.low-latency,ipc.client.tcpnodelay,ipc.ping.interval,ipc.server.handler.queue.size,ipc.server.max.response.size,ipc.server.read.connection-queue.size,ipc.server.read.threadpool.size,ipc.server.reuseaddr,ipc.server.tcpnodelay,rpc.engine.org.apache.hadoop.hdfs.protocolPB.AliasMapProtocolPB];, ipc.9876.faircallqueue.priority-levels=1112614125, ipc.maximum.data.length=139597753, ipc.client.connect.max.retries.on.sasl=1605999801, ipc.client.bind.wildcard.addr=true, hadoop.security.groups.negative-cache.secs=1836655729, ipc.client.kill.max=1920202803, ipc.server.listen.queue.size=1275544803, ipc.9876.backoff.enable=false, ipc.client.connect.max.retries.on.timeouts=1235705548, ipc.9876.scheduler.priority.levels=374287368, rpc.metrics.timeunit=MILLISECONDS, hadoop.security.dns.log-slow-lookups.enabled=false, ipc.client.fallback-to-simple-auth-allowed=true, ipc.client.connect.max.retries=1756336613, ipc.9876.callqueue.overflow.trigger.failover=false, ipc.client.ping=false, ipc.server.log.slow.rpc=true, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, ipc.client.connect.retry.interval=1803247345, ipc.server.purge.interval=1553737518, ipc.server.max.connections=103797173, ipc.maximum.response.length=1087543515];1;['org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TestInMemoryLevelDBAliasMapClient#testNonExistentBlock;id_000001;[hadoop.security.groups.cache.secs=484608846, ipc.client.async.calls.max=881706996, dfs.provided.aliasmap.inmemory.dnrpc-address=localhost:9876, ipc.server.tcpnodelay=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], ipc.client.low-latency=true, ipc.client.idlethreshold=1891085940, ipc.ping.interval=215402588, ipc.server.max.response.size=241631239, ipc.client.connection.idle-scan-interval.ms=709311767, hadoop.kerberos.keytab.login.autorenewal.enabled=true, ipc.server.handler.queue.size=43898155, hadoop.security.authorization=true, ipc.client.connection.maxidletime=70860160, dfs.provided.aliasmap.inmemory.leveldb.dir=/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task125/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/test4170146894297306573, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, ipc.server.read.connection-queue.size=1105118209, ipc.client.tcpnodelay=true, hadoop.security.dns.log-slow-lookups.threshold.ms=1773598540, ipc.client.connect.timeout=1797023503, hadoop.security.authentication=simple, hadoop.security.groups.cache.background.reload.threads=903784191, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1404381820, ipc.server.reuseaddr=true, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, rpc.engine.org.apache.hadoop.hdfs.protocolPB.AliasMapProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, dfs.provided.aliasmap.inmemory.server.log=false, hadoop.security.groups.cache.warn.after.ms=1409042331, ipc.server.read.threadpool.size=1734963306, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.background.reload=true, hadoop.user.group.static.mapping.overrides=dr.who=;, ipc.9876.faircallqueue.priority-levels=1112614125, ipc.maximum.data.length=139597753, ipc.client.connect.max.retries.on.sasl=1605999801, ipc.client.bind.wildcard.addr=true, hadoop.security.groups.negative-cache.secs=1836655729, ipc.client.kill.max=1920202803, ipc.server.listen.queue.size=1275544803, ipc.9876.backoff.enable=false, ipc.client.connect.max.retries.on.timeouts=1235705548, ipc.9876.scheduler.priority.levels=374287368, rpc.metrics.timeunit=MILLISECONDS, hadoop.security.dns.log-slow-lookups.enabled=false, ipc.client.fallback-to-simple-auth-allowed=true, ipc.client.connect.max.retries=1756336613, ipc.9876.callqueue.overflow.trigger.failover=false, ipc.client.ping=false, ipc.server.log.slow.rpc=true, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, ipc.client.connect.retry.interval=1803247345, ipc.server.purge.interval=1553737518, ipc.server.max.connections=103797173, ipc.maximum.response.length=1087543515];java.net.BindException;Address already in use;[sun.nio.ch.Net.bind0(Native Method), sun.nio.ch.Net.bind(Net.java:459), sun.nio.ch.Net.bind(Net.java:448), sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:227), sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:80)]\n']
[fs.permissions.umask-mode];java.lang.IllegalArgumentException;7;['org.apache.hadoop.hdfs.TestPread#testPreadLocalFS;id_000000;[fs.permissions.umask-mode=742065250];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 742065250 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)]\n', 'org.apache.hadoop.hdfs.TestSeekBug#testSeekBugLocalFS;id_000000;[fs.permissions.umask-mode=1968623071];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1968623071 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:644), org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700), org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672), org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)]\n', 'org.apache.hadoop.hdfs.TestDFSUtil#testGetPassword;id_000000;[fs.permissions.umask-mode=1076030209];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1076030209 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1196), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1064), org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1052)]\n', 'org.apache.hadoop.hdfs.TestLease#testFactory;id_000000;[fs.permissions.umask-mode=1660029078];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1660029078 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.TestLease$1.run(TestLease.java:407), org.apache.hadoop.hdfs.TestLease$1.run(TestLease.java:404)]\n', 'org.apache.hadoop.hdfs.TestDeadNodeDetection#testDeadNodeDetectorThreadsShutdown;id_000000;[fs.permissions.umask-mode=1063149675];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1063149675 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:202)]\n', 'org.apache.hadoop.hdfs.client.impl.TestBlockReaderIoProvider#testSlowShortCircuitReadsIsRecorded;id_000000;[fs.permissions.umask-mode=1714036888];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1714036888 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.client.impl.TestBlockReaderIoProvider.testSlowShortCircuitReadsIsRecorded(TestBlockReaderIoProvider.java:53), org.apache.hadoop.hdfs.client.impl.TestBlockReaderIoProvider.testSlowShortCircuitReadsIsRecorded$$CONFUZZ(TestBlockReaderIoProvider.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n', 'org.apache.hadoop.hdfs.TestDeadNodeDetection#testCloseDeadNodeDetector;id_000000;[fs.permissions.umask-mode=1166318651];java.lang.IllegalArgumentException;Unable to parse configuration fs.permissions.umask-mode with value 1166318651 as octal or symbolic umask.;[org.apache.hadoop.fs.permission.FsPermission.getUMask(FsPermission.java:324), org.apache.hadoop.hdfs.client.impl.DfsClientConf.<init>(DfsClientConf.java:250), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:324), org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308), org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:202)]\n']
[dfs.namenode.storage.dir.perm];java.lang.IllegalArgumentException;11;['org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testManyLogsWithCorruptInprogress;id_000000;[dfs.namenode.storage.dir.perm=1518100860];java.lang.IllegalArgumentException;1518100860;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testFinalizeErrorReportedToNNStorage;id_000000;[dfs.namenode.storage.dir.perm=1485283935];java.lang.IllegalArgumentException;1485283935;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testReadFromMiddleOfEditLog;id_000000;[dfs.namenode.storage.dir.perm=1905524307];java.lang.IllegalArgumentException;1905524307;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testInprogressRecovery;id_000000;[dfs.namenode.storage.dir.perm=1658975848];java.lang.IllegalArgumentException;1658975848;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testManyLogsWithGaps;id_000000;[dfs.namenode.storage.dir.perm=1168435732];java.lang.IllegalArgumentException;1168435732;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testInprogressRecoveryAll;id_000000;[dfs.namenode.storage.dir.perm=1503922812];java.lang.IllegalArgumentException;1503922812;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testReadFromStream;id_000000;[dfs.namenode.storage.dir.perm=481880306];java.lang.IllegalArgumentException;481880306;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testAskForTransactionsMidfile;id_000000;[dfs.namenode.storage.dir.perm=391057802];java.lang.IllegalArgumentException;391057802;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testWriteTransactionIdHandlesIOE;id_000000;[dfs.namenode.storage.dir.perm=711252216];java.lang.IllegalArgumentException;711252216;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testExcludeInProgressStreams;id_000000;[dfs.namenode.storage.dir.perm=1932490275];java.lang.IllegalArgumentException;1932490275;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileJournalManager#testNormalOperation;id_000000;[dfs.namenode.storage.dir.perm=1688321784];java.lang.IllegalArgumentException;1688321784;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:339), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[dfs.webhdfs.oauth2.enabled];java.security.GeneralSecurityException;7;["org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testSimpleProxyAuthParamsInUrl;id_000000;[dfs.webhdfs.oauth2.enabled=true];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.<init>(SSLConnectionConfigurator.java:50), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:135)]\n", "org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testSimpleAuthParamsInUrl;id_000000;[dfs.webhdfs.oauth2.enabled=true];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.<init>(SSLConnectionConfigurator.java:50), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:135)]\n", "org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testSecureAuthParamsInUrl;id_000000;[dfs.webhdfs.oauth2.enabled=true];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.<init>(SSLConnectionConfigurator.java:50), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:135)]\n", "org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testBatchedListingUrl;id_000000;[dfs.webhdfs.oauth2.enabled=true];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.<init>(SSLConnectionConfigurator.java:50), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:135)]\n", "org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testCheckAccessUrl;id_000000;[dfs.webhdfs.oauth2.enabled=true];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.<init>(SSLConnectionConfigurator.java:50), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:135)]\n", "org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testWebHdfsUrlEncoding;id_000001;[dfs.webhdfs.oauth2.enabled=true];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.<init>(SSLConnectionConfigurator.java:50), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:135)]\n", "org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testSecureProxyAuthParamsInUrl;id_000000;[dfs.webhdfs.oauth2.enabled=true];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.<init>(SSLConnectionConfigurator.java:50), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:135)]\n"]
[dfs.webhdfs.oauth2.enabled];java.lang.IllegalArgumentException;7;['org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testSimpleProxyAuthParamsInUrl;id_000001;[dfs.webhdfs.oauth2.enabled=true];java.lang.IllegalArgumentException;No value for dfs.webhdfs.oauth2.access.token.provider found in conf file.;[org.apache.hadoop.hdfs.web.oauth2.Utils.notNull(Utils.java:37), org.apache.hadoop.hdfs.web.oauth2.OAuth2ConnectionConfigurator.<init>(OAuth2ConnectionConfigurator.java:55), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:138), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:242), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testSimpleAuthParamsInUrl;id_000001;[dfs.webhdfs.oauth2.enabled=true];java.lang.IllegalArgumentException;No value for dfs.webhdfs.oauth2.access.token.provider found in conf file.;[org.apache.hadoop.hdfs.web.oauth2.Utils.notNull(Utils.java:37), org.apache.hadoop.hdfs.web.oauth2.OAuth2ConnectionConfigurator.<init>(OAuth2ConnectionConfigurator.java:55), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:138), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:242), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testSecureAuthParamsInUrl;id_000002;[dfs.webhdfs.oauth2.enabled=true];java.lang.IllegalArgumentException;No value for dfs.webhdfs.oauth2.access.token.provider found in conf file.;[org.apache.hadoop.hdfs.web.oauth2.Utils.notNull(Utils.java:37), org.apache.hadoop.hdfs.web.oauth2.OAuth2ConnectionConfigurator.<init>(OAuth2ConnectionConfigurator.java:55), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:138), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:242), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testBatchedListingUrl;id_000001;[dfs.webhdfs.oauth2.enabled=true];java.lang.IllegalArgumentException;No value for dfs.webhdfs.oauth2.access.token.provider found in conf file.;[org.apache.hadoop.hdfs.web.oauth2.Utils.notNull(Utils.java:37), org.apache.hadoop.hdfs.web.oauth2.OAuth2ConnectionConfigurator.<init>(OAuth2ConnectionConfigurator.java:55), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:138), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:242), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testCheckAccessUrl;id_000002;[dfs.webhdfs.oauth2.enabled=true];java.lang.IllegalArgumentException;No value for dfs.webhdfs.oauth2.access.token.provider found in conf file.;[org.apache.hadoop.hdfs.web.oauth2.Utils.notNull(Utils.java:37), org.apache.hadoop.hdfs.web.oauth2.OAuth2ConnectionConfigurator.<init>(OAuth2ConnectionConfigurator.java:55), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:138), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:242), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testWebHdfsUrlEncoding;id_000000;[dfs.webhdfs.oauth2.enabled=true];java.lang.IllegalArgumentException;No value for dfs.webhdfs.oauth2.access.token.provider found in conf file.;[org.apache.hadoop.hdfs.web.oauth2.Utils.notNull(Utils.java:37), org.apache.hadoop.hdfs.web.oauth2.OAuth2ConnectionConfigurator.<init>(OAuth2ConnectionConfigurator.java:55), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:138), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:242), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]\n', 'org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testSecureProxyAuthParamsInUrl;id_000001;[dfs.webhdfs.oauth2.enabled=true];java.lang.IllegalArgumentException;No value for dfs.webhdfs.oauth2.access.token.provider found in conf file.;[org.apache.hadoop.hdfs.web.oauth2.Utils.notNull(Utils.java:37), org.apache.hadoop.hdfs.web.oauth2.OAuth2ConnectionConfigurator.<init>(OAuth2ConnectionConfigurator.java:55), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:138), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:242), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]\n']
[ipc.server.read.threadpool.size];java.lang.OutOfMemoryError;1;['org.apache.hadoop.hdfs.TestDFSClientRetries#testClientDNProtocolTimeout;id_000000;[ipc.server.read.threadpool.size=2084233394];java.lang.OutOfMemoryError;Java heap space;[org.apache.hadoop.ipc.Server$Listener.<init>(Server.java:1240), org.apache.hadoop.ipc.Server.<init>(Server.java:3127), org.apache.hadoop.ipc.Server.<init>(Server.java:3046), org.apache.hadoop.hdfs.TestDFSClientRetries$TestServer.<init>(TestDFSClientRetries.java:127), org.apache.hadoop.hdfs.TestDFSClientRetries$TestServer.<init>(TestDFSClientRetries.java:120)]\n']
[dfs.datanode.outliers.report.interval,dfs.namenode.avoid.write.stale.datanode,dfs.namenode.full.block.report.lease.length.ms,dfs.namenode.heartbeat.recheck-interval,dfs.namenode.max.full.block.report.leases,dfs.namenode.stale.datanode.minimum.interval,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.security.groups.negative-cache.secs,net.topology.script.number.args];org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException;1;['org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testgoodScript;id_000003;[net.topology.script.number.args=0, hadoop.security.groups.cache.secs=11898502, dfs.namenode.stale.datanode.minimum.interval=486549180, dfs.namenode.avoid.write.stale.datanode=false, hadoop.security.groups.negative-cache.secs=0, dfs.namenode.max.full.block.report.leases=581959663, dfs.namenode.heartbeat.recheck-interval=0, dfs.datanode.outliers.report.interval=827969310d, dfs.namenode.full.block.report.lease.length.ms=245, hadoop.security.groups.cache.warn.after.ms=1677721600];org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException;Unresolved topology mapping for host null;[org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.resolveNetworkLocation(DatanodeManager.java:1030), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:1250), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:407), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testgoodScript(TestDatanodeManager.java:322), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testgoodScript$$CONFUZZ(TestDatanodeManager.java)]\n']
[net.topology.script.number.args];org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException;1;['org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoad;id_000003;[net.topology.script.number.args=0];org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException;Unresolved topology mapping for host null;[org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.resolveNetworkLocation(DatanodeManager.java:1030), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:1250), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad(TestDatanodeManager.java:553), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoad$$CONFUZZ(TestDatanodeManager.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n']
[dfs.datanode.volumes.replica-add.threadpool.size];org.apache.hadoop.fs.FileAlreadyExistsException;1;['org.apache.hadoop.fs.TestSWebHdfsFileContextMainOperations#testCreateFlagCreateExistingFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.fs.FileAlreadyExistsException;File already exists: swebhdfs://localhost:34051/tmp/TestSWebHdfsFileContextMainOperations/test/testCreateFlagCreateExistingFile. Append or overwrite option must be specified in [CREATE];[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:180), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]\n']
[hadoop.security.auth_to_local,hadoop.security.auth_to_local.mechanism,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.secs,hadoop.ssl.client.conf,hadoop.ssl.enabled.protocols,hadoop.ssl.require.client.cert,hadoop.user.group.static.mapping.overrides];, hadoop.ssl.hostname.verifier=DEFAULT, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=2074563951, ssl.client.truststore.location=, ssl.client.keystore.type=jks, hadoop.security.dns.log-slow-lookups.threshold.ms=667311967, dfs.namenode.shared.edits.dir=qjournal://jn1:8020;1;['org.apache.hadoop.hdfs.tools.TestGetConf#testUnknownJournalNodeHost;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.ssl.require.client.cert=false, hadoop.security.groups.cache.secs=732459138, hadoop.ssl.client.conf=ssl-client.xml, hadoop.ssl.enabled.protocols=TLSv1.2, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.ssl.hostname.verifier=DEFAULT, hadoop.kerberos.keytab.login.autorenewal.enabled=true, hadoop.security.groups.negative-cache.secs=2074563951, ssl.client.truststore.location=, ssl.client.keystore.type=jks, hadoop.security.dns.log-slow-lookups.threshold.ms=667311967, dfs.namenode.shared.edits.dir=qjournal://jn1:8020;jn2:8020;jn3:8020/jndata, hadoop.security.authentication=simple, ssl.client.truststore.type=jks, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1391658088, io.file.buffer.size=1463502453, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=63207911, ssl.client.stores.reload.interval=1652703057, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, hadoop.security.groups.cache.warn.after.ms=529077704];java.net.UnknownHostException;qjournal://jn1:8020;jn2:8020;jn3:8020/jndata;[org.apache.hadoop.hdfs.DFSUtil.getJournalNodeAddresses(DFSUtil.java:521), org.apache.hadoop.hdfs.tools.TestGetConf.testUnknownJournalNodeHost(TestGetConf.java:497), org.apache.hadoop.hdfs.tools.TestGetConf.testUnknownJournalNodeHost$$CONFUZZ(TestGetConf.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n']
[dfs.namenode.delegation.token.always-use];java.io.IOException;1;['org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testCheckpointTriggerOnTxnCount;id_000000;[dfs.namenode.delegation.token.always-use=true];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n']
[dfs.namenode.top.window.num.buckets];java.lang.IllegalArgumentException;21;['org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithForceAndClusterId;id_000000;[dfs.namenode.top.window.num.buckets=804898001];java.lang.IllegalArgumentException;271544803;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithForce;id_000000;[dfs.namenode.top.window.num.buckets=1389705150];java.lang.IllegalArgumentException;613352979;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestAllowFormat#testFormatShouldBeIgnoredForNonFileBasedDirs;id_000000;[dfs.namenode.top.window.num.buckets=892192461];java.lang.IllegalArgumentException;1813093555;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithNonInteractiveAndForce;id_000000;[dfs.namenode.top.window.num.buckets=1429307028];java.lang.IllegalArgumentException;1415657286;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testNNFormatFailure;id_000000;[dfs.namenode.top.window.num.buckets=82557046];java.lang.IllegalArgumentException;937897679;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testFSNamespaceClearLeases;id_000001;[dfs.namenode.top.window.num.buckets=1080646957];java.lang.IllegalArgumentException;1152764639;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServerXFrame#testSecondaryNameNodeXFrame;id_000000;[dfs.namenode.top.window.num.buckets=172860233];java.lang.IllegalArgumentException;1506409072;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testCrashWhenWritingVersionFiles;id_000000;[dfs.namenode.top.window.num.buckets=1728086391];java.lang.IllegalArgumentException;588127031;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testFailedSaveNamespaceWithRecovery;id_000000;[dfs.namenode.top.window.num.buckets=1049269704];java.lang.IllegalArgumentException;215221742;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testSaveWhileEditsRolled;id_000001;[dfs.namenode.top.window.num.buckets=160337786];java.lang.IllegalArgumentException;1625406683;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testFailedSaveNamespace;id_000000;[dfs.namenode.top.window.num.buckets=425728600];java.lang.IllegalArgumentException;1821767877;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testCrashInAllImageDirs;id_000001;[dfs.namenode.top.window.num.buckets=1774430481];java.lang.IllegalArgumentException;1907045242;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testCancelSaveNamespace;id_000000;[dfs.namenode.top.window.num.buckets=1821299197];java.lang.IllegalArgumentException;1355543519;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithoutForceEnterNo;id_000001;[dfs.namenode.top.window.num.buckets=1951701097];java.lang.IllegalArgumentException;1724181297;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithNonInteractive;id_000001;[dfs.namenode.top.window.num.buckets=368145279];java.lang.IllegalArgumentException;1460114628;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings#testThatMatchingRPCandHttpPortsThrowException;id_000001;[dfs.namenode.top.window.num.buckets=2000143364];java.lang.IllegalArgumentException;857928083;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCommitBlockSynchronization#testCommitBlockSynchronizationWithCloseAndNonExistantTarget;id_000000;[dfs.namenode.top.window.num.buckets=392165642];java.lang.IllegalArgumentException;1088251044;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCommitBlockSynchronization#testCommitBlockSynchronizationWithDelete;id_000000;[dfs.namenode.top.window.num.buckets=1718295081];java.lang.IllegalArgumentException;834653836;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCommitBlockSynchronization#testCommitBlockSynchronization;id_000000;[dfs.namenode.top.window.num.buckets=1025027162];java.lang.IllegalArgumentException;197762664;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testTxIdPersistence;id_000000;[dfs.namenode.top.window.num.buckets=458820444];java.lang.IllegalArgumentException;971561150;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestCommitBlockSynchronization#testCommitBlockSynchronizationWithClose;id_000000;[dfs.namenode.top.window.num.buckets=1367978652];java.lang.IllegalArgumentException;533999751;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[dfs.namenode.top.window.num.buckets];java.io.IOException;17;['org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithForceAndClusterId;id_000001;[dfs.namenode.top.window.num.buckets=541480838];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1726), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithForceAndClusterId(TestClusterId.java:219), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithForceAndClusterId$$CONFUZZ(TestClusterId.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithForce;id_000001;[dfs.namenode.top.window.num.buckets=1849317259];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1726), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithForce(TestClusterId.java:193), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithForce$$CONFUZZ(TestClusterId.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testReset;id_000002;[dfs.namenode.top.window.num.buckets=1628173837];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1515018035 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testReset(TestFSNamesystem.java:185), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testReset$$CONFUZZ(TestFSNamesystem.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithNonInteractiveAndForce;id_000001;[dfs.namenode.top.window.num.buckets=1776572935];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1726), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithNonInteractiveAndForce(TestClusterId.java:380), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithNonInteractiveAndForce$$CONFUZZ(TestClusterId.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testNNFormatFailure;id_000001;[dfs.namenode.top.window.num.buckets=783837036];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testNNFormatFailure(TestClusterId.java:471)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testFSNamespaceClearLeases;id_000000;[dfs.namenode.top.window.num.buckets=895788770];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testFSNamespaceClearLeases(TestFSNamesystem.java:88)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testFailedSaveNamespaceWithRecovery;id_000001;[dfs.namenode.top.window.num.buckets=685747493];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.doTestFailedSaveNamespace(TestSaveNamespace.java:398)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testRTEWhileSavingSecondImage;id_000001;[dfs.namenode.top.window.num.buckets=1160193976];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.saveNamespaceWithInjectedFault(TestSaveNamespace.java:155)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testSaveWhileEditsRolled;id_000000;[dfs.namenode.top.window.num.buckets=875790924];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.testSaveWhileEditsRolled(TestSaveNamespace.java:461)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testFailedSaveNamespace;id_000001;[dfs.namenode.top.window.num.buckets=1061157622];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.doTestFailedSaveNamespace(TestSaveNamespace.java:398)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testCrashInAllImageDirs;id_000000;[dfs.namenode.top.window.num.buckets=688922545];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.saveNamespaceWithInjectedFault(TestSaveNamespace.java:155)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testCancelSaveNamespace;id_000001;[dfs.namenode.top.window.num.buckets=95929870];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.testCancelSaveNamespace(TestSaveNamespace.java:535)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithoutForceEnterNo;id_000000;[dfs.namenode.top.window.num.buckets=443078599];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1726), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithoutForceEnterNo(TestClusterId.java:451), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithoutForceEnterNo$$CONFUZZ(TestClusterId.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestClusterId#testFormatWithNonInteractive;id_000000;[dfs.namenode.top.window.num.buckets=1634267263];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1726), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithNonInteractive(TestClusterId.java:331), org.apache.hadoop.hdfs.server.namenode.TestClusterId.testFormatWithNonInteractive$$CONFUZZ(TestClusterId.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings#testThatMatchingRPCandHttpPortsThrowException;id_000000;[dfs.namenode.top.window.num.buckets=2062793231];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestValidateConfigurationSettings.testThatMatchingRPCandHttpPortsThrowException(TestValidateConfigurationSettings.java:69)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testTxIdPersistence;id_000001;[dfs.namenode.top.window.num.buckets=653991028];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.testTxIdPersistence(TestSaveNamespace.java:497)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testInitAuditLoggers;id_000000;[dfs.namenode.top.window.num.buckets=2089286111];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 59155536 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testInitAuditLoggers(TestFSNamesystem.java:256), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testInitAuditLoggers$$CONFUZZ(TestFSNamesystem.java)]\n']
[dfs.namenode.fs-limits.max-xattr-size];java.io.IOException;13;['org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations#testResolveReservedPath;id_000002;[dfs.namenode.fs-limits.max-xattr-size=490894067];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 290013586 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.setupFileSystem(TestGetBlockLocations.java:135), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testResolveReservedPath(TestGetBlockLocations.java:56), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testResolveReservedPath$$CONFUZZ(TestGetBlockLocations.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testSafemodeReplicationConf;id_000001;[dfs.namenode.fs-limits.max-xattr-size=1322977681];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1094578826 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testSafemodeReplicationConf(TestFSNamesystem.java:233), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testSafemodeReplicationConf$$CONFUZZ(TestFSNamesystem.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testParentDirectoryNameIsCorrect;id_000002;[dfs.namenode.fs-limits.max-xattr-size=33016955];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1237611425 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.lazyInitFSDirectory(TestFsLimits.java:296)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxComponentLength;id_000002;[dfs.namenode.fs-limits.max-xattr-size=1776695908];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 236091482 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.lazyInitFSDirectory(TestFsLimits.java:296)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxDirItemsRename;id_000002;[dfs.namenode.fs-limits.max-xattr-size=1977552670];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1339512256 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.lazyInitFSDirectory(TestFsLimits.java:296)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxComponentLengthRename;id_000001;[dfs.namenode.fs-limits.max-xattr-size=1281602175];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1085646195 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.lazyInitFSDirectory(TestFsLimits.java:296)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testMaxDirItemsLimits;id_000002;[dfs.namenode.fs-limits.max-xattr-size=962204787];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1680136099 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.lazyInitFSDirectory(TestFsLimits.java:296)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog#testEditsForCancelOnTokenExpire;id_000000;[dfs.namenode.fs-limits.max-xattr-size=1333007591];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 178926302 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog.testEditsForCancelOnTokenExpire(TestSecurityTokenEditLog.java:205), org.apache.hadoop.hdfs.server.namenode.TestSecurityTokenEditLog.testEditsForCancelOnTokenExpire$$CONFUZZ(TestSecurityTokenEditLog.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations#testGetBlockLocationsRacingWithDelete;id_000001;[dfs.namenode.fs-limits.max-xattr-size=385441017];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1957705437 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.setupFileSystem(TestGetBlockLocations.java:135), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testGetBlockLocationsRacingWithDelete(TestGetBlockLocations.java:65), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testGetBlockLocationsRacingWithDelete$$CONFUZZ(TestGetBlockLocations.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations#testGetBlockLocationsRacingWithRename;id_000002;[dfs.namenode.fs-limits.max-xattr-size=180277219];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1091772197 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.setupFileSystem(TestGetBlockLocations.java:135), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testGetBlockLocationsRacingWithRename(TestGetBlockLocations.java:98), org.apache.hadoop.hdfs.server.namenode.TestGetBlockLocations.testGetBlockLocationsRacingWithRename$$CONFUZZ(TestGetBlockLocations.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testHAStateInNamespaceInfo;id_000001;[dfs.namenode.fs-limits.max-xattr-size=1516721468];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1775777039 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testHAStateInNamespaceInfo(TestFSNamesystem.java:170), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testHAStateInNamespaceInfo$$CONFUZZ(TestFSNamesystem.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testDuringEditLogs;id_000000;[dfs.namenode.fs-limits.max-xattr-size=1186041260];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 159449193 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.lazyInitFSDirectory(TestFsLimits.java:296)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFsLimits#testNoLimits;id_000000;[dfs.namenode.fs-limits.max-xattr-size=665395940];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1217993870 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.getMockNamesystem(TestFsLimits.java:57), org.apache.hadoop.hdfs.server.namenode.TestFsLimits.lazyInitFSDirectory(TestFsLimits.java:296)]\n']
[hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.rpc.socket.factory.class.default,hadoop.security.auth_to_local,hadoop.security.auth_to_local.mechanism,hadoop.security.authentication,hadoop.security.authorization,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.group.mapping,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.security.token.service.use_ip,hadoop.user.group.static.mapping.overrides,ipc.0.callqueue.overflow.trigger.failover,ipc.client.async.calls.max,ipc.client.connect.timeout,ipc.client.connection.idle-scan-interval.ms,ipc.client.connection.maxidletime,ipc.client.idlethreshold,ipc.client.low-latency,ipc.client.tcpnodelay,ipc.ping.interval,ipc.server.handler.queue.size,ipc.server.max.response.size,ipc.server.read.connection-queue.size,ipc.server.read.threadpool.size,ipc.server.reuseaddr,ipc.server.tcpnodelay,rpc.engine.org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolPB];, ipc.maximum.data.length=134217728, ipc.client.connect.max.retries.on.sasl=5, ipc.client.bind.wildcard.addr=false, hadoop.security.groups.negative-cache.secs=343513958, ipc.client.kill.max=10, ipc.0.faircallqueue.priority-levels=0, ipc.server.listen.queue.size=256, ipc.0.backoff.enable=false, ipc.client.connect.max.retries.on.timeouts=45, rpc.metrics.timeunit=MILLISECONDS, hadoop.security.dns.log-slow-lookups.enabled=false, ipc.client.fallback-to-simple-auth-allowed=false, ipc.client.connect.max.retries=10, ipc.server.log.slow.rpc=false, ipc.client.ping=true, ipc.client.connect.retry.interval=1000, ipc.server.purge.interval=15, ipc.0.scheduler.priority.levels=4, ipc.server.max.connections=-1, ipc.maximum.response.length=134217728];1;['org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestInterDatanodeProtocol#testInterDNProtocolTimeout;id_000000;[hadoop.security.groups.cache.secs=338208165, ipc.client.async.calls.max=100, ipc.server.tcpnodelay=true, ipc.0.callqueue.overflow.trigger.failover=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], ipc.client.low-latency=false, ipc.client.idlethreshold=4000, ipc.server.max.response.size=1048576, ipc.client.connection.idle-scan-interval.ms=10000, ipc.ping.interval=60000, hadoop.kerberos.keytab.login.autorenewal.enabled=true, ipc.server.handler.queue.size=100, hadoop.security.authorization=false, ipc.client.connection.maxidletime=10000, ipc.server.read.connection-queue.size=100, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, ipc.client.tcpnodelay=true, rpc.engine.org.apache.hadoop.hdfs.protocolPB.InterDatanodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, hadoop.security.dns.log-slow-lookups.threshold.ms=106920305, ipc.client.connect.timeout=20000, hadoop.security.authentication=simple, hadoop.security.groups.cache.background.reload.threads=1249758458, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=93786185, ipc.server.reuseaddr=true, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, ipc.server.read.threadpool.size=1, hadoop.security.groups.cache.warn.after.ms=872304836, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.background.reload=true, hadoop.user.group.static.mapping.overrides=dr.who=;, ipc.maximum.data.length=134217728, ipc.client.connect.max.retries.on.sasl=5, ipc.client.bind.wildcard.addr=false, hadoop.security.groups.negative-cache.secs=343513958, ipc.client.kill.max=10, ipc.0.faircallqueue.priority-levels=0, ipc.server.listen.queue.size=256, ipc.0.backoff.enable=false, ipc.client.connect.max.retries.on.timeouts=45, rpc.metrics.timeunit=MILLISECONDS, hadoop.security.dns.log-slow-lookups.enabled=false, ipc.client.fallback-to-simple-auth-allowed=false, ipc.client.connect.max.retries=10, ipc.server.log.slow.rpc=false, ipc.client.ping=true, ipc.client.connect.retry.interval=1000, ipc.server.purge.interval=15, ipc.0.scheduler.priority.levels=4, ipc.server.max.connections=-1, ipc.maximum.response.length=134217728];java.net.SocketTimeoutException;500 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:42740 remote=/127.0.0.1:41291];[org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:163), org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161), org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131), java.io.FilterInputStream.read(FilterInputStream.java:133), java.io.BufferedInputStream.fill(BufferedInputStream.java:252)]\n']
[dfs.block.scanner.skip.recent.accessed,dfs.block.scanner.volume.bytes.per.second,dfs.block.scanner.volume.join.timeout.ms,dfs.datanode.du.reserved,dfs.datanode.du.reserved.calculator,dfs.datanode.du.reserved.disk,dfs.datanode.fixed.volume.size,dfs.datanode.scan.period.hours,fs.df.interval];java.lang.ClassNotFoundException;1;['org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList#testNonDfsUsedMetricForVolume;id_000000;[dfs.block.scanner.skip.recent.accessed=true, dfs.datanode.scan.period.hours=-1, dfs.datanode.du.reserved.calculator=, dfs.datanode.du.reserved.disk=1839157127, dfs.datanode.fixed.volume.size=false, dfs.block.scanner.volume.join.timeout.ms=2049160563, fs.df.interval=1705996909, dfs.block.scanner.volume.bytes.per.second=1327962148, dfs.datanode.du.reserved=100];java.lang.ClassNotFoundException;Class  not found;[org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2685), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2779), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2805), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder.build(ReservedSpaceCalculator.java:66), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.<init>(FsVolumeImpl.java:170)]\n']
[dfs.datanode.socket.reuse.keepalive,dfs.namenode.checkpoint.period,dfs.permissions.allow.owner.set.quota];org.apache.hadoop.HadoopIllegalArgumentException;1;['org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl#testReadLockCanBeDisabledByConfig;id_000000;[dfs.datanode.socket.reuse.keepalive=1117610962, dfs.namenode.checkpoint.period=1731559748, dfs.permissions.allow.owner.set.quota=false];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.failed.volumes.tolerated - 1879161450. Value configured is either less than maxVolumeFailureLimit or greater than to the number of configured volumes (2).;[org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.<init>(FsDatasetImpl.java:331), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsDatasetImpl.setUp(TestFsDatasetImpl.java:198), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62), jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)]\n']
[dfs.block.invalidate.limit,dfs.block.placement.ec.classname,dfs.checksum.combine.mode,dfs.client-write-packet-size,dfs.client.block.write.locateFollowingBlock.initial.delay.ms,dfs.client.block.write.retries,dfs.client.datanode-restart.timeout,dfs.client.deadnode.detection.enabled,dfs.client.max.block.acquire.failures,dfs.client.mmap.cache.size,dfs.client.mmap.cache.timeout.ms,dfs.client.read.shortcircuit.streams.cache.size,dfs.client.read.use.cache.priority,dfs.client.refresh.read-block-locations.ms,dfs.client.retry.interval-ms.get-last-block-length,dfs.client.retry.times.get-last-block-length,dfs.client.server-defaults.validity.period.ms,dfs.client.socket-timeout,dfs.client.socketcache.expiryMsec,dfs.client.use.datanode.hostname,dfs.client.write.exclude.nodes.cache.expiry.interval.millis,dfs.content-summary.limit,dfs.content-summary.sleep-microsec,dfs.datanode.hostname,dfs.datanode.peer.stats.enabled,dfs.datanode.socket.write.timeout,dfs.hosts.exclude,dfs.http.policy,dfs.image.parallel.inode.threshold,dfs.image.parallel.load,dfs.image.parallel.target.sections,dfs.namenode.accesstime.precision,dfs.namenode.acls.enabled,dfs.namenode.audit.log.async,dfs.namenode.audit.loggers,dfs.namenode.avoid.read.slow.datanode,dfs.namenode.avoid.read.stale.datanode,dfs.namenode.avoid.write.stale.datanode,dfs.namenode.blockreport.max.lock.hold.time,dfs.namenode.blockreport.queue.size,dfs.namenode.checkpoint.txns,dfs.namenode.datanode.registration.ip-hostname-check,dfs.namenode.decommission.monitor.class,dfs.namenode.delegation.token.max-lifetime,dfs.namenode.ec.policies.max.cellsize,dfs.namenode.edekcacheloader.initial.delay.ms,dfs.namenode.edit.log.autoroll.check.interval.ms,dfs.namenode.edits.noeditlogchannelflush,dfs.namenode.file.close.num-committed-allowed,dfs.namenode.fs-limits.max-xattrs-per-inode,dfs.namenode.fs-limits.min-block-size,dfs.namenode.gc.time.monitor.enable,dfs.namenode.gc.time.monitor.sleep.interval.ms,dfs.namenode.https-address,dfs.namenode.invalidate.work.pct.per.iteration,dfs.namenode.lazypersist.file.scrub.interval.sec,dfs.namenode.lease-recheck-interval-ms,dfs.namenode.list.cache.directives.num.responses,dfs.namenode.list.openfiles.num.responses,dfs.namenode.max-lock-hold-to-release-lease-ms,dfs.namenode.metrics.logger.period.seconds,dfs.namenode.min.supported.datanode.version,dfs.namenode.num.extra.edits.retained,dfs.namenode.path.based.cache.block.map.allocation.percent,dfs.namenode.posix.acl.inheritance.enabled,dfs.namenode.read.considerLoad,dfs.namenode.reconstruction.pending.timeout-sec,dfs.namenode.redundancy.considerLoad,dfs.namenode.redundancy.considerLoad.factor,dfs.namenode.redundancy.considerLoadByStorageType,dfs.namenode.redundancy.queue.restart.iterations,dfs.namenode.reject-unresolved-dn-topology-mapping,dfs.namenode.replication.max-streams,dfs.namenode.replication.max-streams-hard-limit,dfs.namenode.replication.min,dfs.namenode.replication.work.multiplier.per.iteration,dfs.namenode.resource.check.interval,dfs.namenode.resource.du.reserved,dfs.namenode.safemode.extension.testing,dfs.namenode.safemode.min.datanodes,dfs.namenode.safemode.threshold-pct,dfs.namenode.slowpeer.collect.interval,dfs.namenode.snapshot.capture.openfiles,dfs.namenode.snapshot.max.limit,dfs.namenode.snapshot.skip.capture.accesstime-only-change,dfs.namenode.snapshot.skiplist.max.levels,dfs.namenode.stale.datanode.minimum.interval,dfs.namenode.support.allow.format,dfs.namenode.top.enabled,dfs.namenode.top.num.users,dfs.namenode.write-lock-reporting-threshold-ms,dfs.namenode.xattrs.enabled,dfs.net.topology.impl,dfs.permissions.ContentSummary.subAccess,dfs.provided.storage.id,dfs.quota.by.storage.type.enabled,dfs.replication,dfs.use.dfs.network.topology,dfs.xframe.value,fs.automatic.close,fs.client.resolve.topology.enabled,fs.hdfs.impl.disable.cache,fs.trash.interval,hadoop.http.authentication.simple.anonymous.allowed,hadoop.http.authentication.token.validity,hadoop.http.max.request.header.size,hadoop.http.max.threads,hadoop.http.sni.host.check.enabled,hadoop.http.socket.backlog.size,hadoop.prometheus.endpoint.enabled,hadoop.rpc.socket.factory.class.default,hadoop.security.auth_to_local.mechanism,hadoop.security.authentication,hadoop.security.authorization,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.group.mapping,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.ssl.client.conf,hadoop.user.group.static.mapping.overrides,io.file.buffer.size,ipc.0.backoff.enable,ipc.0.callqueue.overflow.trigger.failover,ipc.client.bind.wildcard.addr,ipc.client.connect.max.retries.on.timeouts,ipc.client.connection.maxidletime,ipc.client.idlethreshold,ipc.client.low-latency,ipc.server.handler.queue.size,ipc.server.max.response.size,ipc.server.read.connection-queue.size,ipc.server.reuseaddr,ipc.server.tcpnodelay,rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB,rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB,rpc.engine.org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB,rpc.engine.org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB,ssl.client.truststore.location,ssl.client.truststore.type];, dfs.bytes-per-checksum=1302945458, dfs.namenode.max.objects=303197958, dfs.cluster.administrators=, ipc.maximum.data.length=113700742, hadoop.ssl.hostname.verifier=DEFAULT, dfs.client.cached.conn.retry=2052686672, dfs.client.read.shortcircuit=true, ipc.client.connect.max.retries.on.sasl=154326440, dfs.namenode.lease-hard-limit-sec=1675707541, hadoop.http.authentication.type=simple, ipc.0.faircallqueue.priority-levels=2074574032, dfs.namenode.list.encryption.zones.num.responses=66603119, dfs.client.key.provider.cache.expiry=1600564707, dfs.namenode.audit.log.token.tracking.id=false, dfs.datanode.fsdataset.factory=org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory, ssl.client.keystore.type=jks, dfs.client.failover.sleep.base.millis=459467938, dfs.namenode.decommission.interval=890701955s, dfs.permissions.superusergroup=fpwxhtijgqlfstjtflhcdsnsevijhxlpktwsvukejlbhcvkvmymovqsyigydudynthgroup, dfs.namenode.path.based.cache.refresh.interval.ms=707738608, dfs.block.replicator.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault, dfs.namenode.fs-limits.max-directory-items=293312080, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, dfs.user.home.dir.prefix=/user, dfs.namenode.gc.time.monitor.observation.window.ms=2449ms, dfs.client.socket.send.buffer.size=1744892162, ipc.client.fallback-to-simple-auth-allowed=false, dfs.client.hedged.read.threadpool.size=764703361, dfs.blockreport.initialDelay=1349745181, dfs.namenode.heartbeat.recheck-interval=614609825, dfs.namenode.safemode.extension=353540797, dfs.client.failover.sleep.max.millis=1595674014, dfs.xframe.enabled=false, dfs.namenode.delegation.key.update-interval=1286353654, dfs.namenode.redundancy.interval.seconds=2027960425, hadoop.rpc.protection=authentication, fs.permissions.umask-mode=1259828956, dfs.namenode.fs-limits.max-xattr-size=694093204, ipc.0.scheduler.priority.levels=1019779421, hdfs.minidfs.basedir=/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs, hadoop.http.staticuser.user=dr.who, dfs.datanode.http.address=0.0.0.0:9864, dfs.namenode.blocks.per.postponedblocks.rescan=248312906, rpc.engine.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, ipc.maximum.response.length=2088258997, dfs.lock.suppress.warning.interval=0ms, ipc.client.async.calls.max=1259423335, hadoop.http.authentication.kerberos.keytab=/root/hadoop.keytab, dfs.namenode.maintenance.replication.min=129459941, hadoop.jetty.logs.serve.aliases=false, dfs.client.retry.max.attempts=2123720127, dfs.namenode.fs-limits.max-blocks-per-file=1206507578, dfs.namenode.max.op.size=1478612957, dfs.namenode.hosts.provider.classname=org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager, dfs.namenode.decommission.interval.testing=890701955, dfs.client.block.write.replace-datanode-on-failure.enable=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.caller.context.max.size=325719005, dfs.namenode.checkpoint.dir=file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namesecondary-0-1,file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namesecondary-0-2, dfs.namenode.top.windows.minutes=1,5,25, dfs.client.use.legacy.blockreader.local=true, ipc.client.connection.idle-scan-interval.ms=1352867839, ipc.ping.interval=1519574053, dfs.namenode.edits.dir.minimum=189057310, dfs.client.retry.window.base=1856330182, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, dfs.namenode.startup=REGULAR, hadoop.kerberos.keytab.login.autorenewal.enabled=true, dfs.storage.policy.enabled=true, dfs.checksum.ec.socket-timeout=1308948615, dfs.client.test.drop.namenode.response.number=910217168, dfs.storage.policy.permissions.superuser-only=false, dfs.namenode.list.cache.pools.num.responses=417898669, fs.df.interval=1779171276, fs.creation.parallel.count=293872294, dfs.batched.ls.limit=474239625, hadoop.metrics.jvm.use-thread-mxbean=false, dfs.namenode.ec.userdefined.policy.allowed=false, dfs.checksum.type=CRC32C, dfs.client.read.short.circuit.replica.stale.threshold.ms=2063453268, dfs.namenode.num.checkpoints.retained=1959316287, dfs.namenode.state.context.enabled=true, jvm.pause.warn-threshold.ms=1925501320, dfs.client.mmap.retry.timeout.ms=1276965157, dfs.namenode.fslock.fair=false, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.permissions.enabled=false, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.hdfs.web.AuthFilterInitializer, dfs.namenode.stale.datanode.interval=435705144, ipc.server.read.threadpool.size=204011757, dfs.client.read.shortcircuit.buffer.size=312936632, dfs.namenode.provided.enabled=false, dfs.namenode.snapshotdiff.allow.snap-root-descendant=true, ipc.client.rpc-timeout.ms=604813292, fs.client.resolve.remote.symlinks=true, hadoop.ssl.enabled.protocols=TLSv1.2, dfs.namenode.ec.system.default.policy=RS-6-3-1024k, dfs.domain.socket.path=, dfs.namenode.handler.count=0.9f, dfs.namenode.decommission.blocks.per.interval=260941003, dfs.replication.max=900548716, dfs.namenode.name.dir=file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1,file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2, dfs.namenode.read-lock-reporting-threshold-ms=954287146, dfs.ha.nn.not-become-active-in-safemode=false, dfs.datanode.https.address=0.0.0.0:9865, dfs.ha.standby.checkpoints=false, ipc.client.kill.max=1142634401, ipc.server.listen.queue.size=1845059561, dfs.client.block.write.replace-datanode-on-failure.min-replication=982134486, dfs.namenode.replqueue.threshold-pct=0.36209333, dfs.client.domain.socket.data.traffic=true, dfs.namenode.enable.log.stale.datanode=false, dfs.block.access.token.enable=false, rpc.metrics.timeunit=MILLISECONDS, dfs.blocksize=1392583667, dfs.client.write.byte-array-manager.enabled=true, hadoop.caller.context.signature.max.size=965228980, hadoop.security.dns.log-slow-lookups.enabled=false, bind.address=localhost];1;['org.apache.hadoop.hdfs.TestRollingUpgradeDowngrade#testRejectNewFsImage;id_000000;[dfs.namenode.avoid.read.slow.datanode=true, dfs.namenode.read.considerLoad=true, hadoop.security.groups.cache.secs=489387043, dfs.use.dfs.network.topology=true, hadoop.http.max.threads=899529551, dfs.namenode.resource.check.interval=1726100494, dfs.datanode.peer.stats.enabled=true, ipc.server.tcpnodelay=true, dfs.block.invalidate.limit=1753142513, dfs.replication=0, dfs.namenode.snapshot.skiplist.max.levels=2133429281, dfs.namenode.audit.log.async=true, ipc.client.low-latency=true, dfs.checksum.combine.mode=COMPOSITE_CRC, dfs.namenode.min.supported.datanode.version=2.1.0-beta, dfs.namenode.write-lock-reporting-threshold-ms=1539429569, dfs.namenode.avoid.read.stale.datanode=true, ipc.server.handler.queue.size=621558909, ssl.client.truststore.location=, ipc.client.connection.maxidletime=1402292041, dfs.namenode.lease-recheck-interval-ms=423032125, dfs.client.block.write.locateFollowingBlock.initial.delay.ms=1821072929, fs.automatic.close=true, dfs.namenode.https-address=0.0.0.0:9871, dfs.namenode.stale.datanode.minimum.interval=1573515794, dfs.namenode.ec.policies.max.cellsize=476737704, ssl.client.truststore.type=jks, hadoop.security.groups.cache.background.reload.threads=972521003, dfs.hosts.exclude=, dfs.namenode.replication.min=750864992, dfs.content-summary.sleep-microsec=1222854010, dfs.namenode.redundancy.considerLoad=false, dfs.provided.storage.id=DS-PROVIDED, dfs.client.socketcache.expiryMsec=1010088144, dfs.namenode.fs-limits.min-block-size=1981583716, dfs.image.parallel.load=false, dfs.namenode.acls.enabled=false, dfs.namenode.gc.time.monitor.enable=true, hadoop.http.authentication.token.validity=1171256640, dfs.namenode.resource.du.reserved=2025455746, dfs.namenode.datanode.registration.ip-hostname-check=false, dfs.namenode.path.based.cache.block.map.allocation.percent=0.23766118, dfs.namenode.slowpeer.collect.interval=36007473d, dfs.client.server-defaults.validity.period.ms=626900577, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, dfs.namenode.edits.noeditlogchannelflush=true, dfs.permissions.ContentSummary.subAccess=true, dfs.namenode.blockreport.queue.size=1767390456, dfs.namenode.audit.loggers=default, hadoop.security.groups.cache.warn.after.ms=945782589, hadoop.http.sni.host.check.enabled=true, dfs.namenode.lazypersist.file.scrub.interval.sec=1508870628, hadoop.security.auth_to_local.mechanism=hadoop, dfs.namenode.blockreport.max.lock.hold.time=424519705, dfs.namenode.snapshot.capture.openfiles=false, dfs.http.policy=HTTP_AND_HTTPS, dfs.namenode.safemode.min.datanodes=769897296, rpc.engine.org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, hadoop.http.socket.backlog.size=934114602, dfs.namenode.snapshot.skip.capture.accesstime-only-change=false, dfs.namenode.metrics.logger.period.seconds=608506147, hadoop.prometheus.endpoint.enabled=false, dfs.namenode.delegation.token.max-lifetime=1672740206, dfs.client.use.datanode.hostname=true, ipc.client.bind.wildcard.addr=false, dfs.namenode.avoid.write.stale.datanode=true, fs.trash.interval=305037408, ipc.0.backoff.enable=false, dfs.namenode.num.extra.edits.retained=813745487, dfs.block.placement.ec.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant, ipc.client.connect.max.retries.on.timeouts=1380968965, dfs.net.topology.impl=org.apache.hadoop.hdfs.net.DFSNetworkTopology, fs.client.resolve.topology.enabled=false, dfs.client.read.use.cache.priority=false, dfs.client.refresh.read-block-locations.ms=1494854144, dfs.namenode.edekcacheloader.initial.delay.ms=404723338, dfs.client.mmap.cache.size=298902949, io.file.buffer.size=1661120811, dfs.client.max.block.acquire.failures=936215924, dfs.client.deadnode.detection.enabled=true, dfs.namenode.max-lock-hold-to-release-lease-ms=1060971097, dfs.client.retry.interval-ms.get-last-block-length=1983795560, dfs.client.datanode-restart.timeout=1178302186, dfs.namenode.xattrs.enabled=true, rpc.engine.org.apache.hadoop.ipc.ProtocolMetaInfoPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, dfs.client-write-packet-size=295218802, dfs.namenode.checkpoint.txns=618467375, dfs.client.block.write.retries=1020970908, dfs.namenode.list.openfiles.num.responses=1942852831, dfs.namenode.safemode.threshold-pct=0.85153955, dfs.image.parallel.inode.threshold=1889569794, dfs.namenode.list.cache.directives.num.responses=900135670, dfs.namenode.decommission.monitor.class=org.apache.hadoop.hdfs.server.blockmanagement.DatanodeAdminDefaultMonitor, dfs.namenode.replication.max-streams=1761183618, ipc.0.callqueue.overflow.trigger.failover=false, dfs.client.write.exclude.nodes.cache.expiry.interval.millis=795159566, dfs.namenode.safemode.extension.testing=353540797, dfs.client.mmap.cache.timeout.ms=1936548765, fs.hdfs.impl.disable.cache=true, dfs.image.parallel.target.sections=83340496, ipc.client.idlethreshold=582027314, dfs.namenode.file.close.num-committed-allowed=1483979645, dfs.quota.by.storage.type.enabled=false, dfs.namenode.reconstruction.pending.timeout-sec=10530412, ipc.server.max.response.size=945362494, dfs.namenode.replication.work.multiplier.per.iteration=1686464393, hadoop.http.authentication.simple.anonymous.allowed=true, hadoop.security.authorization=false, dfs.namenode.redundancy.considerLoadByStorageType=false, dfs.datanode.hostname=127.0.0.1, ipc.server.read.connection-queue.size=840951104, hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.StandardSocketFactory, rpc.engine.org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, dfs.client.retry.times.get-last-block-length=1518790416, hadoop.security.dns.log-slow-lookups.threshold.ms=1134296147, dfs.namenode.reject-unresolved-dn-topology-mapping=true, hadoop.security.authentication=simple, dfs.client.socket-timeout=418248642, dfs.client.read.shortcircuit.streams.cache.size=1593403176, dfs.xframe.value=SAMEORIGIN, dfs.namenode.replication.max-streams-hard-limit=1858526697, dfs.namenode.top.num.users=1986845554, rpc.engine.org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, ipc.server.reuseaddr=false, dfs.datanode.socket.write.timeout=1465995456, dfs.namenode.accesstime.precision=1207935780, hadoop.http.max.request.header.size=1135038913, dfs.namenode.redundancy.considerLoad.factor=0.99657124, dfs.namenode.fs-limits.max-xattrs-per-inode=343309966, dfs.namenode.gc.time.monitor.sleep.interval.ms=214s, dfs.namenode.edit.log.autoroll.check.interval.ms=1176337337, dfs.namenode.support.allow.format=true, dfs.namenode.redundancy.queue.restart.iterations=469522631, dfs.namenode.snapshot.max.limit=2035478812, hadoop.ssl.client.conf=ssl-client.xml, dfs.namenode.invalidate.work.pct.per.iteration=1.0f, dfs.content-summary.limit=59513110, hadoop.security.groups.cache.background.reload=false, dfs.namenode.posix.acl.inheritance.enabled=false, dfs.namenode.top.enabled=true, hadoop.user.group.static.mapping.overrides=dr.who=;, dfs.bytes-per-checksum=1302945458, dfs.namenode.max.objects=303197958, dfs.cluster.administrators=, ipc.maximum.data.length=113700742, hadoop.ssl.hostname.verifier=DEFAULT, dfs.client.cached.conn.retry=2052686672, dfs.client.read.shortcircuit=true, ipc.client.connect.max.retries.on.sasl=154326440, dfs.namenode.lease-hard-limit-sec=1675707541, hadoop.http.authentication.type=simple, ipc.0.faircallqueue.priority-levels=2074574032, dfs.namenode.list.encryption.zones.num.responses=66603119, dfs.client.key.provider.cache.expiry=1600564707, dfs.namenode.audit.log.token.tracking.id=false, dfs.datanode.fsdataset.factory=org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory, ssl.client.keystore.type=jks, dfs.client.failover.sleep.base.millis=459467938, dfs.namenode.decommission.interval=890701955s, dfs.permissions.superusergroup=fpwxhtijgqlfstjtflhcdsnsevijhxlpktwsvukejlbhcvkvmymovqsyigydudynthgroup, dfs.namenode.path.based.cache.refresh.interval.ms=707738608, dfs.block.replicator.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault, dfs.namenode.fs-limits.max-directory-items=293312080, rpc.engine.org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, dfs.user.home.dir.prefix=/user, dfs.namenode.gc.time.monitor.observation.window.ms=2449ms, dfs.client.socket.send.buffer.size=1744892162, ipc.client.fallback-to-simple-auth-allowed=false, dfs.client.hedged.read.threadpool.size=764703361, dfs.blockreport.initialDelay=1349745181, dfs.namenode.heartbeat.recheck-interval=614609825, dfs.namenode.safemode.extension=353540797, dfs.client.failover.sleep.max.millis=1595674014, dfs.xframe.enabled=false, dfs.namenode.delegation.key.update-interval=1286353654, dfs.namenode.redundancy.interval.seconds=2027960425, hadoop.rpc.protection=authentication, fs.permissions.umask-mode=1259828956, dfs.namenode.fs-limits.max-xattr-size=694093204, ipc.0.scheduler.priority.levels=1019779421, hdfs.minidfs.basedir=/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs, hadoop.http.staticuser.user=dr.who, dfs.datanode.http.address=0.0.0.0:9864, dfs.namenode.blocks.per.postponedblocks.rescan=248312906, rpc.engine.org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolPB=org.apache.hadoop.ipc.ProtobufRpcEngine2, ipc.maximum.response.length=2088258997, dfs.lock.suppress.warning.interval=0ms, ipc.client.async.calls.max=1259423335, hadoop.http.authentication.kerberos.keytab=/root/hadoop.keytab, dfs.namenode.maintenance.replication.min=129459941, hadoop.jetty.logs.serve.aliases=false, dfs.client.retry.max.attempts=2123720127, dfs.namenode.fs-limits.max-blocks-per-file=1206507578, dfs.namenode.max.op.size=1478612957, dfs.namenode.hosts.provider.classname=org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager, dfs.namenode.decommission.interval.testing=890701955, dfs.client.block.write.replace-datanode-on-failure.enable=false, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.caller.context.max.size=325719005, dfs.namenode.checkpoint.dir=file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namesecondary-0-1,file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namesecondary-0-2, dfs.namenode.top.windows.minutes=1,5,25, dfs.client.use.legacy.blockreader.local=true, ipc.client.connection.idle-scan-interval.ms=1352867839, ipc.ping.interval=1519574053, dfs.namenode.edits.dir.minimum=189057310, dfs.client.retry.window.base=1856330182, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, dfs.namenode.startup=REGULAR, hadoop.kerberos.keytab.login.autorenewal.enabled=true, dfs.storage.policy.enabled=true, dfs.checksum.ec.socket-timeout=1308948615, dfs.client.test.drop.namenode.response.number=910217168, dfs.storage.policy.permissions.superuser-only=false, dfs.namenode.list.cache.pools.num.responses=417898669, fs.df.interval=1779171276, fs.creation.parallel.count=293872294, dfs.batched.ls.limit=474239625, hadoop.metrics.jvm.use-thread-mxbean=false, dfs.namenode.ec.userdefined.policy.allowed=false, dfs.checksum.type=CRC32C, dfs.client.read.short.circuit.replica.stale.threshold.ms=2063453268, dfs.namenode.num.checkpoints.retained=1959316287, dfs.namenode.state.context.enabled=true, jvm.pause.warn-threshold.ms=1925501320, dfs.client.mmap.retry.timeout.ms=1276965157, dfs.namenode.fslock.fair=false, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.permissions.enabled=false, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter,org.apache.hadoop.hdfs.web.AuthFilterInitializer, dfs.namenode.stale.datanode.interval=435705144, ipc.server.read.threadpool.size=204011757, dfs.client.read.shortcircuit.buffer.size=312936632, dfs.namenode.provided.enabled=false, dfs.namenode.snapshotdiff.allow.snap-root-descendant=true, ipc.client.rpc-timeout.ms=604813292, fs.client.resolve.remote.symlinks=true, hadoop.ssl.enabled.protocols=TLSv1.2, dfs.namenode.ec.system.default.policy=RS-6-3-1024k, dfs.domain.socket.path=, dfs.namenode.handler.count=0.9f, dfs.namenode.decommission.blocks.per.interval=260941003, dfs.replication.max=900548716, dfs.namenode.name.dir=file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1,file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task131/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2, dfs.namenode.read-lock-reporting-threshold-ms=954287146, dfs.ha.nn.not-become-active-in-safemode=false, dfs.datanode.https.address=0.0.0.0:9865, dfs.ha.standby.checkpoints=false, ipc.client.kill.max=1142634401, ipc.server.listen.queue.size=1845059561, dfs.client.block.write.replace-datanode-on-failure.min-replication=982134486, dfs.namenode.replqueue.threshold-pct=0.36209333, dfs.client.domain.socket.data.traffic=true, dfs.namenode.enable.log.stale.datanode=false, dfs.block.access.token.enable=false, rpc.metrics.timeunit=MILLISECONDS, dfs.blocksize=1392583667, dfs.client.write.byte-array-manager.enabled=true, hadoop.caller.context.signature.max.size=965228980, hadoop.security.dns.log-slow-lookups.enabled=false, bind.address=localhost];java.lang.IllegalArgumentException;1944366884;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[dfs.namenode.support.allow.format];java.io.IOException;3;['org.apache.hadoop.hdfs.TestRollingUpgradeDowngrade#testRejectNewFsImage;id_000001;[dfs.namenode.support.allow.format=false];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestGenericJournalConf#testNotConfigured;id_000000;[dfs.namenode.support.allow.format=false];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n', 'org.apache.hadoop.hdfs.server.balancer.TestBalancerWithNodeGroup#testBPPNodeGroup;id_000000;[dfs.namenode.support.allow.format=false];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n']
[file.bytes-per-checksum];java.lang.IllegalStateException;6;['org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker#testTimeoutInCheck;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler#testThrottle;id_000002;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]\n', 'org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker#testFailedLocationsBelowThreshold;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler#testThrottleConfigs;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]\n', 'org.apache.hadoop.hdfs.tools.TestDelegationTokenFetcher#testReturnedTokenIsNull;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]\n', 'org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker#testAllLocationsHealthy;id_000001;[file.bytes-per-checksum=0];java.lang.IllegalStateException;bytes per checksum should be positive but was 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkState(Preconditions.java:562), org.apache.hadoop.fs.ChecksumFileSystem.setConf(ChecksumFileSystem.java:80), org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79), org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:139), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3467)]\n']
[dfs.namenode.reencrypt.batch.size];java.lang.OutOfMemoryError;3;['org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler#testThrottleAccumulatingTasks;id_000000;[dfs.namenode.reencrypt.batch.size=1166157354];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionBatch.<init>(ReencryptionHandler.java:488), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.<init>(ReencryptionHandler.java:248), org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler.mockReencryptionhandler(TestReencryptionHandler.java:85), org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler.testThrottleAccumulatingTasks(TestReencryptionHandler.java:171)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler#testThrottleNoOp;id_000000;[dfs.namenode.reencrypt.batch.size=915468936];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionBatch.<init>(ReencryptionHandler.java:488), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.<init>(ReencryptionHandler.java:248), org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler.mockReencryptionhandler(TestReencryptionHandler.java:85), org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler.testThrottleNoOp(TestReencryptionHandler.java:122)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler#testThrottle;id_000000;[dfs.namenode.reencrypt.batch.size=969806285];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler$ReencryptionBatch.<init>(ReencryptionHandler.java:488), org.apache.hadoop.hdfs.server.namenode.ReencryptionHandler.<init>(ReencryptionHandler.java:248), org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler.mockReencryptionhandler(TestReencryptionHandler.java:85), org.apache.hadoop.hdfs.server.namenode.TestReencryptionHandler.testThrottle(TestReencryptionHandler.java:93)]\n']
[io.file.buffer.size];org.apache.hadoop.hdfs.BlockMissingException;2;['org.apache.hadoop.hdfs.TestFileAppend3#testTC1;id_000000;[io.file.buffer.size=2077614173];org.apache.hadoop.hdfs.BlockMissingException;Could not obtain block: BP-53335605-10.0.0.24-1674631252218:blk_1073741825_1001 file=/TC1/foo No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:38299,DS-f3d03d76-f694-4a55-a477-c74083d715eb,DISK] DatanodeInfoWithStorage[127.0.0.1:38769,DS-b1ae5c6f-841f-45e6-bac7-45caa3dd39b6,DISK] DatanodeInfoWithStorage[127.0.0.1:34443,DS-e6b83d35-d049-4909-a034-20e39c1ddfca,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:34443,DS-e6b83d35-d049-4909-a034-20e39c1ddfca,DISK] DatanodeInfoWithStorage[127.0.0.1:38299,DS-f3d03d76-f694-4a55-a477-c74083d715eb,DISK] DatanodeInfoWithStorage[127.0.0.1:38769,DS-b1ae5c6f-841f-45e6-bac7-45caa3dd39b6,DISK];[org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007), org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990), org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969), org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677), org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)]\n', 'org.apache.hadoop.hdfs.TestFileAppend3#testTC12;id_000000;[io.file.buffer.size=2143554397];org.apache.hadoop.hdfs.BlockMissingException;Could not obtain block: BP-2049844900-10.0.0.24-1674635744301:blk_1073741825_1002 file=/TC12/foo1 No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:33569,DS-72759069-6c5f-4192-bf6c-a03abda983e8,DISK] DatanodeInfoWithStorage[127.0.0.1:38531,DS-d30f3e90-7148-44d2-bb27-ce8db956ab5d,DISK] DatanodeInfoWithStorage[127.0.0.1:44901,DS-75f18119-d562-47b9-9b03-3c95d48291df,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:44901,DS-75f18119-d562-47b9-9b03-3c95d48291df,DISK] DatanodeInfoWithStorage[127.0.0.1:33569,DS-72759069-6c5f-4192-bf6c-a03abda983e8,DISK] DatanodeInfoWithStorage[127.0.0.1:38531,DS-d30f3e90-7148-44d2-bb27-ce8db956ab5d,DISK];[org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007), org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990), org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969), org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677), org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)]\n']
[dfs.ha.automatic-failover.enabled];java.lang.IllegalArgumentException;2;['org.apache.hadoop.hdfs.server.namenode.ha.TestHASafeMode#testEnterSafeModeInANNShouldNotThrowNPE;id_000001;[dfs.ha.automatic-failover.enabled=true];java.lang.IllegalArgumentException;968678869;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n', 'org.apache.hadoop.hdfs.tools.TestDFSHAAdminMiniCluster#testTryFailoverToSafeMode;id_000001;[dfs.ha.automatic-failover.enabled=true];java.lang.IllegalArgumentException;2102964757;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[dfs.datanode.max.transfer.threads];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.server.datanode.TestDataXceiverBackwardsCompat#testBackwardsCompat;id_000000;[dfs.datanode.max.transfer.threads=0];java.lang.IllegalArgumentException;duration must be positive: 0 MILLISECONDS;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:387), org.apache.hadoop.thirdparty.com.google.common.cache.CacheBuilder.refreshAfterWrite(CacheBuilder.java:844), org.apache.hadoop.security.Groups.<init>(Groups.java:137), org.apache.hadoop.security.Groups.<init>(Groups.java:102), org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451)]\n']
[dfs.datanode.disk.check.timeout];java.lang.NoClassDefFoundError;1;['org.apache.hadoop.hdfs.server.datanode.TestDataXceiverBackwardsCompat#testBackwardsCompat;id_000001;[dfs.datanode.disk.check.timeout=0h];java.lang.NoClassDefFoundError;Could not initialize class org.apache.hadoop.hdfs.server.common.Util;[org.apache.hadoop.hdfs.server.datanode.ProfilingFileIoEvents.<init>(ProfilingFileIoEvents.java:52), org.apache.hadoop.hdfs.server.datanode.FileIoProvider.<init>(FileIoProvider.java:96), org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:429), org.apache.hadoop.hdfs.server.datanode.TestDataXceiverBackwardsCompat$NullDataNode.<init>(TestDataXceiverBackwardsCompat.java:86), org.apache.hadoop.hdfs.server.datanode.TestDataXceiverBackwardsCompat.testBackwardsCompat(TestDataXceiverBackwardsCompat.java:147)]\n']
[dfs.qjournal.get-journal-state.timeout.ms,dfs.qjournal.http.open.timeout.ms,dfs.qjournal.http.read.timeout.ms,dfs.qjournal.new-epoch.timeout.ms,dfs.qjournal.write-txns.timeout.ms,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.token.service.use_ip,hadoop.ssl.require.client.cert,ipc.maximum.data.length];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManagerUnit#testFSEditLogAutoSyncToQuorumStream;id_000000;[hadoop.security.token.service.use_ip=false, hadoop.security.dns.log-slow-lookups.threshold.ms=2803586, hadoop.kerberos.min.seconds.before.relogin=771751936, dfs.qjournal.new-epoch.timeout.ms=31744, dfs.qjournal.http.open.timeout.ms=23792, hadoop.ssl.require.client.cert=false, hadoop.kerberos.keytab.login.autorenewal.enabled=false, dfs.qjournal.get-journal-state.timeout.ms=49761, dfs.qjournal.http.read.timeout.ms=0, dfs.qjournal.write-txns.timeout.ms=1208156160, ipc.maximum.data.length=0];java.lang.IllegalArgumentException;Attempted to use QJM output buffer capacity (524288) greater than the IPC max data length (ipc.maximum.data.length = 0). This will cause journals to reject edits.;[org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.setOutputBufferCapacity(QuorumJournalManager.java:457), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.<init>(QuorumJournalManager.java:194), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.<init>(QuorumJournalManager.java:121), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManagerUnit$1.<init>(TestQuorumJournalManagerUnit.java:95), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManagerUnit.setup(TestQuorumJournalManagerUnit.java:95)]\n']
[dfs.ha.tail-edits.qjm.rpc.max-txns,dfs.qjm.operations.timeout,dfs.qjournal.get-journal-state.timeout.ms,dfs.qjournal.new-epoch.timeout.ms,dfs.qjournal.select-input-streams.timeout.ms,dfs.qjournal.start-segment.timeout.ms,hadoop.security.groups.cache.secs,ssl.client.stores.reload.interval];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManagerUnit#testFSEditLogAutoSyncToQuorumStream;id_000001;[dfs.qjm.operations.timeout=89227ms, dfs.qjournal.new-epoch.timeout.ms=1845690368, hadoop.security.groups.cache.secs=1903755264, ssl.client.stores.reload.interval=2019858707, dfs.ha.tail-edits.qjm.rpc.max-txns=0, dfs.qjournal.start-segment.timeout.ms=1899871744, dfs.qjournal.select-input-streams.timeout.ms=0, dfs.qjournal.get-journal-state.timeout.ms=0];java.lang.IllegalArgumentException;Must specify dfs.ha.tail-edits.qjm.rpc.max-txns greater than 0!;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:219), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.<init>(QuorumJournalManager.java:151), org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.<init>(QuorumJournalManager.java:121), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManagerUnit$1.<init>(TestQuorumJournalManagerUnit.java:95), org.apache.hadoop.hdfs.qjournal.client.TestQuorumJournalManagerUnit.setup(TestQuorumJournalManagerUnit.java:95)]\n']
[dfs.journalnode.edits.dir];java.lang.OutOfMemoryError;3;['org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalDefaultDirForOneNameSpace;id_000000;[dfs.journalnode.edits.dir=/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task92/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/TestJournalNode];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalDefaultDirForOneNameSpace;id_000001;[dfs.journalnode.edits.dir=/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task92/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/TestJournalNode];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournalNode#testJournalDefaultDirForOneNameSpace;id_000002;[dfs.journalnode.edits.dir=/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task92/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/TestJournalNode];java.lang.OutOfMemoryError;Java heap space;[]\n']
[dfs.namenode.top.window.num.buckets];org.apache.hadoop.metrics2.MetricsException;1;['org.apache.hadoop.hdfs.server.namenode.TestNameNodeHttpServerXFrame#testSecondaryNameNodeXFrame;id_000001;[dfs.namenode.top.window.num.buckets=244111242];org.apache.hadoop.metrics2.MetricsException;Metrics source JvmMetrics already exists!;[org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newSourceName(DefaultMetricsSystem.java:152), org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.sourceName(DefaultMetricsSystem.java:125), org.apache.hadoop.metrics2.impl.MetricsSystemImpl.register(MetricsSystemImpl.java:229), org.apache.hadoop.metrics2.source.JvmMetrics.create(JvmMetrics.java:123), org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize(SecondaryNameNode.java:225)]\n']
[dfs.block.invalidate.limit,dfs.block.placement.ec.classname,dfs.client-write-packet-size,dfs.content-summary.limit,dfs.content-summary.sleep-microsec,dfs.datanode.peer.stats.enabled,dfs.hosts.exclude,dfs.image.parallel.load,dfs.namenode.accesstime.precision,dfs.namenode.acls.enabled,dfs.namenode.audit.log.async,dfs.namenode.audit.loggers,dfs.namenode.avoid.read.slow.datanode,dfs.namenode.avoid.read.stale.datanode,dfs.namenode.avoid.write.stale.datanode,dfs.namenode.blockreport.max.lock.hold.time,dfs.namenode.blockreport.queue.size,dfs.namenode.checkpoint.txns,dfs.namenode.datanode.registration.ip-hostname-check,dfs.namenode.delegation.token.max-lifetime,dfs.namenode.ec.policies.max.cellsize,dfs.namenode.edekcacheloader.initial.delay.ms,dfs.namenode.edit.log.autoroll.check.interval.ms,dfs.namenode.file.close.num-committed-allowed,dfs.namenode.fs-limits.max-xattrs-per-inode,dfs.namenode.fs-limits.min-block-size,dfs.namenode.invalidate.work.pct.per.iteration,dfs.namenode.lazypersist.file.scrub.interval.sec,dfs.namenode.lease-recheck-interval-ms,dfs.namenode.list.cache.directives.num.responses,dfs.namenode.list.openfiles.num.responses,dfs.namenode.max-lock-hold-to-release-lease-ms,dfs.namenode.num.extra.edits.retained,dfs.namenode.path.based.cache.block.map.allocation.percent,dfs.namenode.posix.acl.inheritance.enabled,dfs.namenode.read.considerLoad,dfs.namenode.reconstruction.pending.timeout-sec,dfs.namenode.redundancy.considerLoad,dfs.namenode.redundancy.considerLoad.factor,dfs.namenode.redundancy.considerLoadByStorageType,dfs.namenode.redundancy.queue.restart.iterations,dfs.namenode.reject-unresolved-dn-topology-mapping,dfs.namenode.replication.max-streams,dfs.namenode.replication.max-streams-hard-limit,dfs.namenode.replication.min,dfs.namenode.replication.work.multiplier.per.iteration,dfs.namenode.resource.check.interval,dfs.namenode.safemode.extension.testing,dfs.namenode.safemode.min.datanodes,dfs.namenode.safemode.threshold-pct,dfs.namenode.slowpeer.collect.interval,dfs.namenode.snapshot.capture.openfiles,dfs.namenode.snapshot.max.limit,dfs.namenode.snapshot.skip.capture.accesstime-only-change,dfs.namenode.snapshot.skiplist.max.levels,dfs.namenode.stale.datanode.minimum.interval,dfs.namenode.support.allow.format,dfs.namenode.top.enabled,dfs.namenode.top.num.users,dfs.namenode.write-lock-reporting-threshold-ms,dfs.namenode.xattrs.enabled,dfs.net.topology.impl,dfs.permissions.ContentSummary.subAccess,dfs.provided.storage.id,dfs.quota.by.storage.type.enabled,dfs.replication,dfs.use.dfs.network.topology,fs.trash.interval,hadoop.security.auth_to_local.mechanism,hadoop.security.authentication,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.group.mapping,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.ssl.client.conf,hadoop.user.group.static.mapping.overrides,io.file.buffer.size,ssl.client.truststore.location,ssl.client.truststore.type];, dfs.bytes-per-checksum=661313532, dfs.namenode.max.objects=2135812032, hadoop.ssl.hostname.verifier=DEFAULT, dfs.namenode.lease-hard-limit-sec=1116870743, dfs.namenode.list.encryption.zones.num.responses=616375367, dfs.namenode.audit.log.token.tracking.id=true, dfs.datanode.fsdataset.factory=org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory, ssl.client.keystore.type=jks, dfs.namenode.decommission.interval=735265238s, dfs.permissions.superusergroup=dxoonzbnwgkbwxyotmfmjdejztldsmkcqtuoubzlraxwtjnxpsngbgroup, dfs.namenode.path.based.cache.refresh.interval.ms=1359935293, dfs.block.replicator.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault, dfs.namenode.fs-limits.max-directory-items=69250813, dfs.namenode.heartbeat.recheck-interval=1422203581, dfs.namenode.safemode.extension=324064975, dfs.namenode.delegation.key.update-interval=548228156, dfs.namenode.redundancy.interval.seconds=1204170376, dfs.namenode.fs-limits.max-xattr-size=423615579, hdfs.minidfs.basedir=/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs, dfs.datanode.http.address=0.0.0.0:9864, dfs.namenode.blocks.per.postponedblocks.rescan=214993611, dfs.lock.suppress.warning.interval=20927d, dfs.namenode.maintenance.replication.min=1357712692, dfs.namenode.fs-limits.max-blocks-per-file=1770557638, dfs.namenode.hosts.provider.classname=org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager, dfs.namenode.decommission.interval.testing=735265238, dfs.client.block.write.replace-datanode-on-failure.enable=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.caller.context.max.size=1385027616, dfs.namenode.checkpoint.dir=file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namesecondary-0-1,file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namesecondary-0-2, dfs.namenode.top.windows.minutes=38439965,4977653,293, dfs.namenode.edits.dir.minimum=2021911067, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, dfs.namenode.startup=REGULAR, hadoop.kerberos.keytab.login.autorenewal.enabled=false, dfs.storage.policy.enabled=true, dfs.storage.policy.permissions.superuser-only=false, dfs.namenode.list.cache.pools.num.responses=993076183, fs.creation.parallel.count=1384556113, dfs.batched.ls.limit=226955644, dfs.namenode.ec.userdefined.policy.allowed=true, dfs.checksum.type=CRC32C, dfs.namenode.num.checkpoints.retained=386486612, dfs.namenode.fslock.fair=false, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.permissions.enabled=true, dfs.namenode.stale.datanode.interval=162098747, dfs.namenode.provided.enabled=false, dfs.namenode.snapshotdiff.allow.snap-root-descendant=false, hadoop.ssl.enabled.protocols=TLSv1.2, dfs.namenode.ec.system.default.policy=RS-6-3-1024k, dfs.replication.max=1400013791, dfs.namenode.name.dir=file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1,file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2, dfs.namenode.read-lock-reporting-threshold-ms=2112723020, dfs.datanode.https.address=0.0.0.0:9865, dfs.ha.standby.checkpoints=true, dfs.namenode.replqueue.threshold-pct=0.09896582, dfs.namenode.enable.log.stale.datanode=true, dfs.block.access.token.enable=true, dfs.blocksize=2028895542, hadoop.caller.context.signature.max.size=132109779, hadoop.security.dns.log-slow-lookups.enabled=true];1;['org.apache.hadoop.hdfs.server.namenode.TestGenericJournalConf#testNotConfigured;id_000001;[dfs.namenode.avoid.read.slow.datanode=true, dfs.namenode.read.considerLoad=false, hadoop.security.groups.cache.secs=1876123258, dfs.use.dfs.network.topology=true, dfs.namenode.resource.check.interval=1283974111, dfs.datanode.peer.stats.enabled=true, dfs.block.invalidate.limit=167610450, dfs.replication=0, dfs.namenode.snapshot.skiplist.max.levels=440790199, dfs.namenode.audit.log.async=true, dfs.namenode.write-lock-reporting-threshold-ms=515841529, dfs.namenode.avoid.read.stale.datanode=true, ssl.client.truststore.location=, dfs.namenode.lease-recheck-interval-ms=275300575, dfs.namenode.stale.datanode.minimum.interval=1693600493, dfs.namenode.ec.policies.max.cellsize=934800223, ssl.client.truststore.type=jks, hadoop.security.groups.cache.background.reload.threads=949354654, dfs.hosts.exclude=, dfs.namenode.replication.min=432919830, dfs.content-summary.sleep-microsec=2047538535, dfs.namenode.redundancy.considerLoad=false, dfs.provided.storage.id=DS-PROVIDED, dfs.namenode.fs-limits.min-block-size=1427629443, dfs.image.parallel.load=true, dfs.namenode.acls.enabled=false, dfs.namenode.datanode.registration.ip-hostname-check=false, dfs.namenode.path.based.cache.block.map.allocation.percent=0.9937185, dfs.namenode.slowpeer.collect.interval=80939042m, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, dfs.permissions.ContentSummary.subAccess=false, dfs.namenode.blockreport.queue.size=1967221007, dfs.namenode.audit.loggers=default, hadoop.security.groups.cache.warn.after.ms=1667550531, dfs.namenode.lazypersist.file.scrub.interval.sec=1492645148, hadoop.security.auth_to_local.mechanism=hadoop, dfs.namenode.blockreport.max.lock.hold.time=1033618384, dfs.namenode.snapshot.capture.openfiles=true, dfs.namenode.safemode.min.datanodes=418984441, dfs.namenode.snapshot.skip.capture.accesstime-only-change=true, dfs.namenode.delegation.token.max-lifetime=1685858645, dfs.namenode.avoid.write.stale.datanode=false, fs.trash.interval=2061973259, dfs.namenode.num.extra.edits.retained=546258939, dfs.block.placement.ec.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant, dfs.net.topology.impl=org.apache.hadoop.hdfs.net.DFSNetworkTopology, dfs.namenode.edekcacheloader.initial.delay.ms=1030544459, io.file.buffer.size=815164823, dfs.namenode.max-lock-hold-to-release-lease-ms=867619988, dfs.namenode.xattrs.enabled=true, dfs.client-write-packet-size=660254895, dfs.namenode.checkpoint.txns=1872526376, dfs.namenode.list.openfiles.num.responses=2069598872, dfs.namenode.safemode.threshold-pct=0.52722096, dfs.namenode.list.cache.directives.num.responses=1886376832, dfs.namenode.replication.max-streams=882404226, dfs.namenode.safemode.extension.testing=324064975, dfs.namenode.file.close.num-committed-allowed=787928911, dfs.quota.by.storage.type.enabled=false, dfs.namenode.reconstruction.pending.timeout-sec=1985135376, dfs.namenode.replication.work.multiplier.per.iteration=1982384664, dfs.namenode.redundancy.considerLoadByStorageType=false, hadoop.security.dns.log-slow-lookups.threshold.ms=430931300, dfs.namenode.reject-unresolved-dn-topology-mapping=false, hadoop.security.authentication=simple, dfs.namenode.replication.max-streams-hard-limit=659370209, dfs.namenode.top.num.users=1721679828, dfs.namenode.accesstime.precision=776955783, dfs.namenode.redundancy.considerLoad.factor=0.82100964, dfs.namenode.fs-limits.max-xattrs-per-inode=494026270, dfs.namenode.edit.log.autoroll.check.interval.ms=54466317, dfs.namenode.support.allow.format=true, dfs.namenode.redundancy.queue.restart.iterations=252520109, dfs.namenode.snapshot.max.limit=569580814, hadoop.ssl.client.conf=ssl-client.xml, dfs.namenode.invalidate.work.pct.per.iteration=0.87497882098730088167985778591364369368571755466329223799640601118477218545599354857555698569651620340891369f, dfs.content-summary.limit=513699053, hadoop.security.groups.cache.background.reload=false, dfs.namenode.posix.acl.inheritance.enabled=false, dfs.namenode.top.enabled=false, hadoop.user.group.static.mapping.overrides=dr.who=;, dfs.bytes-per-checksum=661313532, dfs.namenode.max.objects=2135812032, hadoop.ssl.hostname.verifier=DEFAULT, dfs.namenode.lease-hard-limit-sec=1116870743, dfs.namenode.list.encryption.zones.num.responses=616375367, dfs.namenode.audit.log.token.tracking.id=true, dfs.datanode.fsdataset.factory=org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory, ssl.client.keystore.type=jks, dfs.namenode.decommission.interval=735265238s, dfs.permissions.superusergroup=dxoonzbnwgkbwxyotmfmjdejztldsmkcqtuoubzlraxwtjnxpsngbgroup, dfs.namenode.path.based.cache.refresh.interval.ms=1359935293, dfs.block.replicator.classname=org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault, dfs.namenode.fs-limits.max-directory-items=69250813, dfs.namenode.heartbeat.recheck-interval=1422203581, dfs.namenode.safemode.extension=324064975, dfs.namenode.delegation.key.update-interval=548228156, dfs.namenode.redundancy.interval.seconds=1204170376, dfs.namenode.fs-limits.max-xattr-size=423615579, hdfs.minidfs.basedir=/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs, dfs.datanode.http.address=0.0.0.0:9864, dfs.namenode.blocks.per.postponedblocks.rescan=214993611, dfs.lock.suppress.warning.interval=20927d, dfs.namenode.maintenance.replication.min=1357712692, dfs.namenode.fs-limits.max-blocks-per-file=1770557638, dfs.namenode.hosts.provider.classname=org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager, dfs.namenode.decommission.interval.testing=735265238, dfs.client.block.write.replace-datanode-on-failure.enable=true, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.caller.context.max.size=1385027616, dfs.namenode.checkpoint.dir=file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namesecondary-0-1,file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namesecondary-0-2, dfs.namenode.top.windows.minutes=38439965,4977653,293, dfs.namenode.edits.dir.minimum=2021911067, net.topology.node.switch.mapping.impl=org.apache.hadoop.net.StaticMapping, dfs.namenode.startup=REGULAR, hadoop.kerberos.keytab.login.autorenewal.enabled=false, dfs.storage.policy.enabled=true, dfs.storage.policy.permissions.superuser-only=false, dfs.namenode.list.cache.pools.num.responses=993076183, fs.creation.parallel.count=1384556113, dfs.batched.ls.limit=226955644, dfs.namenode.ec.userdefined.policy.allowed=true, dfs.checksum.type=CRC32C, dfs.namenode.num.checkpoints.retained=386486612, dfs.namenode.fslock.fair=false, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, dfs.permissions.enabled=true, dfs.namenode.stale.datanode.interval=162098747, dfs.namenode.provided.enabled=false, dfs.namenode.snapshotdiff.allow.snap-root-descendant=false, hadoop.ssl.enabled.protocols=TLSv1.2, dfs.namenode.ec.system.default.policy=RS-6-3-1024k, dfs.replication.max=1400013791, dfs.namenode.name.dir=file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1,file:/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task9/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2, dfs.namenode.read-lock-reporting-threshold-ms=2112723020, dfs.datanode.https.address=0.0.0.0:9865, dfs.ha.standby.checkpoints=true, dfs.namenode.replqueue.threshold-pct=0.09896582, dfs.namenode.enable.log.stale.datanode=true, dfs.block.access.token.enable=true, dfs.blocksize=2028895542, hadoop.caller.context.signature.max.size=132109779, hadoop.security.dns.log-slow-lookups.enabled=true];java.lang.IllegalArgumentException;214801937;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[file.bytes-per-checksum];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker#testInvalidConfigurationValues;id_000000;[file.bytes-per-checksum=0];java.lang.IllegalArgumentException;Invalid value of fs.creation.parallel.count: 0;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:415), org.apache.hadoop.fs.FileSystem$Cache.<init>(FileSystem.java:3514), org.apache.hadoop.fs.FileSystem.<clinit>(FileSystem.java:206), org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.check(StorageLocationChecker.java:161), org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker.lambda$testInvalidConfigurationValues$0(TestStorageLocationChecker.java:231)]\n']
[dfs.datanode.volumes.replica-add.threadpool.size];java.io.FileNotFoundException;7;['org.apache.hadoop.fs.TestSWebHdfsFileContextMainOperations#testCreateFlagOverwriteNonExistingFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.FileNotFoundException;Non existing file: swebhdfs://localhost:44413/tmp/TestSWebHdfsFileContextMainOperations/test/testCreateFlagOverwriteNonExistingFile. Create option is not specified in [OVERWRITE];[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:184), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]\n', 'org.apache.hadoop.hdfs.TestViewDistributedFileSystemWithMountLinks#testListStatusOnNonMountedPath;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.FileNotFoundException;File /nonMount does not exist.;[org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:1104), org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:147), org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1175), org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1172), org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)]\n', 'org.apache.hadoop.fs.TestHDFSFileContextMainOperations#testCreateFlagAppendNonExistingFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=547805024871503486699384838517306988066425616180761];java.io.FileNotFoundException;failed to append to non-existent file /tmp/TestHDFSFileContextMainOperations/test/testCreateFlagAppendNonExistingFile on client DFSClient_NONMAPREDUCE_-1935696320_1;[org.apache.hadoop.hdfs.DFSClient.primitiveAppend(DFSClient.java:1301), org.apache.hadoop.hdfs.DFSClient.primitiveCreate(DFSClient.java:1323), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:105), org.apache.hadoop.fs.Hdfs.createInternal(Hdfs.java:60), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)]\n', 'org.apache.hadoop.fs.TestWebHdfsFileContextMainOperations#testCreateFlagOverwriteNonExistingFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=9461736209797066691024724216199486128601077001625835822473555378103];java.io.FileNotFoundException;Non existing file: webhdfs://localhost:37987/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task25/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/zm7Wnh74A5/test/testCreateFlagOverwriteNonExistingFile. Create option is not specified in [OVERWRITE];[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:184), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeWithMountTableConfigInHDFS#testListStatusOnNonMountedPath;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=55031433900407257291987];java.io.FileNotFoundException;File/Directory does not exist: /nonMount;[org.apache.hadoop.fs.viewfs.InodeTree.resolve(InodeTree.java:821), org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:603), org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeWithHdfsScheme.testListStatusOnNonMountedPath(TestViewFileSystemOverloadSchemeWithHdfsScheme.java:272), org.apache.hadoop.fs.viewfs.TestViewFSOverloadSchemeWithMountTableConfigInHDFS.testListStatusOnNonMountedPath$$CONFUZZ(TestViewFSOverloadSchemeWithMountTableConfigInHDFS.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n', 'org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeWithHdfsScheme#testListStatusOnNonMountedPath;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];java.io.FileNotFoundException;File/Directory does not exist: /nonMount;[org.apache.hadoop.fs.viewfs.InodeTree.resolve(InodeTree.java:821), org.apache.hadoop.fs.viewfs.ViewFileSystem.listStatus(ViewFileSystem.java:603), org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeWithHdfsScheme.testListStatusOnNonMountedPath(TestViewFileSystemOverloadSchemeWithHdfsScheme.java:272), org.apache.hadoop.fs.viewfs.TestViewFileSystemOverloadSchemeWithHdfsScheme.testListStatusOnNonMountedPath$$CONFUZZ(TestViewFileSystemOverloadSchemeWithHdfsScheme.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n', 'org.apache.hadoop.fs.TestSWebHdfsFileContextMainOperations#testCreateFlagAppendNonExistingFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6410741954320680444947951044637342984303229651967589100433186287735471336617281193];java.io.FileNotFoundException;Non existing file: swebhdfs://localhost:40967/tmp/TestSWebHdfsFileContextMainOperations/test/testCreateFlagAppendNonExistingFile. Create option is not specified in [APPEND];[org.apache.hadoop.fs.CreateFlag.validate(CreateFlag.java:184), org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1294), org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102), org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626), org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)]\n']
[dfs.namenode.delegation.token.always-use];java.lang.OutOfMemoryError;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestStandbyInProgressTail#testUndertailingWhileFailover;id_000000;[dfs.namenode.delegation.token.always-use=true];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n']
[dfs.datanode.volumes.replica-add.threadpool.size];org.junit.runners.model.TestTimedOutException;2;['org.apache.hadoop.hdfs.server.namenode.TestReencryptionWithKMS#testReencryptCommandsQueuedOrdering;id_000003;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.junit.runners.model.TestTimedOutException;test timed out after 180000 milliseconds;[org.eclipse.jetty.util.component.ContainerLifeCycle.addBean(ContainerLifeCycle.java:336), org.eclipse.jetty.util.component.ContainerLifeCycle.addBean(ContainerLifeCycle.java:313), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:260), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)]\n', 'org.apache.hadoop.hdfs.TestEncryptionZonesWithKMS#testBasicOperations;id_000001;[dfs.datanode.volumes.replica-add.threadpool.size=67753310443822];org.junit.runners.model.TestTimedOutException;test timed out after 120000 milliseconds;[org.eclipse.jetty.util.component.ContainerLifeCycle.addBean(ContainerLifeCycle.java:336), org.eclipse.jetty.util.component.ContainerLifeCycle.addBean(ContainerLifeCycle.java:313), org.eclipse.jetty.io.SelectorManager.doStart(SelectorManager.java:260), org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73), org.eclipse.jetty.util.component.ContainerLifeCycle.start(ContainerLifeCycle.java:169)]\n']
[dfs.namenode.checkpoint.period,dfs.namenode.delegation.token.always-use,hadoop.caller.context.enabled];java.io.IOException;1;['org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testCrashWhenWritingVersionFiles;id_000001;[hadoop.caller.context.enabled=true, dfs.namenode.delegation.token.always-use=true, dfs.namenode.checkpoint.period=188749650];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.saveNamespaceWithInjectedFault(TestSaveNamespace.java:155)]\n']
[dfs.namenode.delegation.token.always-use,dfs.permissions.allow.owner.set.quota];java.io.IOException;1;['org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testCrashWhenWritingVersionFileInOneDir;id_000000;[dfs.namenode.delegation.token.always-use=true, dfs.permissions.allow.owner.set.quota=false];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace.saveNamespaceWithInjectedFault(TestSaveNamespace.java:155)]\n']
[dfs.namenode.delegation.token.always-use,hadoop.caller.context.enabled];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testCrashWhenWritingVersionFileInOneDir;id_000001;[hadoop.caller.context.enabled=false, dfs.namenode.delegation.token.always-use=true];java.lang.IllegalArgumentException;825732837;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[dfs.datanode.disk.check.timeout];org.apache.hadoop.HadoopIllegalArgumentException;4;['org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker#testFailedLocationsBelowThreshold;id_000000;[dfs.datanode.disk.check.timeout=00m];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.disk.check.timeout - 0 (should be > 0);[org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.<init>(StorageLocationChecker.java:98), org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker.testFailedLocationsBelowThreshold(TestStorageLocationChecker.java:94), org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker.testFailedLocationsBelowThreshold$$CONFUZZ(TestStorageLocationChecker.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.datanode.checker.TestDatasetVolumeCheckerFailures#testCheckingClosedVolume;id_000000;[dfs.datanode.disk.check.timeout=0s];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.disk.check.timeout - 0 (should be > 0);[org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.<init>(DatasetVolumeChecker.java:125), org.apache.hadoop.hdfs.server.datanode.checker.TestDatasetVolumeCheckerFailures.testCheckingClosedVolume(TestDatasetVolumeCheckerFailures.java:104), org.apache.hadoop.hdfs.server.datanode.checker.TestDatasetVolumeCheckerFailures.testCheckingClosedVolume$$CONFUZZ(TestDatasetVolumeCheckerFailures.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.datanode.checker.TestDatasetVolumeCheckerFailures#testMinGapIsEnforcedForSyncChecks;id_000000;[dfs.datanode.disk.check.timeout=0h];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.disk.check.timeout - 0 (should be > 0);[org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker.<init>(DatasetVolumeChecker.java:125), org.apache.hadoop.hdfs.server.datanode.checker.TestDatasetVolumeCheckerFailures.testMinGapIsEnforcedForSyncChecks(TestDatasetVolumeCheckerFailures.java:120), org.apache.hadoop.hdfs.server.datanode.checker.TestDatasetVolumeCheckerFailures.testMinGapIsEnforcedForSyncChecks$$CONFUZZ(TestDatasetVolumeCheckerFailures.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n', 'org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker#testAllLocationsHealthy;id_000000;[dfs.datanode.disk.check.timeout=0d];org.apache.hadoop.HadoopIllegalArgumentException;Invalid value configured for dfs.datanode.disk.check.timeout - 0 (should be > 0);[org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.<init>(StorageLocationChecker.java:98), org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker.testAllLocationsHealthy(TestStorageLocationChecker.java:69), org.apache.hadoop.hdfs.server.datanode.checker.TestStorageLocationChecker.testAllLocationsHealthy$$CONFUZZ(TestStorageLocationChecker.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n']
[dfs.namenode.max.full.block.report.leases];java.lang.IllegalArgumentException;4;['org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock#testAviodSlowDatanodes;id_000000;[dfs.namenode.max.full.block.report.leases=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.mockDatanodeManager(TestSortLocatedBlock.java:308), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodSlowDatanodes(TestSortLocatedBlock.java:225)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderStorageType;id_000000;[dfs.namenode.max.full.block.report.leases=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderStorageType(TestDatanodeManager.java:688)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testRemoveIncludedNode;id_000002;[dfs.namenode.max.full.block.report.leases=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:89), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testRemoveIncludedNode(TestDatanodeManager.java:858)]\n', 'org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock#testAviodStaleAndSlowDatanodes;id_000000;[dfs.namenode.max.full.block.report.leases=0];java.lang.IllegalArgumentException;Cannot set the maximum number of block report leases to a value less than 1.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:170), org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager.<init>(BlockReportLeaseManager.java:161), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.mockDatanodeManager(TestSortLocatedBlock.java:308), org.apache.hadoop.hdfs.server.blockmanagement.TestSortLocatedBlock.testAviodStaleAndSlowDatanodes(TestSortLocatedBlock.java:135)]\n']
[dfs.client.block.reader.remote.buffer.size];java.io.IOException;1;['org.apache.hadoop.hdfs.TestLeaseRecovery2#testHardLeaseRecovery;id_000000;[dfs.client.block.reader.remote.buffer.size=2089061880];java.io.IOException;Timed out waiting for Mini HDFS Cluster to start;[org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp(MiniDFSCluster.java:1503), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:973), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.TestLeaseRecovery2.startUp(TestLeaseRecovery2.java:103)]\n']
[dfs.datanode.cache.revocation.polling.ms];java.io.IOException;1;['org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics#testReadWriteOps;id_000000;[dfs.datanode.cache.revocation.polling.ms=1187355044];java.io.IOException;Timed out waiting for Mini HDFS Cluster to start;[org.apache.hadoop.hdfs.MiniDFSCluster.waitClusterUp(MiniDFSCluster.java:1503), org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:973), org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:576), org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:518), org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.setUp(TestNameNodeMetrics.java:166)]\n']
[dfs.client.short.circuit.num];org.apache.hadoop.hdfs.BlockMissingException;1;['org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics#testReadWriteOps;id_000001;[dfs.client.short.circuit.num=1922275450];org.apache.hadoop.hdfs.BlockMissingException;Could not obtain block: BP-911616442-10.0.0.22-1674627228655:blk_1073741825_1001 file=/testNameNodeMetrics/ReadData.dat No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:38073,DS-181ad3b7-1c6a-4dc0-9b3f-7feef892d3a8,DISK] DatanodeInfoWithStorage[127.0.0.1:35337,DS-cc76e43d-73c1-4911-abe6-36f2d1249d2e,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:35337,DS-cc76e43d-73c1-4911-abe6-36f2d1249d2e,DISK] DatanodeInfoWithStorage[127.0.0.1:38073,DS-181ad3b7-1c6a-4dc0-9b3f-7feef892d3a8,DISK];[org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007), org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990), org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969), org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677), org.apache.hadoop.hdfs.DFSInputStream.seekToNewSource(DFSInputStream.java:1665)]\n']
[dfs.datanode.ipc.address,dfs.namenode.delegation.token.always-use,dfs.namenode.fs-limits.max-component-length,dfs.namenode.snapshot.skiplist.interval];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.server.namenode.TestSaveNamespace#testRTEWhileSavingSecondImage;id_000000;[dfs.namenode.fs-limits.max-component-length=1810896824, dfs.datanode.ipc.address=0.0.0.0:9867, dfs.namenode.snapshot.skiplist.interval=2135163407, dfs.namenode.delegation.token.always-use=true];java.lang.IllegalArgumentException;2098432590;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.server.namenode.NNStorage.setStorageDirectories(NNStorage.java:323), org.apache.hadoop.hdfs.server.namenode.NNStorage.<init>(NNStorage.java:179)]\n']
[dfs.client.block.write.replace-datanode-on-failure.policy];java.lang.AssertionError;1;['org.apache.hadoop.hdfs.TestReplaceDatanodeOnFailure#testDefaultPolicy;id_000000;[dfs.client.block.write.replace-datanode-on-failure.policy=ALWAYS];java.lang.AssertionError;expected:<false> but was:<true>;[org.junit.Assert.fail(Assert.java:89), org.junit.Assert.failNotEquals(Assert.java:835), org.junit.Assert.assertEquals(Assert.java:120), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.TestReplaceDatanodeOnFailure.testDefaultPolicy(TestReplaceDatanodeOnFailure.java:103)]\n']
[dfs.datanode.volumes.replica-add.threadpool.size];org.apache.hadoop.ipc.RemoteException;10;['org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl#testSetAclDefaultOnFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.ipc.RemoteException;Invalid ACL: only directories may have a default ACL. Path: /p1 Path: /p1,at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.setAcl(FSDirAclOp.java:148),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setAcl(FSNamesystem.java:7728),at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setAcl(NameNodeRpcServer.java:2182),at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setAcl(ClientNamenodeProtocolServerSideTranslatorPB.java:1565),at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556),at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971),at java.base/java.security.AccessController.doPrivileged(Native Method),at java.base/javax.security.auth.Subject.doAs(Subject.java:423),at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878),at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976),Caused by: org.apache.hadoop.hdfs.protocol.AclException: Invalid ACL: only directories may have a default ACL. Path: /p1,at org.apache.hadoop.hdfs.server.namenode.AclStorage.updateINodeAcl(AclStorage.java:285),at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.unprotectedSetAcl(FSDirAclOp.java:198),at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.setAcl(FSDirAclOp.java:145),... 14 more,;[org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFSAcl#testSetAclDefaultOnFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=5161269549247641821298455884];org.apache.hadoop.ipc.RemoteException;Invalid ACL: only directories may have a default ACL. Path: /p1 Path: /p1;[org.apache.hadoop.hdfs.web.JsonUtilClient.toRemoteException(JsonUtilClient.java:89), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:522), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$300(WebHdfsFileSystem.java:145), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:739), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:814)]\n', 'org.apache.hadoop.hdfs.web.TestWebHDFSAcl#testModifyAclEntriesDefaultOnFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=101332400044905542824430650662266188553952492885585554463904671791408306353];org.apache.hadoop.ipc.RemoteException;Invalid ACL: only directories may have a default ACL. Path: /p1 Path: /p1;[org.apache.hadoop.hdfs.web.JsonUtilClient.toRemoteException(JsonUtilClient.java:89), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.validateResponse(WebHdfsFileSystem.java:522), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.access$300(WebHdfsFileSystem.java:145), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.connect(WebHdfsFileSystem.java:739), org.apache.hadoop.hdfs.web.WebHdfsFileSystem$AbstractRunner.runWithRetry(WebHdfsFileSystem.java:814)]\n', 'org.apache.hadoop.fs.TestHDFSFileContextMainOperations#testCreateFlagCreateExistingFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=4960203288983686867544152009293298530676708337758424495990735980];org.apache.hadoop.ipc.RemoteException;/tmp/TestHDFSFileContextMainOperations/test/testCreateFlagCreateExistingFile for client 127.0.0.1 already exists,at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.startFile(FSDirWriteFileOp.java:389),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2703),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596),at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799),at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494),at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556),at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971),at java.base/java.security.AccessController.doPrivileged(Native Method),at java.base/javax.security.auth.Subject.doAs(Subject.java:423),at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878),at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976),;[org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)]\n', 'org.apache.hadoop.fs.TestHDFSFileContextMainOperations#testRenameRoot;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.ipc.RemoteException;rename source cannot be the root,at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.validateRenameSource(FSDirRenameOp.java:595),at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.unprotectedRenameTo(FSDirRenameOp.java:378),at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameTo(FSDirRenameOp.java:311),at org.apache.hadoop.hdfs.server.namenode.FSDirRenameOp.renameToInt(FSDirRenameOp.java:256),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameTo(FSNamesystem.java:3225),at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rename2(NameNodeRpcServer.java:1088),at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rename2(ClientNamenodeProtocolServerSideTranslatorPB.java:701),at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556),at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971),at java.base/java.security.AccessController.doPrivileged(Native Method),at java.base/javax.security.auth.Subject.doAs(Subject.java:423),at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878),at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976),;[org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)]\n', "org.apache.hadoop.fs.TestHDFSFileContextMainOperations#testCreateFlagOverwriteNonExistingFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=5389712548194392923068749144131387853298220386374475986441529623262714666220015241];org.apache.hadoop.ipc.RemoteException;Can't overwrite non-existent /tmp/TestHDFSFileContextMainOperations/test/testCreateFlagOverwriteNonExistingFile,at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:342),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2656),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596),at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799),at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494),at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556),at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971),at java.base/java.security.AccessController.doPrivileged(Native Method),at java.base/javax.security.auth.Subject.doAs(Subject.java:423),at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878),at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976),;[org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)]\n", 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestDisallowModifyROSnapshot#testCreate;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=92806878407382031422888049566613272126404];org.apache.hadoop.ipc.RemoteException;Modification on a read-only snapshot is disallowed,at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1913),at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:727),at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.resolvePathForStartFile(FSDirWriteFileOp.java:321),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2656),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2596),at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:799),at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:494),at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556),at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971),at java.base/java.security.AccessController.doPrivileged(Native Method),at java.base/javax.security.auth.Subject.doAs(Subject.java:423),at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878),at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976),;[org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)]\n', 'org.apache.hadoop.hdfs.server.namenode.snapshot.TestDisallowModifyROSnapshot#testCreateSymlink;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=0];org.apache.hadoop.ipc.RemoteException;Modification on a read-only snapshot is disallowed,at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1913),at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:727),at org.apache.hadoop.hdfs.server.namenode.FSDirSymlinkOp.createSymlinkInt(FSDirSymlinkOp.java:59),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlink(FSNamesystem.java:2309),at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.createSymlink(NameNodeRpcServer.java:1543),at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.createSymlink(ClientNamenodeProtocolServerSideTranslatorPB.java:1142),at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556),at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971),at java.base/java.security.AccessController.doPrivileged(Native Method),at java.base/javax.security.auth.Subject.doAs(Subject.java:423),at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878),at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976),;[org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestFileContextAcl#testModifyAclEntriesDefaultOnFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=6402329];org.apache.hadoop.ipc.RemoteException;Invalid ACL: only directories may have a default ACL. Path: /p1 Path: /p1,at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.modifyAclEntries(FSDirAclOp.java:57),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.modifyAclEntries(FSNamesystem.java:7635),at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.modifyAclEntries(NameNodeRpcServer.java:2157),at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.modifyAclEntries(ClientNamenodeProtocolServerSideTranslatorPB.java:1518),at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556),at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971),at java.base/java.security.AccessController.doPrivileged(Native Method),at java.base/javax.security.auth.Subject.doAs(Subject.java:423),at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878),at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976),Caused by: org.apache.hadoop.hdfs.protocol.AclException: Invalid ACL: only directories may have a default ACL. Path: /p1,at org.apache.hadoop.hdfs.server.namenode.AclStorage.updateINodeAcl(AclStorage.java:285),at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.modifyAclEntries(FSDirAclOp.java:54),... 14 more,;[org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNameNodeAcl#testSetAclDefaultOnFile;id_000000;[dfs.datanode.volumes.replica-add.threadpool.size=1748331861695709778485677112724764149064044831785683492693513587017377846];org.apache.hadoop.ipc.RemoteException;Invalid ACL: only directories may have a default ACL. Path: /p1 Path: /p1,at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.setAcl(FSDirAclOp.java:148),at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setAcl(FSNamesystem.java:7728),at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setAcl(NameNodeRpcServer.java:2182),at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setAcl(ClientNamenodeProtocolServerSideTranslatorPB.java:1565),at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:604),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:572),at org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine2.java:556),at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1093),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1043),at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:971),at java.base/java.security.AccessController.doPrivileged(Native Method),at java.base/javax.security.auth.Subject.doAs(Subject.java:423),at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878),at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2976),Caused by: org.apache.hadoop.hdfs.protocol.AclException: Invalid ACL: only directories may have a default ACL. Path: /p1,at org.apache.hadoop.hdfs.server.namenode.AclStorage.updateINodeAcl(AclStorage.java:285),at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.unprotectedSetAcl(FSDirAclOp.java:198),at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.setAcl(FSDirAclOp.java:145),... 14 more,;[org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612), org.apache.hadoop.ipc.Client.call(Client.java:1558), org.apache.hadoop.ipc.Client.call(Client.java:1455), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242), org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)]\n']
[dfs.client.short.circuit.num];org.junit.internal.runners.model.MultipleFailureException;3;['org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics#testCorruptBlock;id_000000;[dfs.client.short.circuit.num=174459284];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.util.concurrent.TimeoutException(Timed out waiting for condition. ,Thread diagnostics:,Timestamp: 2023-01-25 06:33:33,813,,"IPC Server handler 6 on default port 40077" daemon prio=5 tid=149 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"StorageLocationChecker thread 0" daemon prio=5 tid=191 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"nioEventLoopGroup-8-1"  prio=10 tid=209 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68),        at app//io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:810),        at app//io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:457),        at app//io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986),        at app//io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74),        at app//io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"StorageLocationChecker thread 0" daemon prio=5 tid=116 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp308667396-126" daemon prio=5 tid=126 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183),        at app//org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137),        at app//org.eclipse.jetty.io.ManagedSelector$$Lambda$221/0x0000000840370440.run(Unknown Source),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp311808843-206" daemon prio=5 tid=206 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Session-HouseKeeper-659c84b8-1"  prio=5 tid=169 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Session-HouseKeeper-1a8b8898-1"  prio=5 tid=130 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server listener on 0" daemon prio=5 tid=213 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener.run(Server.java:1358),"nioEventLoopGroup-2-1"  prio=10 tid=95 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68),        at app//io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:810),        at app//io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:457),        at app//io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986),        at app//io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74),        at app//io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor@7f99a0a7" daemon prio=5 tid=48 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager$Monitor.run(HeartbeatManager.java:542),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server listener on 0" daemon prio=5 tid=97 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener.run(Server.java:1358),"IPC Server handler 8 on default port 46829" daemon prio=5 tid=231 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"process reaper" daemon prio=10 tid=11 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 7 on default port 44243" daemon prio=5 tid=113 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"Session-HouseKeeper-34f862e3-1"  prio=5 tid=208 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner" daemon prio=5 tid=15 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:155),        at java.base@11.0.17/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:176),        at app//org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:4021),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"VolumeScannerThread(/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3)" daemon prio=5 tid=236 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:656),"VolumeScannerThread(/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6)" daemon prio=5 tid=242 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:656),"refreshUsed-/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data3/current/BP-1753835085-10.0.0.67-1674628375238" daemon prio=5 tid=252 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:205),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 9 on default port 40077" daemon prio=5 tid=152 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"qtp1911144089-166-acceptor-0@5097eec5-ServerConnector@23d0c264{HTTP/1.1, (http/1.1)}{localhost:46015}" daemon prio=3 tid=166 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:533),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:285),        at app//org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388),        at app//org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:702),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"refreshUsed-/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8/current/BP-1753835085-10.0.0.67-1674628375238" daemon prio=5 tid=262 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:205),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server Responder" daemon prio=5 tid=136 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:136),        at app//org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1532),        at app//org.apache.hadoop.ipc.Server$Responder.run(Server.java:1515),"IPC Server handler 7 on default port 40077" daemon prio=5 tid=150 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"org.apache.hadoop.util.JvmPauseMonitor$Monitor@23d29fb9" daemon prio=5 tid=210 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"refreshUsed-/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5/current/BP-1753835085-10.0.0.67-1674628375238" daemon prio=5 tid=265 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:205),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"StorageLocationChecker thread 1" daemon prio=5 tid=154 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 1 on default port 46429" daemon prio=5 tid=182 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"Socket Reader #1 for port 0"  prio=5 tid=214 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1296),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1275),"MarkedDeleteBlockScrubberThread" daemon prio=5 tid=47 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$MarkedDeleteBlockScrubber.run(BlockManager.java:4963),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-17-thread-1"  prio=5 tid=105 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"DataNode DiskChecker thread 0" daemon prio=5 tid=278 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@79a17158" daemon prio=5 tid=124 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:533),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:285),        at java.base@11.0.17/sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:114),        at app//org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:85),        at app//org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:229),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp308667396-128" daemon prio=5 tid=128 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-13-thread-1" daemon prio=5 tid=283 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"nioEventLoopGroup-6-1"  prio=10 tid=170 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68),        at app//io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:810),        at app//io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:457),        at app//io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986),        at app//io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74),        at app//io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 1 on default port 46829" daemon prio=5 tid=224 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"nioEventLoopGroup-4-1"  prio=10 tid=131 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68),        at app//io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:810),        at app//io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:457),        at app//io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986),        at app//io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74),        at app//io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Socket Reader #1 for port 0"  prio=5 tid=173 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1296),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1275),"Session-HouseKeeper-6a72ad14-1"  prio=5 tid=94 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Timer for \'NameNode\' metrics system" daemon prio=5 tid=32 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.util.TimerThread.mainLoop(Timer.java:553),        at java.base@11.0.17/java.util.TimerThread.run(Timer.java:506),"IPC Server handler 0 on default port 40077" daemon prio=5 tid=143 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"pool-32-thread-1"  prio=5 tid=180 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Parameter Sending Thread #0" daemon prio=5 tid=157 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server idle connection scanner for port 46429" daemon prio=5 tid=174 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.util.TimerThread.mainLoop(Timer.java:553),        at java.base@11.0.17/java.util.TimerThread.run(Timer.java:506),"IPC Server handler 7 on default port 46429" daemon prio=5 tid=188 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"Finalizer" daemon prio=8 tid=3 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:155),        at java.base@11.0.17/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:176),        at java.base@11.0.17/java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:170),"DataNode DiskChecker thread 0" daemon prio=5 tid=280 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"StorageLocationChecker thread 1" daemon prio=5 tid=80 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-41-thread-1"  prio=5 tid=200 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 3 on default port 40341" daemon prio=5 tid=65 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 6 on default port 40341" daemon prio=5 tid=68 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 7 on default port 40341" daemon prio=5 tid=69 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 8 on default port 40341" daemon prio=5 tid=70 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 3 on default port 46429" daemon prio=5 tid=184 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server idle connection scanner for port 46829" daemon prio=5 tid=216 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.util.TimerThread.mainLoop(Timer.java:553),        at java.base@11.0.17/java.util.TimerThread.run(Timer.java:506),"IPC Server listener on 0" daemon prio=5 tid=133 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener.run(Server.java:1358),"IPC Server handler 6 on default port 46829" daemon prio=5 tid=229 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"qtp311808843-207" daemon prio=5 tid=207 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"CacheReplicationMonitor(552605285)"  prio=5 tid=78 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2211),        at app//org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor.run(CacheReplicationMonitor.java:181),"qtp1474125899-38" daemon prio=5 tid=38 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"FSEditLogAsync"  prio=5 tid=52 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081),        at java.base@11.0.17/java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:417),        at app//org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync.dequeueEdit(FSEditLogAsync.java:235),        at app//org.apache.hadoop.hdfs.server.namenode.FSEditLogAsync.run(FSEditLogAsync.java:244),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.util.JvmPauseMonitor$Monitor@426bbfb4" daemon prio=5 tid=33 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"DataNode DiskChecker thread 0" daemon prio=5 tid=279 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"MutableQuantiles-0" daemon prio=5 tid=14 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Command processor" daemon prio=5 tid=140 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1331),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.run(BPServiceActor.java:1315),"qtp1527195473-92" daemon prio=5 tid=92 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Client (643123282) connection to localhost/127.0.0.1:40341 from root" daemon prio=5 tid=137 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:1086),        at app//org.apache.hadoop.ipc.Client$Connection.run(Client.java:1133),"qtp1474125899-36" daemon prio=5 tid=36 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183),        at app//org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137),        at app//org.eclipse.jetty.io.ManagedSelector$$Lambda$221/0x0000000840370440.run(Unknown Source),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 1 on default port 40077" daemon prio=5 tid=144 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 9 on default port 40341" daemon prio=5 tid=71 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"org.apache.hadoop.util.JvmPauseMonitor$Monitor@6dad9ae6" daemon prio=5 tid=96 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"VolumeScannerThread(/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4)" daemon prio=5 tid=241 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:656),"qtp311808843-205-acceptor-0@2d1133f6-ServerConnector@3fc755ff{HTTP/1.1, (http/1.1)}{localhost:43577}" daemon prio=3 tid=205 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:533),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:285),        at app//org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388),        at app//org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:702),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Command processor" daemon prio=5 tid=220 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1331),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.run(BPServiceActor.java:1315),"qtp1474125899-43" daemon prio=5 tid=43 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-5-thread-1"  prio=5 tid=35 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-6-thread-1"  prio=5 tid=45 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"StorageLocationChecker thread 1" daemon prio=5 tid=192 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Block report processor" daemon prio=5 tid=50 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081),        at java.base@11.0.17/java.util.concurrent.ArrayBlockingQueue.take(ArrayBlockingQueue.java:417),        at app//org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.processQueue(BlockManager.java:5261),        at app//org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$BlockReportProcessingThread.run(BlockManager.java:5250),"IPC Server handler 4 on default port 40341" daemon prio=5 tid=66 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"GcTimeMonitor obsWindow = 60000, sleepInterval = 5000, maxGcTimePerc = 100" daemon prio=5 tid=34 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.util.GcTimeMonitor.run(GcTimeMonitor.java:155),"IPC Server handler 9 on default port 46429" daemon prio=5 tid=190 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@3c2eaed5" daemon prio=5 tid=88 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:533),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:285),        at java.base@11.0.17/sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:114),        at app//org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:85),        at app//org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:229),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server idle connection scanner for port 44243" daemon prio=5 tid=99 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.util.TimerThread.mainLoop(Timer.java:553),        at java.base@11.0.17/java.util.TimerThread.run(Timer.java:506),"VolumeScannerThread(/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)" daemon prio=5 tid=240 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:656),"IPC Server handler 5 on default port 46829" daemon prio=5 tid=228 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"Command processor" daemon prio=5 tid=178 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1331),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.run(BPServiceActor.java:1315),"SSL Certificates Store Monitor" daemon prio=5 tid=24 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.lang.Object.wait(Object.java:328),        at java.base@11.0.17/java.util.TimerThread.mainLoop(Timer.java:527),        at java.base@11.0.17/java.util.TimerThread.run(Timer.java:506),"IPC Server handler 8 on default port 46429" daemon prio=5 tid=189 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 9 on default port 46829" daemon prio=5 tid=232 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"Command processor" daemon prio=5 tid=103 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1331),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.run(BPServiceActor.java:1315),"pool-10-thread-1"  prio=5 tid=72 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server Responder" daemon prio=5 tid=100 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:136),        at app//org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1532),        at app//org.apache.hadoop.ipc.Server$Responder.run(Server.java:1515),"pool-21-thread-1" daemon prio=5 tid=285 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp308667396-129" daemon prio=5 tid=129 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"java.util.concurrent.ThreadPoolExecutor$Worker@38f7d537[State = -1, empty queue]" daemon prio=5 tid=288 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp311808843-201" daemon prio=5 tid=201 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183),        at app//org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137),        at app//org.eclipse.jetty.io.ManagedSelector$$Lambda$221/0x0000000840370440.run(Unknown Source),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"ForkJoinPool-2-worker-1" daemon prio=5 tid=273 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194),        at java.base@11.0.17/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1628),        at java.base@11.0.17/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183),"Socket Reader #1 for port 0"  prio=5 tid=98 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1296),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1275),"qtp1527195473-91-acceptor-0@3eb75f96-ServerConnector@4c489d5c{HTTP/1.1, (http/1.1)}{localhost:36413}" daemon prio=3 tid=91 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:533),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:285),        at app//org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388),        at app//org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:702),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server Responder" daemon prio=5 tid=217 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:136),        at app//org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1532),        at app//org.apache.hadoop.ipc.Server$Responder.run(Server.java:1515),"Session-HouseKeeper-4529cb82-1"  prio=5 tid=44 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"StorageLocationChecker thread 1" daemon prio=5 tid=117 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 2 on default port 40077" daemon prio=5 tid=145 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"org.apache.hadoop.util.JvmPauseMonitor$Monitor@1e8fd0b" daemon prio=5 tid=171 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 3 on default port 40077" daemon prio=5 tid=146 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"pool-28-thread-1" daemon prio=5 tid=282 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Socket Reader #1 for port 0"  prio=5 tid=54 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1296),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1275),"qtp1527195473-90" daemon prio=5 tid=90 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183),        at app//org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137),        at app//org.eclipse.jetty.io.ManagedSelector$$Lambda$221/0x0000000840370440.run(Unknown Source),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor@d08b73b" daemon prio=5 tid=74 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor.run(LeaseManager.java:537),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"DataNode DiskChecker thread 0" daemon prio=5 tid=281 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 0 on default port 46429" daemon prio=5 tid=181 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"VolumeScannerThread(/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data8)" daemon prio=5 tid=239 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:656),"process reaper" daemon prio=10 tid=82 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-42-thread-1"  prio=5 tid=222 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server Responder" daemon prio=5 tid=56 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:136),        at app//org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1532),        at app//org.apache.hadoop.ipc.Server$Responder.run(Server.java:1515),"IPC Server idle connection scanner for port 40077" daemon prio=5 tid=135 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.util.TimerThread.mainLoop(Timer.java:553),        at java.base@11.0.17/java.util.TimerThread.run(Timer.java:506),"java.util.concurrent.ThreadPoolExecutor$Worker@5f14ed6[State = -1, empty queue]" daemon prio=5 tid=291 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"VolumeScannerThread(/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)" daemon prio=5 tid=237 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:656),"IPC Server handler 2 on default port 46429" daemon prio=5 tid=183 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"Reference Handler" daemon prio=10 tid=2 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/java.lang.ref.Reference.waitForReferencePendingList(Native Method),        at java.base@11.0.17/java.lang.ref.Reference.processPendingReferences(Reference.java:241),        at java.base@11.0.17/java.lang.ref.Reference$ReferenceHandler.run(Reference.java:213),"BP-1753835085-10.0.0.67-1674628375238 heartbeating to localhost/127.0.0.1:40341" daemon prio=5 tid=221 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager.waitTillNextIBR(IncrementalBlockReportManager.java:158),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:739),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:879),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-25-thread-1"  prio=5 tid=142 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp1911144089-168" daemon prio=5 tid=168 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor@52ac8f34" daemon prio=5 tid=75 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeResourceMonitor.run(FSNamesystem.java:4384),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 3 on default port 46829" daemon prio=5 tid=226 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Parameter Sending Thread #1" daemon prio=5 tid=159 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 1 on default port 40341" daemon prio=5 tid=63 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 0 on default port 40341" daemon prio=5 tid=62 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 6 on default port 46429" daemon prio=5 tid=187 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"java.util.concurrent.ThreadPoolExecutor$Worker@2b8e2547[State = -1, empty queue]" daemon prio=5 tid=308 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"BP-1753835085-10.0.0.67-1674628375238 heartbeating to localhost/127.0.0.1:40341" daemon prio=5 tid=104 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager.waitTillNextIBR(IncrementalBlockReportManager.java:158),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:739),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:879),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 0 on default port 46829" daemon prio=5 tid=223 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"Common-Cleaner" daemon prio=8 tid=9 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:155),        at java.base@11.0.17/jdk.internal.ref.CleanerImpl.run(CleanerImpl.java:148),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),        at java.base@11.0.17/jdk.internal.misc.InnocuousThread.run(InnocuousThread.java:161),"IPC Server handler 4 on default port 40077" daemon prio=5 tid=147 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 2 on default port 40341" daemon prio=5 tid=64 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 5 on default port 40341" daemon prio=5 tid=67 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"refreshUsed-/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-1753835085-10.0.0.67-1674628375238" daemon prio=5 tid=267 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:205),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@42d6f75e" daemon prio=5 tid=77 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber.run(FSNamesystem.java:4523),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"StorageLocationChecker thread 0" daemon prio=5 tid=79 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 5 on default port 40077" daemon prio=5 tid=148 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server listener on 0" daemon prio=5 tid=53 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener.run(Server.java:1358),"qtp308667396-127-acceptor-0@3142bef2-ServerConnector@342af214{HTTP/1.1, (http/1.1)}{localhost:42297}" daemon prio=3 tid=127 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:533),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:285),        at app//org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388),        at app//org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:702),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"ForkJoinPool-2-worker-3" daemon prio=5 tid=272 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkUntil(LockSupport.java:275),        at java.base@11.0.17/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1619),        at java.base@11.0.17/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183),"IPC Client (643123282) connection to localhost/127.0.0.1:40341 from root" daemon prio=5 tid=299 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:1086),        at app//org.apache.hadoop.ipc.Client$Connection.run(Client.java:1133),"IPC Server handler 0 on default port 44243" daemon prio=5 tid=106 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"Listener at 127.0.0.1/46829"  prio=5 tid=1 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/java.lang.Thread.dumpThreads(Native Method),        at java.base@11.0.17/java.lang.Thread.getAllStackTraces(Thread.java:1653),        at app//org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:87),        at app//org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:73),        at app//org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:461),        at app//org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:421),        at app//org.apache.hadoop.hdfs.DFSTestUtil.waitForReplication(DFSTestUtil.java:608),        at app//org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCorruptBlock(TestNameNodeMetrics.java:525),        at app//org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCorruptBlock$$CONFUZZ(TestNameNodeMetrics.java),        at java.base@11.0.17/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method),        at java.base@11.0.17/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62),        at java.base@11.0.17/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43),        at java.base@11.0.17/java.lang.reflect.Method.invoke(Method.java:566),        at app//org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59),        at app//org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12),        at app//org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56),        at app//edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59),        at app//org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26),        at app//org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27),        at app//org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),        at app//edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65),        at app//edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222),        at app//edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144),        at app//org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100),        at app//org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366),        at app//org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103),        at app//org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),        at app//org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),        at app//org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79),        at app//org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),        at app//org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),        at app//org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293),        at app//org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),        at app//org.junit.runners.ParentRunner.run(ParentRunner.java:413),        at app//org.junit.runner.JUnitCore.run(JUnitCore.java:137),        at app//edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),        at app//edu.illinois.confuzz.internal.ConfuzzReplayer.execute(ConfuzzReplayer.java:40),        at app//edu.illinois.confuzz.internal.ConfuzzReplayer.accept(ConfuzzReplayer.java:33),        at app//edu.neu.ccs.prl.meringue.AnalysisForkMain.main(AnalysisForkMain.java:18),"DatanodeAdminMonitor-0" daemon prio=5 tid=60 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"refreshUsed-/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data4/current/BP-1753835085-10.0.0.67-1674628375238" daemon prio=5 tid=266 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:205),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp1474125899-40" daemon prio=5 tid=40 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 5 on default port 44243" daemon prio=5 tid=111 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"StorageLocationChecker thread 0" daemon prio=5 tid=153 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"refreshUsed-/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data6/current/BP-1753835085-10.0.0.67-1674628375238" daemon prio=5 tid=261 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:205),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server idle connection scanner for port 40341" daemon prio=5 tid=55 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.util.TimerThread.mainLoop(Timer.java:553),        at java.base@11.0.17/java.util.TimerThread.run(Timer.java:506),"IPC Server handler 4 on default port 46429" daemon prio=5 tid=185 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server listener on 0" daemon prio=5 tid=172 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener.run(Server.java:1358),"IPC Server handler 7 on default port 46829" daemon prio=5 tid=230 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"Socket Reader #1 for port 0"  prio=5 tid=134 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:1296),        at app//org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:1275),"qtp1474125899-39" daemon prio=5 tid=39 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp1527195473-93" daemon prio=5 tid=93 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"refreshUsed-/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7/current/BP-1753835085-10.0.0.67-1674628375238" daemon prio=5 tid=264 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:205),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp1911144089-165" daemon prio=5 tid=165 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:141),        at app//org.eclipse.jetty.io.ManagedSelector.nioSelect(ManagedSelector.java:183),        at app//org.eclipse.jetty.io.ManagedSelector.select(ManagedSelector.java:190),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.select(ManagedSelector.java:606),        at app//org.eclipse.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:543),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173),        at app//org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137),        at app//org.eclipse.jetty.io.ManagedSelector$$Lambda$221/0x0000000840370440.run(Unknown Source),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-31-thread-1"  prio=5 tid=164 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"BP-1753835085-10.0.0.67-1674628375238 heartbeating to localhost/127.0.0.1:40341" daemon prio=5 tid=179 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager.waitTillNextIBR(IncrementalBlockReportManager.java:158),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:739),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:879),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-38-thread-1" daemon prio=5 tid=284 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 3 on default port 44243" daemon prio=5 tid=109 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"VolumeScannerThread(/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data7)" daemon prio=5 tid=238 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:656),"IPC Server handler 2 on default port 44243" daemon prio=5 tid=108 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 9 on default port 44243" daemon prio=5 tid=115 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"pool-24-thread-1"  prio=5 tid=125 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Signal Dispatcher" daemon prio=9 tid=4 runnable,java.lang.Thread.State: RUNNABLE,"IPC Server handler 5 on default port 46429" daemon prio=5 tid=186 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"process reaper" daemon prio=10 tid=259 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp1474125899-41" daemon prio=5 tid=41 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"LeaseRenewer:root@localhost:40341" daemon prio=5 tid=302 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.hdfs.client.impl.LeaseRenewer.run(LeaseRenewer.java:441),        at app//org.apache.hadoop.hdfs.client.impl.LeaseRenewer.access$800(LeaseRenewer.java:77),        at app//org.apache.hadoop.hdfs.client.impl.LeaseRenewer$1.run(LeaseRenewer.java:336),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 2 on default port 46829" daemon prio=5 tid=225 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 1 on default port 44243" daemon prio=5 tid=107 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"IPC Server handler 8 on default port 40077" daemon prio=5 tid=151 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"qtp1474125899-42" daemon prio=5 tid=42 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"java.util.concurrent.ThreadPoolExecutor$Worker@5c302340[State = -1, empty queue]" daemon prio=5 tid=297 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"qtp1474125899-37-acceptor-0@5a47e3ca-ServerConnector@36edbb99{HTTP/1.1, (http/1.1)}{127.0.0.1:42689}" daemon prio=3 tid=37 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:533),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:285),        at app//org.eclipse.jetty.server.ServerConnector.accept(ServerConnector.java:388),        at app//org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:702),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.util.JvmPauseMonitor$Monitor@677521" daemon prio=5 tid=132 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.util.JvmPauseMonitor$Monitor.run(JvmPauseMonitor.java:192),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server Responder" daemon prio=5 tid=175 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.EPoll.wait(Native Method),        at java.base@11.0.17/sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:120),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:124),        at java.base@11.0.17/sun.nio.ch.SelectorImpl.select(SelectorImpl.java:136),        at app//org.apache.hadoop.ipc.Server$Responder.doRunLoop(Server.java:1532),        at app//org.apache.hadoop.ipc.Server$Responder.run(Server.java:1515),"VolumeScannerThread(/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data5)" daemon prio=5 tid=235 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.VolumeScanner.run(VolumeScanner.java:656),"IPC Server handler 4 on default port 46829" daemon prio=5 tid=227 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@43d2a216" daemon prio=5 tid=76 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller.run(FSNamesystem.java:4426),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 8 on default port 44243" daemon prio=5 tid=114 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"refreshUsed-/mnt/batch/tasks/workitems/FUZZ_hdfs_r125_1M24d23h38m7s/job-1/Task3/wd/confuzz/scripts/fuzz-hadoop/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-1753835085-10.0.0.67-1674628375238" daemon prio=5 tid=263 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread.run(CachingGetSpaceUsed.java:205),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 6 on default port 44243" daemon prio=5 tid=112 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"qtp1911144089-167" daemon prio=5 tid=167 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at app//org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:382),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.idleJobPoll(QueuedThreadPool.java:974),        at app//org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1018),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"pool-16-thread-1"  prio=5 tid=89 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"IPC Server handler 4 on default port 44243" daemon prio=5 tid=110 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.poll(LinkedBlockingQueue.java:458),        at app//org.apache.hadoop.ipc.CallQueueManager.take(CallQueueManager.java:317),        at app//org.apache.hadoop.ipc.Server$Handler.run(Server.java:2942),"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6a7ee5f5" daemon prio=5 tid=163 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:533),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:285),        at java.base@11.0.17/sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:114),        at app//org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:85),        at app//org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:229),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"java.util.concurrent.ThreadPoolExecutor$Worker@2692f2d3[State = -1, empty queue]" daemon prio=5 tid=294 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2123),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1182),        at java.base@11.0.17/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:899),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1054),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6122a8d5" daemon prio=5 tid=199 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:533),        at java.base@11.0.17/sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:285),        at java.base@11.0.17/sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:114),        at app//org.apache.hadoop.hdfs.net.TcpPeerServer.accept(TcpPeerServer.java:85),        at app//org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:229),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks$PendingReconstructionMonitor@a526b3a" daemon prio=5 tid=59 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Thread.sleep(Native Method),        at app//org.apache.hadoop.hdfs.server.blockmanagement.PendingReconstructionBlocks$PendingReconstructionMonitor.run(PendingReconstructionBlocks.java:259),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"BP-1753835085-10.0.0.67-1674628375238 heartbeating to localhost/127.0.0.1:40341" daemon prio=5 tid=141 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at app//org.apache.hadoop.hdfs.server.datanode.IncrementalBlockReportManager.waitTillNextIBR(IncrementalBlockReportManager.java:158),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:739),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:879),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),,),  org.mockito.exceptions.verification.junit.ArgumentsAreDifferent(,Argument(s) are different! Wanted:,metricsRecordBuilder.addGauge(,    Info with name=GetGroups1sNumOps,,    geq(0),);,-> at org.apache.hadoop.test.MetricsAsserts.assertQuantileGauges(MetricsAsserts.java:395),Actual invocations have different arguments:,metricsRecordBuilder.parent(,    ,);,-> at org.apache.hadoop.test.MetricsAsserts.getMetrics(MetricsAsserts.java:93),metricsRecordBuilder.add(,    MetricsTag{info=MsInfo{name=Context, description=Metrics context}, value=ugi},);,-> at org.apache.hadoop.metrics2.lib.MetricsRegistry.snapshot(MetricsRegistry.java:444),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=GetGroupsNumOps, description=Number of ops for getGroups},,    1L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroupsAvgTime, description=Average time for getGroups},,    21.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=LoginFailureNumOps, description=Number of ops for rate of failed kerberos logins and latency (milliseconds)},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=LoginFailureAvgTime, description=Average time for rate of failed kerberos logins and latency (milliseconds)},,    0.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=LoginSuccessNumOps, description=Number of ops for rate of successful kerberos logins and latency (milliseconds)},,    1L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=LoginSuccessAvgTime, description=Average time for rate of successful kerberos logins and latency (milliseconds)},,    31.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=RenewalFailures, description=Renewal failures since last successful login},,    0,);,-> at org.apache.hadoop.metrics2.lib.MutableGaugeInt.snapshot(MutableGaugeInt.java:86),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=RenewalFailuresTotal, description=Renewal failures since startup},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableGaugeLong.snapshot(MutableGaugeLong.java:86),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups694740338sNumOps, description=Number of ops for get groups with 694740338s interval},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:117),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups694740338s50thPercentileLatency, description=50 percentile latency with 694740338 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups694740338s75thPercentileLatency, description=75 percentile latency with 694740338 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups694740338s90thPercentileLatency, description=90 percentile latency with 694740338 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups694740338s95thPercentileLatency, description=95 percentile latency with 694740338 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups694740338s99thPercentileLatency, description=99 percentile latency with 694740338 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics#testExcessBlocks;id_000001;[dfs.client.short.circuit.num=1976386361];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  java.lang.AssertionError(Bad value for metric ExcessBlocks expected:<1> but was:<0>),  org.mockito.exceptions.verification.junit.ArgumentsAreDifferent(,Argument(s) are different! Wanted:,metricsRecordBuilder.addGauge(,    Info with name=GetGroups1sNumOps,,    geq(0),);,-> at org.apache.hadoop.test.MetricsAsserts.assertQuantileGauges(MetricsAsserts.java:395),Actual invocations have different arguments:,metricsRecordBuilder.parent(,    ,);,-> at org.apache.hadoop.test.MetricsAsserts.getMetrics(MetricsAsserts.java:93),metricsRecordBuilder.add(,    MetricsTag{info=MsInfo{name=Context, description=Metrics context}, value=ugi},);,-> at org.apache.hadoop.metrics2.lib.MetricsRegistry.snapshot(MetricsRegistry.java:444),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=GetGroupsNumOps, description=Number of ops for getGroups},,    1L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroupsAvgTime, description=Average time for getGroups},,    10.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=LoginFailureNumOps, description=Number of ops for rate of failed kerberos logins and latency (milliseconds)},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=LoginFailureAvgTime, description=Average time for rate of failed kerberos logins and latency (milliseconds)},,    0.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=LoginSuccessNumOps, description=Number of ops for rate of successful kerberos logins and latency (milliseconds)},,    1L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=LoginSuccessAvgTime, description=Average time for rate of successful kerberos logins and latency (milliseconds)},,    26.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=RenewalFailures, description=Renewal failures since last successful login},,    0,);,-> at org.apache.hadoop.metrics2.lib.MutableGaugeInt.snapshot(MutableGaugeInt.java:86),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=RenewalFailuresTotal, description=Renewal failures since startup},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableGaugeLong.snapshot(MutableGaugeLong.java:86),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024sNumOps, description=Number of ops for get groups with 1194352024s interval},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:117),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s50thPercentileLatency, description=50 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s75thPercentileLatency, description=75 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s90thPercentileLatency, description=90 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s95thPercentileLatency, description=95 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s99thPercentileLatency, description=99 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n', 'org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics#testGetBlockLocationMetric;id_000000;[dfs.client.short.circuit.num=424698492];org.junit.internal.runners.model.MultipleFailureException;There were 2 errors:,  org.apache.hadoop.hdfs.BlockMissingException(Could not obtain block: BP-33871308-10.0.0.48-1674637331538:blk_1073741825_1001 file=/testNameNodeMetrics/file1.dat No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:40719,DS-fb453848-d09e-47a4-a579-8d00a18129bb,DISK] DatanodeInfoWithStorage[127.0.0.1:39951,DS-ec986e16-92b4-410c-a08e-61239b3eeacf,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:40719,DS-fb453848-d09e-47a4-a579-8d00a18129bb,DISK] DatanodeInfoWithStorage[127.0.0.1:39951,DS-ec986e16-92b4-410c-a08e-61239b3eeacf,DISK]),  org.mockito.exceptions.verification.junit.ArgumentsAreDifferent(,Argument(s) are different! Wanted:,metricsRecordBuilder.addGauge(,    Info with name=GetGroups1sNumOps,,    geq(0),);,-> at org.apache.hadoop.test.MetricsAsserts.assertQuantileGauges(MetricsAsserts.java:395),Actual invocations have different arguments:,metricsRecordBuilder.parent(,    ,);,-> at org.apache.hadoop.test.MetricsAsserts.getMetrics(MetricsAsserts.java:93),metricsRecordBuilder.add(,    MetricsTag{info=MsInfo{name=Context, description=Metrics context}, value=ugi},);,-> at org.apache.hadoop.metrics2.lib.MetricsRegistry.snapshot(MetricsRegistry.java:444),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=GetGroupsNumOps, description=Number of ops for getGroups},,    1L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroupsAvgTime, description=Average time for getGroups},,    13.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=LoginFailureNumOps, description=Number of ops for rate of failed kerberos logins and latency (milliseconds)},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=LoginFailureAvgTime, description=Average time for rate of failed kerberos logins and latency (milliseconds)},,    0.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=LoginSuccessNumOps, description=Number of ops for rate of successful kerberos logins and latency (milliseconds)},,    1L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=LoginSuccessAvgTime, description=Average time for rate of successful kerberos logins and latency (milliseconds)},,    28.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=RenewalFailures, description=Renewal failures since last successful login},,    0,);,-> at org.apache.hadoop.metrics2.lib.MutableGaugeInt.snapshot(MutableGaugeInt.java:86),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=RenewalFailuresTotal, description=Renewal failures since startup},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableGaugeLong.snapshot(MutableGaugeLong.java:86),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups419177621sNumOps, description=Number of ops for get groups with 419177621s interval},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:117),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups419177621s50thPercentileLatency, description=50 percentile latency with 419177621 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups419177621s75thPercentileLatency, description=75 percentile latency with 419177621 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups419177621s90thPercentileLatency, description=90 percentile latency with 419177621 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups419177621s95thPercentileLatency, description=95 percentile latency with 419177621 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups419177621s99thPercentileLatency, description=99 percentile latency with 419177621 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),);[org.junit.runners.model.MultipleFailureException.assertEmpty(MultipleFailureException.java:102), org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:39), org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306), edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65), edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222)]\n']
[dfs.client.short.circuit.num];org.mockito.exceptions.verification.junit.ArgumentsAreDifferent;1;['org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics#testExcessBlocks;id_000000;[dfs.client.short.circuit.num=1976386361];org.mockito.exceptions.verification.junit.ArgumentsAreDifferent;,Argument(s) are different! Wanted:,metricsRecordBuilder.addGauge(,    Info with name=GetGroups1sNumOps,,    geq(0),);,-> at org.apache.hadoop.test.MetricsAsserts.assertQuantileGauges(MetricsAsserts.java:395),Actual invocations have different arguments:,metricsRecordBuilder.parent(,    ,);,-> at org.apache.hadoop.test.MetricsAsserts.getMetrics(MetricsAsserts.java:93),metricsRecordBuilder.add(,    MetricsTag{info=MsInfo{name=Context, description=Metrics context}, value=ugi},);,-> at org.apache.hadoop.metrics2.lib.MetricsRegistry.snapshot(MetricsRegistry.java:444),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=GetGroupsNumOps, description=Number of ops for getGroups},,    1L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroupsAvgTime, description=Average time for getGroups},,    10.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=LoginFailureNumOps, description=Number of ops for rate of failed kerberos logins and latency (milliseconds)},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=LoginFailureAvgTime, description=Average time for rate of failed kerberos logins and latency (milliseconds)},,    0.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addCounter(,    MetricsInfoImpl{name=LoginSuccessNumOps, description=Number of ops for rate of successful kerberos logins and latency (milliseconds)},,    1L,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:142),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=LoginSuccessAvgTime, description=Average time for rate of successful kerberos logins and latency (milliseconds)},,    26.0d,);,-> at org.apache.hadoop.metrics2.lib.MutableStat.snapshot(MutableStat.java:143),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=RenewalFailures, description=Renewal failures since last successful login},,    0,);,-> at org.apache.hadoop.metrics2.lib.MutableGaugeInt.snapshot(MutableGaugeInt.java:86),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=RenewalFailuresTotal, description=Renewal failures since startup},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableGaugeLong.snapshot(MutableGaugeLong.java:86),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024sNumOps, description=Number of ops for get groups with 1194352024s interval},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:117),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s50thPercentileLatency, description=50 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s75thPercentileLatency, description=75 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s90thPercentileLatency, description=90 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s95thPercentileLatency, description=95 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),metricsRecordBuilder.addGauge(,    MetricsInfoImpl{name=GetGroups1194352024s99thPercentileLatency, description=99 percentile latency with 1194352024 second interval for get groups},,    0L,);,-> at org.apache.hadoop.metrics2.lib.MutableQuantiles.snapshot(MutableQuantiles.java:124),;[org.apache.hadoop.test.MetricsAsserts.assertQuantileGauges(MetricsAsserts.java:395), org.apache.hadoop.test.MetricsAsserts.assertQuantileGauges(MetricsAsserts.java:381), org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.tearDown(TestNameNodeMetrics.java:183), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n']
[dfs.blockreport.intervalMsec,dfs.heartbeat.interval];java.util.concurrent.TimeoutException;1;['org.apache.hadoop.hdfs.server.datanode.TestBPOfferService#testBPInitErrorHandling;id_000000;[dfs.blockreport.intervalMsec=7, dfs.heartbeat.interval=0];java.util.concurrent.TimeoutException;Timed out waiting for condition. ,Thread diagnostics:,Timestamp: 2023-01-25 11:57:53,951,,"Command processor" daemon prio=5 tid=15 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1331),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.run(BPServiceActor.java:1315),"Finalizer" daemon prio=8 tid=3 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:155),        at java.base@11.0.17/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:176),        at java.base@11.0.17/java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:170),"Common-Cleaner" daemon prio=8 tid=9 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/java.lang.Object.wait(Native Method),        at java.base@11.0.17/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:155),        at java.base@11.0.17/jdk.internal.ref.CleanerImpl.run(CleanerImpl.java:148),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),        at java.base@11.0.17/jdk.internal.misc.InnocuousThread.run(InnocuousThread.java:161),"fake bpid heartbeating to 0.0.0.0/0.0.0.0:0" daemon prio=5 tid=17 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/jdk.internal.misc.Unsafe.unpark(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.unpark(LockSupport.java:160),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer.unparkSuccessor(AbstractQueuedSynchronizer.java:709),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer.release(AbstractQueuedSynchronizer.java:1305),        at java.base@11.0.17/java.util.concurrent.locks.ReentrantLock.unlock(ReentrantLock.java:439),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.signalNotEmpty(LinkedBlockingQueue.java:177),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:351),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.enqueue(BPServiceActor.java:1405),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:706),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:879),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"Signal Dispatcher" daemon prio=9 tid=4 runnable,java.lang.Thread.State: RUNNABLE,"process reaper" daemon prio=10 tid=11 timed_waiting,java.lang.Thread.State: TIMED_WAITING,        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:234),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:462),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:361),        at java.base@11.0.17/java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:937),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1053),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1114),        at java.base@11.0.17/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),"main"  prio=5 tid=1 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/java.lang.Thread.dumpThreads(Native Method),        at java.base@11.0.17/java.lang.Thread.getAllStackTraces(Thread.java:1653),        at app//org.apache.hadoop.test.TimedOutTestsListener.buildThreadDump(TimedOutTestsListener.java:87),        at app//org.apache.hadoop.test.TimedOutTestsListener.buildThreadDiagnosticString(TimedOutTestsListener.java:73),        at app//org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:461),        at app//org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:421),        at app//org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.waitForBlockReport(TestBPOfferService.java:640),        at app//org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBPInitErrorHandling(TestBPOfferService.java:532),        at app//org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBPInitErrorHandling$$CONFUZZ(TestBPOfferService.java),        at java.base@11.0.17/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method),        at java.base@11.0.17/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62),        at java.base@11.0.17/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43),        at java.base@11.0.17/java.lang.reflect.Method.invoke(Method.java:566),        at app//org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59),        at app//org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12),        at app//org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56),        at app//edu.berkeley.cs.jqf.fuzz.junit.TrialRunner$1.evaluate(TrialRunner.java:59),        at app//org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26),        at app//org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),        at app//edu.berkeley.cs.jqf.fuzz.junit.TrialRunner.run(TrialRunner.java:65),        at app//edu.berkeley.cs.jqf.fuzz.guidance.Guidance.run(Guidance.java:222),        at app//edu.berkeley.cs.jqf.fuzz.junit.quickcheck.FuzzStatement.evaluate(FuzzStatement.java:144),        at app//org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100),        at app//org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366),        at app//org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103),        at app//org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63),        at app//org.junit.runners.ParentRunner$4.run(ParentRunner.java:331),        at app//org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79),        at app//org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329),        at app//org.junit.runners.ParentRunner.access$100(ParentRunner.java:66),        at app//org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293),        at app//org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306),        at app//org.junit.runners.ParentRunner.run(ParentRunner.java:413),        at app//org.junit.runner.JUnitCore.run(JUnitCore.java:137),        at app//edu.berkeley.cs.jqf.fuzz.junit.GuidedFuzzing.run(GuidedFuzzing.java:208),        at app//edu.illinois.confuzz.internal.ConfuzzReplayer.execute(ConfuzzReplayer.java:40),        at app//edu.illinois.confuzz.internal.ConfuzzReplayer.accept(ConfuzzReplayer.java:33),        at app//edu.neu.ccs.prl.meringue.AnalysisForkMain.main(AnalysisForkMain.java:18),"Reference Handler" daemon prio=10 tid=2 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/java.lang.ref.Reference.waitForReferencePendingList(Native Method),        at java.base@11.0.17/java.lang.ref.Reference.processPendingReferences(Reference.java:241),        at java.base@11.0.17/java.lang.ref.Reference$ReferenceHandler.run(Reference.java:213),"Command processor" daemon prio=5 tid=16 in Object.wait(),java.lang.Thread.State: WAITING (on object monitor),        at java.base@11.0.17/jdk.internal.misc.Unsafe.park(Native Method),        at java.base@11.0.17/java.util.concurrent.locks.LockSupport.park(LockSupport.java:194),        at java.base@11.0.17/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2081),        at java.base@11.0.17/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.processQueue(BPServiceActor.java:1331),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor$CommandProcessingThread.run(BPServiceActor.java:1315),"fake bpid heartbeating to 0.0.0.0/0.0.0.0:1" daemon prio=5 tid=18 runnable,java.lang.Thread.State: RUNNABLE,        at java.base@11.0.17/java.lang.StackTraceElement.initStackTraceElements(Native Method),        at java.base@11.0.17/java.lang.StackTraceElement.of(StackTraceElement.java:526),        at java.base@11.0.17/java.lang.Throwable.getOurStackTrace(Throwable.java:828),        at java.base@11.0.17/java.lang.Throwable.getStackTrace(Throwable.java:820),        at app//org.mockito.internal.exceptions.stacktrace.StackTraceFilter.filterFirst(StackTraceFilter.java:109),        at app//org.mockito.internal.debugging.LocationImpl.computeStackTraceInformation(LocationImpl.java:48),        at app//org.mockito.internal.debugging.LocationImpl.<init>(LocationImpl.java:33),        at app//org.mockito.internal.debugging.LocationImpl.<init>(LocationImpl.java:25),        at app//org.mockito.internal.debugging.LocationImpl.<init>(LocationImpl.java:21),        at app//org.mockito.internal.creation.bytebuddy.MockMethodInterceptor.doIntercept(MockMethodInterceptor.java:49),        at app//org.mockito.internal.creation.bytebuddy.MockMethodInterceptor$DispatcherDefaultingToRealMethod.interceptSuperCallable(MockMethodInterceptor.java:108),        at app//org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset$MockitoMock$2143225563.getCacheCapacity(Unknown Source),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:545),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:677),        at app//org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:879),        at java.base@11.0.17/java.lang.Thread.run(Thread.java:829),,;[org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:462), org.apache.hadoop.test.GenericTestUtils.waitFor(GenericTestUtils.java:421), org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.waitForBlockReport(TestBPOfferService.java:640), org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBPInitErrorHandling(TestBPOfferService.java:532), org.apache.hadoop.hdfs.server.datanode.TestBPOfferService.testBPInitErrorHandling$$CONFUZZ(TestBPOfferService.java)]\n']
[dfs.datanode.du.reserved.calculator];java.lang.ClassNotFoundException;1;['org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestReservedSpaceCalculator#testInvalidCalculator;id_000000;[dfs.datanode.du.reserved.calculator=INVALIDTYPE];java.lang.ClassNotFoundException;Class INVALIDTYPE not found;[org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2685), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2779), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2805), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder.build(ReservedSpaceCalculator.java:66), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestReservedSpaceCalculator.testInvalidCalculator(TestReservedSpaceCalculator.java:160)]\n']
[dfs.http.policy];org.apache.hadoop.HadoopIllegalArgumentException;1;["org.apache.hadoop.hdfs.TestHttpPolicy#testInvalidPolicyValue;id_000000;[dfs.http.policy=invalid];org.apache.hadoop.HadoopIllegalArgumentException;Unrecognized value 'invalid' for dfs.http.policy;[org.apache.hadoop.hdfs.DFSUtil.getHttpPolicy(DFSUtil.java:1536), org.apache.hadoop.hdfs.TestHttpPolicy.testInvalidPolicyValue(TestHttpPolicy.java:30), org.apache.hadoop.hdfs.TestHttpPolicy.testInvalidPolicyValue$$CONFUZZ(TestHttpPolicy.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method), jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)]\n"]
[dfs.ha.tail-edits.in-progress,dfs.journalnode.edit-cache-size.bytes,dfs.journalnode.edits.dir.perm,dfs.namenode.edits.noeditlogchannelflush];java.lang.IllegalArgumentException;7;['org.apache.hadoop.hdfs.qjournal.server.TestJournal#testRestartJournal;id_000000;[dfs.namenode.edits.noeditlogchannelflush=false, dfs.ha.tail-edits.in-progress=true, dfs.journalnode.edits.dir.perm=944425953, dfs.journalnode.edit-cache-size.bytes=2089149380];java.lang.IllegalArgumentException;944425953;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:70), org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:163)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournal#testAbortOldSegmentIfFinalizeIsMissed;id_000000;[dfs.namenode.edits.noeditlogchannelflush=true, dfs.ha.tail-edits.in-progress=true, dfs.journalnode.edits.dir.perm=288550846, dfs.journalnode.edit-cache-size.bytes=1623084259];java.lang.IllegalArgumentException;288550846;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:70), org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:163)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournal#testEmptyEditsInProgressMovedAside;id_000000;[dfs.namenode.edits.noeditlogchannelflush=false, dfs.ha.tail-edits.in-progress=true, dfs.journalnode.edits.dir.perm=1373413504, dfs.journalnode.edit-cache-size.bytes=572794891];java.lang.IllegalArgumentException;1373413504;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:70), org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:163)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournal#testFinalizeWhenEditsAreMissed;id_000000;[dfs.namenode.edits.noeditlogchannelflush=true, dfs.ha.tail-edits.in-progress=true, dfs.journalnode.edits.dir.perm=1727750001, dfs.journalnode.edit-cache-size.bytes=402941235];java.lang.IllegalArgumentException;1727750001;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:70), org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:163)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournal#testScanEditLog;id_000000;[dfs.namenode.edits.noeditlogchannelflush=true, dfs.ha.tail-edits.in-progress=true, dfs.journalnode.edits.dir.perm=1994318092, dfs.journalnode.edit-cache-size.bytes=1892510705];java.lang.IllegalArgumentException;1994318092;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:70), org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:163)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournal#testFormatResetsCachedValues;id_000000;[dfs.namenode.edits.noeditlogchannelflush=false, dfs.ha.tail-edits.in-progress=true, dfs.journalnode.edits.dir.perm=676089348, dfs.journalnode.edit-cache-size.bytes=909404047];java.lang.IllegalArgumentException;676089348;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:70), org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:163)]\n', 'org.apache.hadoop.hdfs.qjournal.server.TestJournal#testNewEpochAtBeginningOfSegment;id_000000;[dfs.namenode.edits.noeditlogchannelflush=false, dfs.ha.tail-edits.in-progress=true, dfs.journalnode.edits.dir.perm=165650745, dfs.journalnode.edit-cache-size.bytes=342972269];java.lang.IllegalArgumentException;165650745;[org.apache.hadoop.fs.permission.PermissionParser.<init>(PermissionParser.java:60), org.apache.hadoop.fs.permission.RawParser.<init>(RawParser.java:36), org.apache.hadoop.fs.permission.FsPermission.<init>(FsPermission.java:145), org.apache.hadoop.hdfs.qjournal.server.JNStorage.<init>(JNStorage.java:70), org.apache.hadoop.hdfs.qjournal.server.Journal.<init>(Journal.java:163)]\n']
[dfs.nameservices,hadoop.security.auth_to_local,hadoop.security.auth_to_local.mechanism,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.secs,hadoop.user.group.static.mapping.overrides];, hadoop.kerberos.keytab.login.autorenewal.enabled=false, dfs.client.failover.connection.retries.on.timeouts=665169355, hadoop.security.groups.negative-cache.secs=547237959, dfs.client.failover.resolver.useFQDN.testcluster=true, dfs.ha.namenodes.testcluster=nn0,nn1,nn2, ipc.client.connect.max.retries.on.timeouts=665169355, dfs.namenode.rpc-address.testcluster.nn1=namenode1.test:8020, dfs.namenode.rpc-address.testcluster.nn0=namenode0.test:8020, hadoop.security.dns.log-slow-lookups.threshold.ms=1572958677, dfs.namenode.rpc-address.testcluster.nn2=namenode2.test:8020, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1055580819, ipc.client.connect.max.retries=294310828, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=361445332, dfs.client.failover.random.order=false, dfs.client.failover.observer.auto-msync-period.testcluster=447186552, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, dfs.client.failover.connection.retries=294310828, dfs.client.failover.observer.probe.retry.period=0ms, hadoop.security.groups.cache.warn.after.ms=1601998382, dfs.client.failover.resolve-needed.testcluster=true];1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testReadOperationOnObserver;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1517190635, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, dfs.nameservices=testcluster, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.kerberos.keytab.login.autorenewal.enabled=false, dfs.client.failover.connection.retries.on.timeouts=665169355, hadoop.security.groups.negative-cache.secs=547237959, dfs.client.failover.resolver.useFQDN.testcluster=true, dfs.ha.namenodes.testcluster=nn0,nn1,nn2, ipc.client.connect.max.retries.on.timeouts=665169355, dfs.namenode.rpc-address.testcluster.nn1=namenode1.test:8020, dfs.namenode.rpc-address.testcluster.nn0=namenode0.test:8020, hadoop.security.dns.log-slow-lookups.threshold.ms=1572958677, dfs.namenode.rpc-address.testcluster.nn2=namenode2.test:8020, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=1055580819, ipc.client.connect.max.retries=294310828, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=361445332, dfs.client.failover.random.order=false, dfs.client.failover.observer.auto-msync-period.testcluster=447186552, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, dfs.client.failover.connection.retries=294310828, dfs.client.failover.observer.probe.retry.period=0ms, hadoop.security.groups.cache.warn.after.ms=1601998382, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test: Name or service not known;[java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method), java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929), java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1529), java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848), java.net.InetAddress.getAllByName0(InetAddress.java:1519)]\n']
[dfs.client.failover.connection.retries,dfs.client.failover.connection.retries.on.timeouts,dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.resolve-needed.testcluster,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.groups.cache.background.reload.threads,ipc.client.connect.max.retries,ipc.client.connect.max.retries.on.timeouts];java.net.UnknownHostException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testReadOperationOnObserver;id_000001;[hadoop.kerberos.min.seconds.before.relogin=952495153, hadoop.kerberos.keytab.login.autorenewal.enabled=false, dfs.client.failover.connection.retries.on.timeouts=999048617, hadoop.security.groups.cache.background.reload.threads=443131719, dfs.client.failover.observer.auto-msync-period.testcluster=1675262529, dfs.client.failover.connection.retries=1521842899, ipc.client.connect.max.retries=1521842899, ipc.client.connect.max.retries.on.timeouts=999048617, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test;[java.net.InetAddress$CachedAddresses.get(InetAddress.java:797), java.net.InetAddress.getAllByName0(InetAddress.java:1519), java.net.InetAddress.getAllByName(InetAddress.java:1378), java.net.InetAddress.getAllByName(InetAddress.java:1306), org.apache.hadoop.net.DNSDomainNameResolver.getAllByDomainName(DNSDomainNameResolver.java:33)]\n']
[dfs.namenode.write.stale.datanode.ratio];java.lang.IllegalArgumentException;1;["org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testGetBlockLocationConsiderLoadWithNodesOfSameDistance;id_000000;[dfs.namenode.write.stale.datanode.ratio=0.0f];java.lang.IllegalArgumentException;dfs.namenode.write.stale.datanode.ratio = '0.0' is invalid. It should be a positive non-zero float value, not greater than 1.0f.;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.<init>(DatanodeManager.java:361), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.mockDatanodeManager(TestDatanodeManager.java:91), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoadWithNodesOfSameDistance(TestDatanodeManager.java:621), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testGetBlockLocationConsiderLoadWithNodesOfSameDistance$$CONFUZZ(TestDatanodeManager.java)]\n"]
[dfs.ha.automatic-failover.enabled];java.io.IOException;1;['org.apache.hadoop.hdfs.tools.TestDFSHAAdminMiniCluster#testTryFailoverToSafeMode;id_000000;[dfs.ha.automatic-failover.enabled=true];java.io.IOException;The option dfs.namenode.support.allow.format is set to false for this filesystem, so it cannot be formatted. You will need to set dfs.namenode.support.allow.format parameter to true in order to format this filesystem;[org.apache.hadoop.hdfs.server.namenode.NameNode.checkAllowFormat(NameNode.java:1296), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1228), org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:450), org.apache.hadoop.hdfs.DFSTestUtil.formatNameNode(DFSTestUtil.java:261), org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1132)]\n']
[dfs.namenode.max.extra.edits.segments.retained,dfs.namenode.num.checkpoints.retained];java.lang.IllegalArgumentException;8;['org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager#testExtraInprogressFilesAreRemovedOrMarkedStale;id_000000;[dfs.namenode.num.checkpoints.retained=0, dfs.namenode.max.extra.edits.segments.retained=0];java.lang.IllegalArgumentException;Must retain at least one checkpoint;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.<init>(NNStorageRetentionManager.java:79), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.runTest(TestNNStorageRetentionManager.java:298), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testExtraInprogressFilesAreRemovedOrMarkedStale(TestNNStorageRetentionManager.java:283), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testExtraInprogressFilesAreRemovedOrMarkedStale$$CONFUZZ(TestNNStorageRetentionManager.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager#testOldInProgress;id_000000;[dfs.namenode.num.checkpoints.retained=0, dfs.namenode.max.extra.edits.segments.retained=0];java.lang.IllegalArgumentException;Must retain at least one checkpoint;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.<init>(NNStorageRetentionManager.java:79), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.runTest(TestNNStorageRetentionManager.java:298), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testOldInProgress(TestNNStorageRetentionManager.java:168), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testOldInProgress$$CONFUZZ(TestNNStorageRetentionManager.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager#testPurgeMultipleDirs;id_000000;[dfs.namenode.num.checkpoints.retained=0, dfs.namenode.max.extra.edits.segments.retained=0];java.lang.IllegalArgumentException;Must retain at least one checkpoint;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.<init>(NNStorageRetentionManager.java:79), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.runTest(TestNNStorageRetentionManager.java:298), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testPurgeMultipleDirs(TestNNStorageRetentionManager.java:113), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testPurgeMultipleDirs$$CONFUZZ(TestNNStorageRetentionManager.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager#testPurgeEasyCase;id_000000;[dfs.namenode.num.checkpoints.retained=0, dfs.namenode.max.extra.edits.segments.retained=0];java.lang.IllegalArgumentException;Must retain at least one checkpoint;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.<init>(NNStorageRetentionManager.java:79), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.runTest(TestNNStorageRetentionManager.java:298), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testPurgeEasyCase(TestNNStorageRetentionManager.java:91), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testPurgeEasyCase$$CONFUZZ(TestNNStorageRetentionManager.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager#testNoLogs;id_000000;[dfs.namenode.num.checkpoints.retained=0, dfs.namenode.max.extra.edits.segments.retained=0];java.lang.IllegalArgumentException;Must retain at least one checkpoint;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.<init>(NNStorageRetentionManager.java:79), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.runTest(TestNNStorageRetentionManager.java:298), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testNoLogs(TestNNStorageRetentionManager.java:143), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testNoLogs$$CONFUZZ(TestNNStorageRetentionManager.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager#testPurgeLessThanRetention;id_000000;[dfs.namenode.num.checkpoints.retained=0, dfs.namenode.max.extra.edits.segments.retained=0];java.lang.IllegalArgumentException;Must retain at least one checkpoint;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.<init>(NNStorageRetentionManager.java:79), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.runTest(TestNNStorageRetentionManager.java:298), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testPurgeLessThanRetention(TestNNStorageRetentionManager.java:129), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testPurgeLessThanRetention$$CONFUZZ(TestNNStorageRetentionManager.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager#testSeparateEditDirs;id_000000;[dfs.namenode.num.checkpoints.retained=0, dfs.namenode.max.extra.edits.segments.retained=0];java.lang.IllegalArgumentException;Must retain at least one checkpoint;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.<init>(NNStorageRetentionManager.java:79), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.runTest(TestNNStorageRetentionManager.java:298), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testSeparateEditDirs(TestNNStorageRetentionManager.java:185), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testSeparateEditDirs$$CONFUZZ(TestNNStorageRetentionManager.java)]\n', 'org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager#testEmptyDir;id_000000;[dfs.namenode.num.checkpoints.retained=0, dfs.namenode.max.extra.edits.segments.retained=0];java.lang.IllegalArgumentException;Must retain at least one checkpoint;[org.apache.hadoop.thirdparty.com.google.common.base.Preconditions.checkArgument(Preconditions.java:144), org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager.<init>(NNStorageRetentionManager.java:79), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.runTest(TestNNStorageRetentionManager.java:298), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testEmptyDir(TestNNStorageRetentionManager.java:153), org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager.testEmptyDir$$CONFUZZ(TestNNStorageRetentionManager.java)]\n']
[dfs.namenode.max.extra.edits.segments.retained,dfs.namenode.num.checkpoints.retained];java.lang.IndexOutOfBoundsException;1;['org.apache.hadoop.hdfs.server.namenode.TestNNStorageRetentionManager#testExtraInprogressFilesAreRemovedOrMarkedStale;id_000002;[dfs.namenode.num.checkpoints.retained=578428413, dfs.namenode.max.extra.edits.segments.retained=-2147483648];java.lang.IndexOutOfBoundsException;Index 0 out of bounds for length 0;[jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64), jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70), jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248), java.util.Objects.checkIndex(Objects.java:372), java.util.ArrayList.get(ArrayList.java:459)]\n']
[dfs.client.failover.observer.auto-msync-period.testcluster,hadoop.security.groups.cache.background.reload.threads];java.io.IOException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testUnreachableObserverWithMultiple;id_000000;[hadoop.security.groups.cache.background.reload.threads=1113751296, dfs.client.failover.observer.auto-msync-period.testcluster=38921];java.io.IOException;Unavailable;[org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider$NameNodeAnswer$ClientProtocolAnswer.answer(TestObserverReadProxyProvider.java:370), org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39), org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96), org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29), org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35)]\n']
[dfs.client.failover.connection.retries,dfs.client.failover.connection.retries.on.timeouts,dfs.client.failover.observer.auto-msync-period.testcluster,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.warn.after.ms,hadoop.security.groups.negative-cache.secs,ipc.client.connect.max.retries,ipc.client.connect.max.retries.on.timeouts];org.junit.ComparisonFailure;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testUnreachableObserverWithMultiple;id_000001;[hadoop.security.dns.log-slow-lookups.threshold.ms=1491781838, hadoop.kerberos.min.seconds.before.relogin=1436704622, dfs.client.failover.connection.retries.on.timeouts=165634831, hadoop.security.groups.negative-cache.secs=2144679680, hadoop.security.groups.cache.background.reload.threads=1107343348, dfs.client.failover.observer.auto-msync-period.testcluster=892692344, dfs.client.failover.connection.retries=1855205918, hadoop.security.groups.cache.background.reload=false, ipc.client.connect.max.retries=1855205918, ipc.client.connect.max.retries.on.timeouts=165634831, hadoop.security.groups.cache.warn.after.ms=1853489514];org.junit.ComparisonFailure;expected:<namenode[2].test:8020> but was:<namenode[3].test:8020>;[org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.assertHandledBy(TestObserverReadProxyProvider.java:336), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple(TestObserverReadProxyProvider.java:193), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testUnreachableObserverWithMultiple$$CONFUZZ(TestObserverReadProxyProvider.java)]\n']
[dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.random.order,dfs.client.failover.resolve-needed.testcluster,dfs.client.failover.resolver.useFQDN.testcluster,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.secs];java.net.UnknownHostException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testUnreachableObserverWithMultiple;id_000002;[dfs.client.failover.resolver.useFQDN.testcluster=true, hadoop.security.dns.log-slow-lookups.threshold.ms=1794998342, dfs.client.failover.random.order=false, hadoop.security.groups.cache.secs=1782332432, dfs.client.failover.observer.auto-msync-period.testcluster=1759740122, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test;[java.net.InetAddress$CachedAddresses.get(InetAddress.java:797), java.net.InetAddress.getAllByName0(InetAddress.java:1519), java.net.InetAddress.getAllByName(InetAddress.java:1378), java.net.InetAddress.getAllByName(InetAddress.java:1306), org.apache.hadoop.net.DNSDomainNameResolver.getAllByDomainName(DNSDomainNameResolver.java:33)]\n']
[dfs.client.failover.connection.retries,dfs.client.failover.connection.retries.on.timeouts,dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.random.order,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.security.groups.negative-cache.secs,ipc.client.connect.max.retries,ipc.client.connect.max.retries.on.timeouts];org.apache.hadoop.ipc.RemoteException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testObserverToActive;id_000000;[hadoop.security.dns.log-slow-lookups.threshold.ms=1799239192, hadoop.security.groups.cache.secs=1768141966, hadoop.security.groups.cache.background.reload.threads=1310514229, ipc.client.connect.max.retries=296711183, hadoop.kerberos.min.seconds.before.relogin=774452056, dfs.client.failover.random.order=true, hadoop.kerberos.keytab.login.autorenewal.enabled=false, dfs.client.failover.connection.retries.on.timeouts=819552651, hadoop.security.groups.negative-cache.secs=273393426, dfs.client.failover.observer.auto-msync-period.testcluster=1789207566, dfs.client.failover.connection.retries=296711183, ipc.client.connect.max.retries.on.timeouts=819552651, hadoop.security.groups.cache.warn.after.ms=784615106];org.apache.hadoop.ipc.RemoteException;No writes!;[org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider$NameNodeAnswer$ClientProtocolAnswer.answer(TestObserverReadProxyProvider.java:398), org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39), org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96), org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29), org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35)]\n']
[dfs.client.failover.connection.retries.on.timeouts,dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.resolve-needed.testcluster,dfs.client.failover.resolver.useFQDN.testcluster,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.security.groups.negative-cache.secs,ipc.client.connect.max.retries.on.timeouts];java.net.UnknownHostException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testObserverToActive;id_000001;[dfs.client.failover.resolver.useFQDN.testcluster=true, hadoop.security.dns.log-slow-lookups.threshold.ms=1162684432, hadoop.kerberos.min.seconds.before.relogin=1727963605, hadoop.security.groups.cache.secs=1767545342, hadoop.kerberos.keytab.login.autorenewal.enabled=false, dfs.client.failover.connection.retries.on.timeouts=1269576349, hadoop.security.groups.negative-cache.secs=449326574, dfs.client.failover.observer.auto-msync-period.testcluster=644839287, hadoop.security.groups.cache.background.reload=false, ipc.client.connect.max.retries.on.timeouts=1269576349, hadoop.security.groups.cache.warn.after.ms=1519263838, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test;[java.net.InetAddress$CachedAddresses.get(InetAddress.java:797), java.net.InetAddress.getAllByName0(InetAddress.java:1519), java.net.InetAddress.getAllByName(InetAddress.java:1378), java.net.InetAddress.getAllByName(InetAddress.java:1306), org.apache.hadoop.net.DNSDomainNameResolver.getAllByDomainName(DNSDomainNameResolver.java:33)]\n']
[dfs.client.failover.connection.retries,dfs.client.failover.connection.retries.on.timeouts,dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.resolve-needed.testcluster,dfs.client.failover.resolver.useFQDN.testcluster,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.secs,ipc.client.connect.max.retries,ipc.client.connect.max.retries.on.timeouts];java.net.UnknownHostException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testObserverToStandby;id_000000;[dfs.client.failover.resolver.useFQDN.testcluster=true, hadoop.security.dns.log-slow-lookups.threshold.ms=1494433838, hadoop.kerberos.min.seconds.before.relogin=1959060318, hadoop.security.groups.cache.secs=800396591, hadoop.kerberos.keytab.login.autorenewal.enabled=false, dfs.client.failover.connection.retries.on.timeouts=329717410, dfs.client.failover.observer.auto-msync-period.testcluster=2002564767, dfs.client.failover.connection.retries=1290545411, ipc.client.connect.max.retries=1290545411, ipc.client.connect.max.retries.on.timeouts=329717410, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test;[java.net.InetAddress$CachedAddresses.get(InetAddress.java:797), java.net.InetAddress.getAllByName0(InetAddress.java:1519), java.net.InetAddress.getAllByName(InetAddress.java:1378), java.net.InetAddress.getAllByName(InetAddress.java:1306), org.apache.hadoop.net.DNSDomainNameResolver.getAllByDomainName(DNSDomainNameResolver.java:33)]\n']
[dfs.client.failover.connection.retries,dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.resolve-needed.testcluster,dfs.client.failover.resolver.useFQDN.testcluster,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.background.reload.threads,ipc.client.connect.max.retries];java.net.UnknownHostException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testWriteOperationOnActive;id_000001;[dfs.client.failover.resolver.useFQDN.testcluster=true, hadoop.security.dns.log-slow-lookups.threshold.ms=533946637, hadoop.kerberos.min.seconds.before.relogin=202869727, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.cache.background.reload.threads=1831940868, dfs.client.failover.observer.auto-msync-period.testcluster=927305886, dfs.client.failover.connection.retries=1055147743, ipc.client.connect.max.retries=1055147743, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test: Name or service not known;[java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method), java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929), java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1529), java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848), java.net.InetAddress.getAllByName0(InetAddress.java:1519)]\n']
[dfs.client.failover.connection.retries,dfs.client.failover.connection.retries.on.timeouts,dfs.client.failover.resolve-needed.testcluster,dfs.client.failover.resolver.useFQDN.testcluster,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.dns.log-slow-lookups.enabled,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.security.groups.negative-cache.secs,ipc.client.connect.max.retries,ipc.client.connect.max.retries.on.timeouts];java.net.UnknownHostException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testWriteOperationOnActive;id_000002;[dfs.client.failover.resolver.useFQDN.testcluster=true, hadoop.security.dns.log-slow-lookups.threshold.ms=536058874, hadoop.kerberos.min.seconds.before.relogin=2077559815, hadoop.security.groups.cache.secs=360315323, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.dns.log-slow-lookups.enabled=true, dfs.client.failover.connection.retries.on.timeouts=14027157, hadoop.security.groups.negative-cache.secs=348458847, dfs.client.failover.connection.retries=1582716150, ipc.client.connect.max.retries=1582716150, ipc.client.connect.max.retries.on.timeouts=14027157, hadoop.security.groups.cache.warn.after.ms=1709112567, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test;[java.net.InetAddress$CachedAddresses.get(InetAddress.java:797), java.net.InetAddress.getAllByName0(InetAddress.java:1519), java.net.InetAddress.getAllByName(InetAddress.java:1378), java.net.InetAddress.getAllByName(InetAddress.java:1306), org.apache.hadoop.net.DNSDomainNameResolver.getAllByDomainName(DNSDomainNameResolver.java:33)]\n']
[dfs.client.failover.connection.retries,dfs.client.failover.connection.retries.on.timeouts,dfs.client.failover.resolver.useFQDN.testcluster,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,ipc.client.connect.max.retries,ipc.client.connect.max.retries.on.timeouts];org.apache.hadoop.ipc.RemoteException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testWriteOperationOnActive;id_000004;[dfs.client.failover.resolver.useFQDN.testcluster=true, hadoop.security.groups.cache.secs=1280260624, dfs.client.failover.connection.retries.on.timeouts=129410034, hadoop.security.groups.cache.background.reload.threads=2055811844, dfs.client.failover.connection.retries=1627208458, ipc.client.connect.max.retries=1627208458, ipc.client.connect.max.retries.on.timeouts=129410034];org.apache.hadoop.ipc.RemoteException;No writes!;[org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider$NameNodeAnswer$ClientProtocolAnswer.answer(TestObserverReadProxyProvider.java:398), org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39), org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96), org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29), org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35)]\n']
[dfs.client.failover.connection.retries,dfs.client.failover.connection.retries.on.timeouts,dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.random.order,dfs.client.failover.resolve-needed.testcluster,dfs.client.failover.resolver.useFQDN.testcluster,hadoop.kerberos.keytab.login.autorenewal.enabled,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.dns.log-slow-lookups.enabled,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.negative-cache.secs,hadoop.security.token.service.use_ip,ipc.client.connect.max.retries,ipc.client.connect.max.retries.on.timeouts];java.net.UnknownHostException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testObserverRetriableException;id_000000;[dfs.client.failover.resolver.useFQDN.testcluster=true, hadoop.security.dns.log-slow-lookups.threshold.ms=966645485, hadoop.security.groups.cache.secs=429721445, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=880636269, ipc.client.connect.max.retries=1326811168, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=1814736010, dfs.client.failover.random.order=false, hadoop.kerberos.keytab.login.autorenewal.enabled=true, dfs.client.failover.connection.retries.on.timeouts=292152657, hadoop.security.groups.negative-cache.secs=120555533, dfs.client.failover.observer.auto-msync-period.testcluster=934869308, dfs.client.failover.connection.retries=1326811168, ipc.client.connect.max.retries.on.timeouts=292152657, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test;[java.net.InetAddress$CachedAddresses.get(InetAddress.java:797), java.net.InetAddress.getAllByName0(InetAddress.java:1519), java.net.InetAddress.getAllByName(InetAddress.java:1378), java.net.InetAddress.getAllByName(InetAddress.java:1306), org.apache.hadoop.net.DNSDomainNameResolver.getAllByDomainName(DNSDomainNameResolver.java:33)]\n']
[dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.resolver.useFQDN.testcluster,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.secs,hadoop.security.groups.negative-cache.secs];org.junit.ComparisonFailure;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testObserverRetriableException;id_000003;[dfs.client.failover.resolver.useFQDN.testcluster=true, hadoop.security.dns.log-slow-lookups.threshold.ms=146944410, hadoop.security.groups.cache.secs=860701151, hadoop.security.groups.negative-cache.secs=121505351, dfs.client.failover.observer.auto-msync-period.testcluster=1086036495];org.junit.ComparisonFailure;expected:<namenode[0].test:8020> but was:<namenode[2].test:8020>;[org.junit.Assert.assertEquals(Assert.java:117), org.junit.Assert.assertEquals(Assert.java:146), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.assertHandledBy(TestObserverReadProxyProvider.java:336), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testObserverRetriableException(TestObserverReadProxyProvider.java:319), org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider.testObserverRetriableException$$CONFUZZ(TestObserverReadProxyProvider.java)]\n']
[dfs.nameservices,hadoop.security.auth_to_local,hadoop.security.auth_to_local.mechanism,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.secs,hadoop.user.group.static.mapping.overrides];, hadoop.kerberos.keytab.login.autorenewal.enabled=true, dfs.client.failover.connection.retries.on.timeouts=283247109, hadoop.security.groups.negative-cache.secs=1826971429, dfs.client.failover.resolver.useFQDN.testcluster=true, dfs.ha.namenodes.testcluster=nn0,nn1, ipc.client.connect.max.retries.on.timeouts=283247109, dfs.namenode.rpc-address.testcluster.nn1=namenode1.test:8020, dfs.namenode.rpc-address.testcluster.nn0=namenode0.test:8020, hadoop.security.dns.log-slow-lookups.threshold.ms=1266846333, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1332948142, ipc.client.connect.max.retries=695563542, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=778268758, dfs.client.failover.random.order=false, dfs.client.failover.observer.auto-msync-period.testcluster=1288923679, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, dfs.client.failover.connection.retries=695563542, dfs.client.failover.observer.probe.retry.period=0ms, hadoop.security.groups.cache.warn.after.ms=1453036372, dfs.client.failover.resolve-needed.testcluster=true];1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testSingleObserverToStandby;id_000000;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=656133562, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, dfs.nameservices=testcluster, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.kerberos.keytab.login.autorenewal.enabled=true, dfs.client.failover.connection.retries.on.timeouts=283247109, hadoop.security.groups.negative-cache.secs=1826971429, dfs.client.failover.resolver.useFQDN.testcluster=true, dfs.ha.namenodes.testcluster=nn0,nn1, ipc.client.connect.max.retries.on.timeouts=283247109, dfs.namenode.rpc-address.testcluster.nn1=namenode1.test:8020, dfs.namenode.rpc-address.testcluster.nn0=namenode0.test:8020, hadoop.security.dns.log-slow-lookups.threshold.ms=1266846333, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=false, hadoop.security.groups.cache.background.reload.threads=1332948142, ipc.client.connect.max.retries=695563542, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=778268758, dfs.client.failover.random.order=false, dfs.client.failover.observer.auto-msync-period.testcluster=1288923679, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, dfs.client.failover.connection.retries=695563542, dfs.client.failover.observer.probe.retry.period=0ms, hadoop.security.groups.cache.warn.after.ms=1453036372, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test: Name or service not known;[java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method), java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:929), java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1529), java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:848), java.net.InetAddress.getAllByName0(InetAddress.java:1519)]\n']
[dfs.nameservices,hadoop.security.auth_to_local,hadoop.security.auth_to_local.mechanism,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.secs,hadoop.user.group.static.mapping.overrides];, hadoop.kerberos.keytab.login.autorenewal.enabled=true, dfs.client.failover.connection.retries.on.timeouts=1674782329, hadoop.security.groups.negative-cache.secs=1031052900, dfs.client.failover.resolver.useFQDN.testcluster=false, dfs.ha.namenodes.testcluster=nn0,nn1, ipc.client.connect.max.retries.on.timeouts=1674782329, dfs.namenode.rpc-address.testcluster.nn1=namenode1.test:8020, dfs.namenode.rpc-address.testcluster.nn0=namenode0.test:8020, hadoop.security.dns.log-slow-lookups.threshold.ms=2132022523, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=90906798, ipc.client.connect.max.retries=959588119, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=2100122183, dfs.client.failover.random.order=true, dfs.client.failover.observer.auto-msync-period.testcluster=377031725, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, dfs.client.failover.connection.retries=959588119, dfs.client.failover.observer.probe.retry.period=0ms, hadoop.security.groups.cache.warn.after.ms=615497416, dfs.client.failover.resolve-needed.testcluster=false];1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testSingleObserverToStandby;id_000001;[hadoop.security.auth_to_local.mechanism=hadoop, hadoop.security.groups.cache.secs=1689054665, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, dfs.nameservices=testcluster, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.kerberos.keytab.login.autorenewal.enabled=true, dfs.client.failover.connection.retries.on.timeouts=1674782329, hadoop.security.groups.negative-cache.secs=1031052900, dfs.client.failover.resolver.useFQDN.testcluster=false, dfs.ha.namenodes.testcluster=nn0,nn1, ipc.client.connect.max.retries.on.timeouts=1674782329, dfs.namenode.rpc-address.testcluster.nn1=namenode1.test:8020, dfs.namenode.rpc-address.testcluster.nn0=namenode0.test:8020, hadoop.security.dns.log-slow-lookups.threshold.ms=2132022523, hadoop.security.authentication=simple, hadoop.security.dns.log-slow-lookups.enabled=true, hadoop.security.groups.cache.background.reload.threads=90906798, ipc.client.connect.max.retries=959588119, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=2100122183, dfs.client.failover.random.order=true, dfs.client.failover.observer.auto-msync-period.testcluster=377031725, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, dfs.client.failover.connection.retries=959588119, dfs.client.failover.observer.probe.retry.period=0ms, hadoop.security.groups.cache.warn.after.ms=615497416, dfs.client.failover.resolve-needed.testcluster=false];org.apache.hadoop.ipc.RemoteException;No reads!;[org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider$NameNodeAnswer$ClientProtocolAnswer.answer(TestObserverReadProxyProvider.java:404), org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:39), org.mockito.internal.handler.MockHandlerImpl.handle(MockHandlerImpl.java:96), org.mockito.internal.handler.NullResultGuardian.handle(NullResultGuardian.java:29), org.mockito.internal.handler.InvocationNotifierHandler.handle(InvocationNotifierHandler.java:35)]\n']
[dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.random.order,dfs.client.failover.resolve-needed.testcluster];java.net.UnknownHostException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testSingleObserverToStandby;id_000002;[dfs.client.failover.random.order=true, dfs.client.failover.observer.auto-msync-period.testcluster=653848568, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test;[java.net.InetAddress$CachedAddresses.get(InetAddress.java:797), java.net.InetAddress.getAllByName0(InetAddress.java:1519), java.net.InetAddress.getAllByName(InetAddress.java:1378), java.net.InetAddress.getAllByName(InetAddress.java:1306), org.apache.hadoop.net.DNSDomainNameResolver.getAllByDomainName(DNSDomainNameResolver.java:33)]\n']
[dfs.client.failover.connection.retries,dfs.client.failover.connection.retries.on.timeouts,dfs.client.failover.observer.auto-msync-period.testcluster,dfs.client.failover.resolve-needed.testcluster,dfs.client.failover.resolver.useFQDN.testcluster,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.security.groups.negative-cache.secs,ipc.client.connect.max.retries,ipc.client.connect.max.retries.on.timeouts];java.net.UnknownHostException;1;['org.apache.hadoop.hdfs.server.namenode.ha.TestObserverReadProxyProvider#testWithNonClientProxy;id_000000;[dfs.client.failover.resolver.useFQDN.testcluster=true, hadoop.kerberos.min.seconds.before.relogin=1527905729, hadoop.security.groups.cache.secs=607990848, dfs.client.failover.connection.retries.on.timeouts=593344955, hadoop.security.groups.negative-cache.secs=489827943, hadoop.security.groups.cache.background.reload.threads=1782103600, dfs.client.failover.observer.auto-msync-period.testcluster=1609018563, dfs.client.failover.connection.retries=361817020, ipc.client.connect.max.retries=361817020, ipc.client.connect.max.retries.on.timeouts=593344955, hadoop.security.groups.cache.warn.after.ms=2052678472, dfs.client.failover.resolve-needed.testcluster=true];java.net.UnknownHostException;namenode0.test;[java.net.InetAddress$CachedAddresses.get(InetAddress.java:797), java.net.InetAddress.getAllByName0(InetAddress.java:1519), java.net.InetAddress.getAllByName(InetAddress.java:1378), java.net.InetAddress.getAllByName(InetAddress.java:1306), org.apache.hadoop.net.DNSDomainNameResolver.getAllByDomainName(DNSDomainNameResolver.java:33)]\n']
[dfs.block.scanner.skip.recent.accessed,dfs.block.scanner.volume.bytes.per.second,dfs.block.scanner.volume.join.timeout.ms,dfs.datanode.du.reserved.calculator,dfs.datanode.du.reserved.ram_disk,dfs.datanode.fixed.volume.size,dfs.datanode.scan.period.hours];java.lang.ClassNotFoundException;1;['org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList#testGetCachedVolumeCapacity;id_000000;[dfs.block.scanner.skip.recent.accessed=false, dfs.datanode.du.reserved.ram_disk=1815148519, dfs.datanode.scan.period.hours=-1, dfs.datanode.du.reserved.calculator=, dfs.datanode.fixed.volume.size=false, dfs.block.scanner.volume.join.timeout.ms=513688756, dfs.block.scanner.volume.bytes.per.second=41082738];java.lang.ClassNotFoundException;Class  not found;[org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2685), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2779), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2805), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder.build(ReservedSpaceCalculator.java:66), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.<init>(FsVolumeImpl.java:170)]\n']
[dfs.block.scanner.skip.recent.accessed,dfs.block.scanner.volume.bytes.per.second,dfs.block.scanner.volume.join.timeout.ms,dfs.datanode.du.reserved.calculator,dfs.datanode.du.reserved.disk,dfs.datanode.fixed.volume.size,dfs.datanode.scan.period.hours,fs.df.interval];java.lang.ClassNotFoundException;1;['org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList#testReleaseVolumeRefIfNoBlockScanner;id_000000;[dfs.block.scanner.skip.recent.accessed=false, dfs.datanode.scan.period.hours=-1, dfs.datanode.du.reserved.calculator=, dfs.datanode.du.reserved.disk=217526654, dfs.datanode.fixed.volume.size=false, dfs.block.scanner.volume.join.timeout.ms=1777821742, fs.df.interval=794625601, dfs.block.scanner.volume.bytes.per.second=95562677];java.lang.ClassNotFoundException;Class  not found;[org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2685), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2779), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2805), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder.build(ReservedSpaceCalculator.java:66), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.<init>(FsVolumeImpl.java:170)]\n']
[dfs.block.scanner.skip.recent.accessed,dfs.block.scanner.volume.bytes.per.second,dfs.block.scanner.volume.join.timeout.ms,dfs.datanode.du.reserved,dfs.datanode.du.reserved.calculator,dfs.datanode.du.reserved.disk,dfs.datanode.du.reserved.ram_disk,dfs.datanode.du.reserved.ssd,dfs.datanode.fixed.volume.size,dfs.datanode.scan.period.hours,fs.df.interval];java.lang.ClassNotFoundException;1;['org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.TestFsVolumeList#testDfsReservedForDifferentStorageTypes;id_000000;[dfs.block.scanner.skip.recent.accessed=false, dfs.datanode.du.reserved.ram_disk=1115005179, dfs.datanode.scan.period.hours=-1, dfs.datanode.du.reserved.calculator=, dfs.datanode.du.reserved.disk=1269087249, dfs.datanode.fixed.volume.size=true, dfs.datanode.du.reserved.ssd=884332606, dfs.block.scanner.volume.join.timeout.ms=976267773, fs.df.interval=1156600876, dfs.block.scanner.volume.bytes.per.second=32190132, dfs.datanode.du.reserved=100];java.lang.ClassNotFoundException;Class  not found;[org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2685), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2779), org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2805), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator$Builder.build(ReservedSpaceCalculator.java:66), org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsVolumeImpl.<init>(FsVolumeImpl.java:170)]\n']
[hadoop.http.authentication.kerberos.keytab,hadoop.http.idle_timeout.ms,hadoop.http.max.threads,hadoop.jetty.logs.serve.aliases,hadoop.security.auth_to_local,hadoop.security.auth_to_local.mechanism,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.secs,hadoop.ssl.client.conf,hadoop.ssl.enabled.protocols,hadoop.ssl.require.client.cert,hadoop.user.group.static.mapping.overrides];, hadoop.http.socket.backlog.size=618618514, hadoop.prometheus.endpoint.enabled=false, hadoop.ssl.hostname.verifier=DEFAULT, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=390352012, hadoop.http.authentication.simple.anonymous.allowed=true, hadoop.http.authentication.type=simple, ssl.client.truststore.location=, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, hadoop.http.max.response.header.size=1271453916, ssl.client.keystore.type=jks, hadoop.security.dns.log-slow-lookups.threshold.ms=2060900136, hadoop.security.authentication=simple, bind.address=localhost, hadoop.security.dns.log-slow-lookups.enabled=true, ssl.client.truststore.type=jks, hadoop.security.groups.cache.background.reload.threads=2059489579, hadoop.http.acceptor.count=674269064, io.file.buffer.size=428198738, hadoop.http.selector.count=28148542, hadoop.http.logs.enabled=false, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=377447435, hadoop.http.authentication.token.validity=843020062, ssl.client.stores.reload.interval=386961779, hadoop.http.max.request.header.size=1115989853, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.http.authentication.signature.secret.file=/root/hadoop-http-auth-signature-secret, hadoop.http.staticuser.user=dr.who, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, hadoop.http.sni.host.check.enabled=false, hadoop.security.groups.cache.warn.after.ms=432043021];1;['org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage#testGetImageTimeout;id_000000;[hadoop.http.idle_timeout.ms=1440114326, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.ssl.require.client.cert=false, hadoop.security.groups.cache.secs=1497716406, hadoop.http.authentication.kerberos.keytab=/root/hadoop.keytab, hadoop.http.max.threads=45620729, hadoop.jetty.logs.serve.aliases=false, hadoop.ssl.client.conf=ssl-client.xml, hadoop.ssl.enabled.protocols=TLSv1.2, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=true, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.http.socket.backlog.size=618618514, hadoop.prometheus.endpoint.enabled=false, hadoop.ssl.hostname.verifier=DEFAULT, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=390352012, hadoop.http.authentication.simple.anonymous.allowed=true, hadoop.http.authentication.type=simple, ssl.client.truststore.location=, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, hadoop.http.max.response.header.size=1271453916, ssl.client.keystore.type=jks, hadoop.security.dns.log-slow-lookups.threshold.ms=2060900136, hadoop.security.authentication=simple, bind.address=localhost, hadoop.security.dns.log-slow-lookups.enabled=true, ssl.client.truststore.type=jks, hadoop.security.groups.cache.background.reload.threads=2059489579, hadoop.http.acceptor.count=674269064, io.file.buffer.size=428198738, hadoop.http.selector.count=28148542, hadoop.http.logs.enabled=false, hadoop.security.token.service.use_ip=true, hadoop.kerberos.min.seconds.before.relogin=377447435, hadoop.http.authentication.token.validity=843020062, ssl.client.stores.reload.interval=386961779, hadoop.http.max.request.header.size=1115989853, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.http.authentication.signature.secret.file=/root/hadoop-http-auth-signature-secret, hadoop.http.staticuser.user=dr.who, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, hadoop.http.sni.host.check.enabled=false, hadoop.security.groups.cache.warn.after.ms=432043021];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.server.AbstractConnector.<init>(AbstractConnector.java:228), org.eclipse.jetty.server.AbstractNetworkConnector.<init>(AbstractNetworkConnector.java:44), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:215), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114), org.apache.hadoop.http.HttpServer2$Builder.createHttpChannelConnector(HttpServer2.java:534)]\n']
[hadoop.http.authentication.kerberos.keytab,hadoop.http.idle_timeout.ms,hadoop.http.max.threads,hadoop.jetty.logs.serve.aliases,hadoop.security.auth_to_local,hadoop.security.auth_to_local.mechanism,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.secs,hadoop.ssl.client.conf,hadoop.ssl.enabled.protocols,hadoop.ssl.require.client.cert,hadoop.user.group.static.mapping.overrides];, hadoop.http.socket.backlog.size=1881349205, hadoop.prometheus.endpoint.enabled=false, hadoop.ssl.hostname.verifier=DEFAULT, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=1807194052, hadoop.http.authentication.simple.anonymous.allowed=true, hadoop.http.authentication.type=simple, ssl.client.truststore.location=, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, hadoop.http.max.response.header.size=1775349975, ssl.client.keystore.type=jks, hadoop.security.dns.log-slow-lookups.threshold.ms=1284141436, hadoop.security.authentication=simple, bind.address=localhost, hadoop.security.dns.log-slow-lookups.enabled=false, ssl.client.truststore.type=jks, hadoop.security.groups.cache.background.reload.threads=98289187, hadoop.http.acceptor.count=189470855, io.file.buffer.size=537453790, hadoop.http.selector.count=1645171285, hadoop.http.logs.enabled=true, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1376156775, hadoop.http.authentication.token.validity=1039493131, ssl.client.stores.reload.interval=1058823919, hadoop.http.max.request.header.size=816877127, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.http.authentication.signature.secret.file=/root/hadoop-http-auth-signature-secret, hadoop.http.staticuser.user=dr.who, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, hadoop.http.sni.host.check.enabled=false, hadoop.security.groups.cache.warn.after.ms=1644902771];1;['org.apache.hadoop.hdfs.server.namenode.TestTransferFsImage#testGetImageTimeout;id_000001;[hadoop.http.idle_timeout.ms=536969760, hadoop.security.auth_to_local.mechanism=hadoop, hadoop.ssl.require.client.cert=false, hadoop.security.groups.cache.secs=1590894459, hadoop.http.authentication.kerberos.keytab=/root/hadoop.keytab, hadoop.http.max.threads=1386382500, hadoop.jetty.logs.serve.aliases=false, hadoop.ssl.client.conf=ssl-client.xml, hadoop.ssl.enabled.protocols=TLSv1.2, hadoop.security.auth_to_local=RULE:[1:$1] RULE:[2:$1], hadoop.security.groups.cache.background.reload=false, hadoop.user.group.static.mapping.overrides=dr.who=;, hadoop.http.socket.backlog.size=1881349205, hadoop.prometheus.endpoint.enabled=false, hadoop.ssl.hostname.verifier=DEFAULT, hadoop.kerberos.keytab.login.autorenewal.enabled=false, hadoop.security.groups.negative-cache.secs=1807194052, hadoop.http.authentication.simple.anonymous.allowed=true, hadoop.http.authentication.type=simple, ssl.client.truststore.location=, hadoop.http.authentication.kerberos.principal=HTTP/_HOST@LOCALHOST, hadoop.http.max.response.header.size=1775349975, ssl.client.keystore.type=jks, hadoop.security.dns.log-slow-lookups.threshold.ms=1284141436, hadoop.security.authentication=simple, bind.address=localhost, hadoop.security.dns.log-slow-lookups.enabled=false, ssl.client.truststore.type=jks, hadoop.security.groups.cache.background.reload.threads=98289187, hadoop.http.acceptor.count=189470855, io.file.buffer.size=537453790, hadoop.http.selector.count=1645171285, hadoop.http.logs.enabled=true, hadoop.security.token.service.use_ip=false, hadoop.kerberos.min.seconds.before.relogin=1376156775, hadoop.http.authentication.token.validity=1039493131, ssl.client.stores.reload.interval=1058823919, hadoop.http.max.request.header.size=816877127, hadoop.security.group.mapping=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback, hadoop.http.authentication.signature.secret.file=/root/hadoop-http-auth-signature-secret, hadoop.http.staticuser.user=dr.who, hadoop.ssl.keystores.factory.class=org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory, hadoop.http.filter.initializers=org.apache.hadoop.http.lib.StaticUserWebFilter, hadoop.http.sni.host.check.enabled=false, hadoop.security.groups.cache.warn.after.ms=1644902771];java.lang.OutOfMemoryError;Java heap space;[org.eclipse.jetty.io.SelectorManager.<init>(SelectorManager.java:99), org.eclipse.jetty.server.ServerConnector$ServerConnectorManager.<init>(ServerConnector.java:600), org.eclipse.jetty.server.ServerConnector.newSelectorManager(ServerConnector.java:223), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:216), org.eclipse.jetty.server.ServerConnector.<init>(ServerConnector.java:114)]\n']
[dfs.datanode.peer.metrics.min.outlier.detection.samples];org.junit.runners.model.TestTimedOutException;4;['org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics#testWithNoOutliers;id_000000;[dfs.datanode.peer.metrics.min.outlier.detection.samples=1974499669];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[java.util.Random.nextInt(Random.java:390), org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics.injectFastNodesSamples(TestDataNodeOutlierDetectionViaMetrics.java:145), org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics.testWithNoOutliers(TestDataNodeOutlierDetectionViaMetrics.java:122), org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics.testWithNoOutliers$$CONFUZZ(TestDataNodeOutlierDetectionViaMetrics.java), jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)]\n', 'org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics#testWithNoOutliers;id_000001;[dfs.datanode.peer.metrics.min.outlier.detection.samples=2081214407];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936), org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation.add(MutableRatesWithAggregation.java:107), org.apache.hadoop.metrics2.lib.MutableRollingAverages.add(MutableRollingAverages.java:211), org.apache.hadoop.hdfs.server.datanode.metrics.DataNodePeerMetrics.addSendPacketDownstream(DataNodePeerMetrics.java:106), org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics.injectFastNodesSamples(TestDataNodeOutlierDetectionViaMetrics.java:144)]\n', 'org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics#testOutlierIsDetected;id_000000;[dfs.datanode.peer.metrics.min.outlier.detection.samples=1506778489];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat.add(MutableRatesWithAggregation.java), org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation.add(MutableRatesWithAggregation.java:112), org.apache.hadoop.metrics2.lib.MutableRollingAverages.add(MutableRollingAverages.java:211), org.apache.hadoop.hdfs.server.datanode.metrics.DataNodePeerMetrics.addSendPacketDownstream(DataNodePeerMetrics.java:106), org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics.injectFastNodesSamples(TestDataNodeOutlierDetectionViaMetrics.java:144)]\n', 'org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics#testOutlierIsDetected;id_000001;[dfs.datanode.peer.metrics.min.outlier.detection.samples=1385990951];org.junit.runners.model.TestTimedOutException;test timed out after 300000 milliseconds;[org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation.add(MutableRatesWithAggregation.java), org.apache.hadoop.metrics2.lib.MutableRollingAverages.add(MutableRollingAverages.java:211), org.apache.hadoop.hdfs.server.datanode.metrics.DataNodePeerMetrics.addSendPacketDownstream(DataNodePeerMetrics.java:106), org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics.injectFastNodesSamples(TestDataNodeOutlierDetectionViaMetrics.java:144), org.apache.hadoop.hdfs.server.datanode.metrics.TestDataNodeOutlierDetectionViaMetrics.testOutlierIsDetected(TestDataNodeOutlierDetectionViaMetrics.java:90)]\n']
[dfs.http.policy,fs.creation.parallel.count,fs.webhdfs.impl.disable.cache,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.security.groups.negative-cache.secs,ssl.client.keystore.location,ssl.client.stores.reload.interval];java.security.GeneralSecurityException;1;["org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testEncodedPathUrl;id_000000;[ssl.client.keystore.location=, hadoop.kerberos.min.seconds.before.relogin=1273197447, hadoop.security.groups.cache.secs=515020177, ssl.client.stores.reload.interval=10000, hadoop.security.groups.negative-cache.secs=1686029710, hadoop.security.groups.cache.background.reload.threads=340148094, fs.webhdfs.impl.disable.cache=true, dfs.http.policy=HTTPS_ONLY, hadoop.security.groups.cache.background.reload=true, fs.creation.parallel.count=2112035836, hadoop.security.groups.cache.warn.after.ms=1450249019];java.security.GeneralSecurityException;The property 'ssl.client.keystore.location' has not been set in the ssl configuration file.;[org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.createKeyManagersFromConfiguration(FileBasedKeyStoresFactory.java:167), org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:272), org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:180), org.apache.hadoop.hdfs.web.SSLConnectionConfigurator.<init>(SSLConnectionConfigurator.java:50), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:135)]\n"]
[dfs.http.client.retry.policy.enabled,dfs.webhdfs.socket.connect-timeout,dfs.webhdfs.socket.read-timeout,fs.automatic.close,fs.creation.parallel.count,fs.webhdfs.impl.disable.cache,hadoop.kerberos.min.seconds.before.relogin,hadoop.security.dns.log-slow-lookups.threshold.ms,hadoop.security.groups.cache.background.reload,hadoop.security.groups.cache.background.reload.threads,hadoop.security.groups.cache.secs,hadoop.security.groups.cache.warn.after.ms,hadoop.ssl.require.client.cert,ipc.client.fallback-to-simple-auth-allowed,ssl.client.keystore.location,ssl.client.stores.reload.interval];java.lang.IllegalArgumentException;1;['org.apache.hadoop.hdfs.web.TestWebHdfsUrl#testEncodedPathUrl;id_000002;[ssl.client.keystore.location=, hadoop.security.dns.log-slow-lookups.threshold.ms=999312426, hadoop.security.groups.cache.secs=487114859, hadoop.ssl.require.client.cert=false, hadoop.security.groups.cache.background.reload.threads=410985168, fs.webhdfs.impl.disable.cache=true, dfs.webhdfs.socket.connect-timeout=961h, ipc.client.fallback-to-simple-auth-allowed=true, hadoop.security.groups.cache.background.reload=true, dfs.http.client.retry.policy.enabled=false, hadoop.kerberos.min.seconds.before.relogin=715578169, dfs.webhdfs.socket.read-timeout=9367640s, ssl.client.stores.reload.interval=10000, fs.creation.parallel.count=1108295315, fs.automatic.close=false, hadoop.security.groups.cache.warn.after.ms=1169824203];java.lang.IllegalArgumentException;No value for dfs.webhdfs.oauth2.access.token.provider found in conf file.;[org.apache.hadoop.hdfs.web.oauth2.Utils.notNull(Utils.java:37), org.apache.hadoop.hdfs.web.oauth2.OAuth2ConnectionConfigurator.<init>(OAuth2ConnectionConfigurator.java:55), org.apache.hadoop.hdfs.web.URLConnectionFactory.newOAuth2URLConnectionFactory(URLConnectionFactory.java:138), org.apache.hadoop.hdfs.web.WebHdfsFileSystem.initialize(WebHdfsFileSystem.java:242), org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)]\n']
[dfs.namenode.top.window.num.buckets];java.lang.OutOfMemoryError;1;['org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testInitAuditLoggers;id_000001;[dfs.namenode.top.window.num.buckets=685698257];java.lang.OutOfMemoryError;Java heap space;[java.util.ArrayList.<init>(ArrayList.java:154), org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:484), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testInitAuditLoggers(TestFSNamesystem.java:256)]\n']
[dfs.namenode.blocks.per.postponedblocks.rescan,dfs.permissions.allow.owner.set.quota];java.io.IOException;1;['org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem#testStartupSafemode;id_000000;[dfs.permissions.allow.owner.set.quota=true, dfs.namenode.blocks.per.postponedblocks.rescan=473565715];java.io.IOException;Unexpected configuration parameters: dfs.replication.max = 1686804515 > 32767;[org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.<init>(BlockManager.java:528), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:869), org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:796), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testStartupSafemode(TestFSNamesystem.java:108), org.apache.hadoop.hdfs.server.namenode.TestFSNamesystem.testStartupSafemode$$CONFUZZ(TestFSNamesystem.java)]\n']
[dfs.namenode.reject-unresolved-dn-topology-mapping];org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException;1;['org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager#testBadScript;id_000000;[dfs.namenode.reject-unresolved-dn-topology-mapping=true];org.apache.hadoop.hdfs.server.blockmanagement.UnresolvedTopologyException;Unresolved topology mapping for host null;[org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.resolveNetworkLocation(DatanodeManager.java:1030), org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.registerDatanode(DatanodeManager.java:1250), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.HelperFunction(TestDatanodeManager.java:407), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testBadScript(TestDatanodeManager.java:335), org.apache.hadoop.hdfs.server.blockmanagement.TestDatanodeManager.testBadScript$$CONFUZZ(TestDatanodeManager.java)]\n']
[dfs.datanode.simulateddatastorage.capacity];java.io.IOException;2;['org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset#testInjectionNonEmpty;id_000000;[dfs.datanode.simulateddatastorage.capacity=0];java.io.IOException;Creating block, no free space available;[org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset$BInfo.finalizeBlock(SimulatedFSDataset.java:269), org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset.finalizeBlock(SimulatedFSDataset.java:785), org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset.addSomeBlocks(TestSimulatedFSDataset.java:108), org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset.addSomeBlocks(TestSimulatedFSDataset.java:81), org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset.addSomeBlocks(TestSimulatedFSDataset.java:76)]\n', 'org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset#testInjectionNonEmpty;id_000001;[dfs.datanode.simulateddatastorage.capacity=18994];java.io.IOException;Creating block, no free space available;[org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset$BInfo.finalizeBlock(SimulatedFSDataset.java:269), org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset.finalizeBlock(SimulatedFSDataset.java:785), org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset.addSomeBlocks(TestSimulatedFSDataset.java:108), org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset.testInjectionNonEmpty(TestSimulatedFSDataset.java:238), org.apache.hadoop.hdfs.server.datanode.TestSimulatedFSDataset.testInjectionNonEmpty$$CONFUZZ(TestSimulatedFSDataset.java)]\n']
